{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8db6bff7",
   "metadata": {},
   "source": [
    "# Outline\n",
    "TBD\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e050949",
   "metadata": {},
   "source": [
    "# Configuring the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ec8d7d",
   "metadata": {},
   "source": [
    "## Module installation\n",
    "We’ll begin by installing the necessary Python modules for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b966059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# - Install modules from requirements.txt if present\n",
    "if os.path.isfile(\"requirements_plasticc.txt\"):\n",
    "  print(\"Installing modules from local requirements_plasticc.txt file ...\")\n",
    "  %pip install -q -r requirements_plasticc.txt\n",
    "else:\n",
    "  print(\"Installing modules ...\")  \n",
    "\n",
    "  %pip install -q pandas                                         # Data analysis modules                     \n",
    "  %pip install -q torch torchvision torchmetrics torchinfo    # ML modules\n",
    "  ##%pip install -q torch torchvision torchmetrics torchsummary    # ML modules\n",
    "  %pip install -q sh gdown matplotlib tqdm                          # Plot/util modules\n",
    "    \n",
    "  # - Create requirements file\n",
    "  %pip freeze > requirements_plasticc.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06a734e",
   "metadata": {},
   "source": [
    "## Import modules\n",
    "Next, we import the essential modules needed throughout the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be88dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "##   STANDARD MODULES\n",
    "###########################\n",
    "import os\n",
    "import math\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import gdown\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import json\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "from itertools import islice\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import urllib.request\n",
    "from sh import gunzip\n",
    "\n",
    "###########################\n",
    "##   DATA/TORCH MODULES\n",
    "###########################\n",
    "# - Data analysis\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# - Torch modules\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, Subset, random_split, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchmetrics\n",
    "import torchinfo\n",
    "from torchinfo import summary\n",
    "#from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535f6790",
   "metadata": {},
   "source": [
    "## Project folders\n",
    "We create a working directory `rundir` to run the tutorial in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155fba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "topdir= os.getcwd()\n",
    "rundir= os.path.join(topdir, \"run-plasticc_classifier\")\n",
    "path = Path(rundir)\n",
    "path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1a621a",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "For this tutorial, we will use the [**PLASTICC dataset**](https://zenodo.org/records/2539456).\n",
    "\n",
    "TBD\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08970dd9",
   "metadata": {},
   "source": [
    "## Dataset Download\n",
    "Next, we download the dataset from Google Drive and unzip it in the main folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1daff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(url, data_path, destdir):\n",
    "  \"\"\" Download data \"\"\"\n",
    "  data_fullpath= os.path.join(destdir, data_path)\n",
    "    \n",
    "  print(\"Downloading file from url %s ...\" % (url))\n",
    "  urllib.request.urlretrieve(url, data_path)  \n",
    "  print(\"DONE!\")\n",
    "\n",
    "  print(\"Moving file %s to dir %s ...\" % (data_path, destdir))\n",
    "  shutil.move(data_path, destdir)\n",
    "\n",
    "  print(\"Unzipping dataset file %s ...\" % (data_fullpath))\n",
    "  gunzip(data_fullpath)\n",
    "\n",
    "\n",
    "# - Download train metadata\n",
    "train_metadata_url= \"https://zenodo.org/records/2539456/files/plasticc_train_metadata.csv.gz?download=1\"\n",
    "train_metadata_gz_path= 'plasticc_train_metadata.csv.gz'\n",
    "train_metadata_gz_fullpath= os.path.join(rundir, train_metadata_gz_path)\n",
    "train_metadata_fullpath= os.path.join(rundir, 'plasticc_train_metadata.csv')\n",
    "if not os.path.isfile(train_metadata_fullpath):\n",
    "  download_data(train_metadata_url, train_metadata_path, rundir)\n",
    "\n",
    "# - Download train data\n",
    "train_data_url= \"https://zenodo.org/records/2539456/files/plasticc_train_lightcurves.csv.gz?download=1\"\n",
    "train_data_gz_path= 'plasticc_train_lightcurves.csv.gz'\n",
    "train_data_gz_fullpath= os.path.join(rundir, train_data_gz_path)\n",
    "train_data_fullpath= os.path.join(rundir, 'plasticc_train_lightcurves.csv')\n",
    "if not os.path.isfile(train_data_fullpath):\n",
    "  download_data(train_data_url, train_data_path, rundir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64be565b",
   "metadata": {},
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271e36bc",
   "metadata": {},
   "source": [
    "Define class names and other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c2b1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array([6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95, 99], dtype='int32')\n",
    "class_names = ['class_6','class_15','class_16','class_42','class_52','class_53','class_62','class_64','class_65','class_67','class_88','class_90','class_92','class_95','class_99']\n",
    "class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1, 99: 1}\n",
    "\n",
    "# LSST passbands (nm)  u    g    r    i    z    y      \n",
    "passbands = np.array([357, 477, 621, 754, 871, 1004], dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5bd5c2",
   "metadata": {},
   "source": [
    "### Loading train metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d6ca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Read train metadata as panda data frame\n",
    "train_metadata= pd.read_csv(train_metadata_fullpath)\n",
    "print(\"--> Train metadata\")\n",
    "print(train_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f17e47",
   "metadata": {},
   "source": [
    "### Loading train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66a336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Load data\n",
    "print(f\"Loading train data from file {train_data_fullpath} ...\")\n",
    "train_data = pd.read_csv(train_data_fullpath)\n",
    "print(\"train_data\")\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535c8007",
   "metadata": {},
   "source": [
    "### Splitting train/val sets\n",
    "Let's reserve a small portion (10%) of the training dataset for validation scopes. Below, we split the original training dataset into train and validation data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25addb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Split metadata in train/val sets\n",
    "random_state= 42\n",
    "test_size= 0.1\n",
    "meta_df_train, meta_df_val = train_test_split(\n",
    "  train_metadata,\n",
    "  test_size=test_size,\n",
    "  random_state=random_state,\n",
    "  shuffle=True\n",
    ")\n",
    "\n",
    "# - Get object_ids of train/test splits\n",
    "train_object_ids = meta_df_train['object_id'].unique()\n",
    "val_object_ids = meta_df_val['object_id'].unique()\n",
    "\n",
    "# - Use object IDs to split data_df\n",
    "data_df_train = train_data[train_data['object_id'].isin(train_object_ids)]\n",
    "data_df_val  = train_data[train_data['object_id'].isin(val_object_ids)]\n",
    "\n",
    "print(f\"#{len(meta_df_train)}/{len(meta_df_val)} data entries in train/val sets ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd455cd",
   "metadata": {},
   "source": [
    "Compute train class weights. They will be used when training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c969ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wtable(df, classes):\n",
    "  \"\"\" Compute class weights for all data entries \"\"\"  \n",
    "  all_y = np.array(df['target'], dtype = 'int32')\n",
    "  nsamples= all_y.shape[0]\n",
    "  y_count = np.unique(all_y, return_counts=True)[1]\n",
    "  \n",
    "  wtable= [float(count/nsamples) for count in y_count]\n",
    "  wtable.append(1.0) # Add weights for 'OTHER' class  \n",
    "    \n",
    "  return wtable\n",
    "\n",
    "# - Compute class weights\n",
    "wtable = get_wtable(meta_df_train, classes)\n",
    "print(\"--> wtable\")\n",
    "print(wtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7186b301",
   "metadata": {},
   "source": [
    "### Create PyTorch datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46153081",
   "metadata": {},
   "source": [
    "Define PyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da43bff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences_numpy(sequences, maxlen=None, dtype='float32', padding='post', truncating='post', value=0.0):\n",
    "  \"\"\"\n",
    "    Pads a list of 2D numpy arrays (sequence_len_i, n_features) to shape (N, maxlen, n_features).\n",
    "    \n",
    "    Parameters:\n",
    "        sequences : list of np.ndarray of shape (Ti, D)\n",
    "        maxlen    : int or None, length to pad/truncate to. If None, use max sequence length.\n",
    "        dtype     : data type of output array\n",
    "        padding   : 'pre' or 'post'\n",
    "        truncating: 'pre' or 'post'\n",
    "        value     : value used for padding\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray of shape (N, maxlen, D)\n",
    "  \"\"\"\n",
    "  num_samples = len(sequences)\n",
    "  feature_dim = sequences[0].shape[1]\n",
    "  lengths = [seq.shape[0] for seq in sequences]\n",
    "\n",
    "  if maxlen is None:\n",
    "    maxlen = max(lengths)\n",
    "\n",
    "  padded = np.full((num_samples, maxlen, feature_dim), value, dtype=dtype)\n",
    "\n",
    "  for i, seq in enumerate(sequences):\n",
    "    if truncating == 'pre':\n",
    "      trunc = seq[-maxlen:]\n",
    "    else:\n",
    "      trunc = seq[:maxlen]\n",
    "\n",
    "    if padding == 'pre':\n",
    "      padded[i, -len(trunc):] = trunc\n",
    "    else:\n",
    "      padded[i, :len(trunc)] = trunc\n",
    "\n",
    "  return padded\n",
    "\n",
    "class PlasticcDataset(Dataset):\n",
    "  def __init__(\n",
    "    self,\n",
    "    data_df,     # Pandas data frame\n",
    "    meta_df, # Pandas data frame\n",
    "    augment=False,\n",
    "    use_specz=False,\n",
    "    extragalactic=None,  \n",
    "    nmax=-1\n",
    "  ):\n",
    "    # - Set options\n",
    "    self.meta_df= meta_df\n",
    "    self.data_df= data_df\n",
    "    self.X_seq = None    # shape: (N, seq_len, 4)\n",
    "    self.X_meta = None   # shape: (N, num_features)\n",
    "    self.Y= None  # one-hot labels: (N, num_classes)\n",
    "    self.wtable= None\n",
    "    self.class_weights= None\n",
    "    self.nmax= nmax\n",
    "    self.use_specz= use_specz\n",
    "    self.extragalactic= extragalactic\n",
    "    self.classes = np.array([6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95, 99], dtype='int32')\n",
    "    self.class_names = ['class_6','class_15','class_16','class_42','class_52','class_53','class_62','class_64','class_65','class_67','class_88','class_90','class_92','class_95','class_99']\n",
    "    self.classid2label= {\n",
    "      6: \"PS-MULENS\",# Point_source_mu-lensing\n",
    "      15: \"TDE\", # Tidal disruption event\n",
    "      16: \"EBE\", # Eclipsing binary event\n",
    "      42: \"SN-II\", # Core-collapse supernova Type II\n",
    "      52: \"SN-Iax\", # Supernova Type Ia-x\n",
    "      53: \"MIRA\", # Mira variable\n",
    "      62: \"SN-Ibc\", # Core-collapse supernova Type Ibc\n",
    "      64: \"KN\", # Kilonova\n",
    "      65: \"M-DWARF\", # M dwarf\n",
    "      67: \"SN-Ia-91bg\", # Supernova Type Ia-91bg\n",
    "      88: \"AGN\", # Active galactic nucleus\n",
    "      90: \"SN-Ia\", # Supernova Type Ia\n",
    "      92: \"RR-LY\", # RR Lyrae\n",
    "      95: \"SLSN\", # Superluminous supernova\n",
    "      99: \"OTHER\", # Other class\n",
    "    } \n",
    "    self.nclasses= len(self.classes)\n",
    "    self.class_weight_factors= np.array([2,2,1,1,1,1,1,2,1,1,1,1,1,2,2], dtype='float32')\n",
    "    \n",
    "    # LSST passbands (nm)  u    g    r    i    z    y      \n",
    "    self.passbands = np.array([357, 477, 621, 754, 871, 1004], dtype='float32')\n",
    "    \n",
    "    # - Load data\n",
    "    self.__load_data()\n",
    "    \n",
    "  def __compute_wtable(self):\n",
    "    \"\"\"\n",
    "      Compute:\n",
    "        - wtable: class frequencies (N_class,)\n",
    "        - class_weights: inverse frequency, normalized to sum to num_classes\n",
    "\n",
    "      Returns:\n",
    "        wtable (torch.Tensor), class_weights (torch.Tensor)\n",
    "    \"\"\"\n",
    "    class_counts = self.Y.sum(dim=0)  # sum over all samples (across rows)\n",
    "    total_samples = self.Y.shape[0]\n",
    "    wtable = class_counts / total_samples\n",
    "    wtable[self.nclasses-1]= 1.0\n",
    "    \n",
    "    # Inverse frequency as weight (avoid divide-by-zero)\n",
    "    class_weights = 1.0 / (wtable + 1e-8)\n",
    "    class_weights = class_weights * (len(wtable) / class_weights.sum())  # normalize to mean 1\n",
    "\n",
    "    return wtable, class_weights\n",
    "\n",
    "\n",
    "  def __load_data(self):\n",
    "    \"\"\" Load data/metadata from files \"\"\"      \n",
    "\n",
    "    # - Group data by object_id\n",
    "    groups = self.data_df.groupby('object_id')\n",
    "    print(f\"Reading {len(groups)} data entries ...\")\n",
    "    \n",
    "    features_seq_all= []\n",
    "    features_meta_all= []\n",
    "    target_ids= []\n",
    "    seq_min_size= 1.e+99\n",
    "    seq_max_size= -1\n",
    "    \n",
    "    for g in groups:\n",
    "      # - Find data with object_id\n",
    "      id = g[0]\n",
    "      meta = self.meta_df.loc[self.meta_df['object_id'] == id]\n",
    "    \n",
    "      z_photo= meta['hostgal_photoz'].iloc[0]         # photometric host-redshift (float32)\n",
    "      zerr_photo= meta['hostgal_photoz_err'].iloc[0]  # uncertainty on photometric host-redshift\n",
    "      ddf= meta['ddf_bool'].iloc[0]                   # boolean flag: 1 for DDF, 0 for WFD\n",
    "      mwebv= meta['mwebv'].iloc[0]                    # Galactic E(B-V) extinction\n",
    "      z_spec= meta['hostgal_specz'].iloc[0]           # accurate spectroscopic-redshift for small subset\n",
    "      z= z_spec if self.use_specz else z_photo\n",
    "      z_err= 0.0 if self.use_specz else zerr_photo\n",
    "    \n",
    "      # - Skip source with invalid redshift?\n",
    "      if self.extragalactic == True and z_photo==0:\n",
    "        continue\n",
    "\n",
    "      if self.extragalactic == False and z_photo>0:\n",
    "        continue\n",
    "\n",
    "      # - Set target id\n",
    "      if 'target' in meta:\n",
    "        class_id= int(meta['target'].iloc[0])\n",
    "        target_id= np.where(self.classes == class_id)[0][0]\n",
    "      else:\n",
    "        target_id= len(self.classes) - 1  # interpret as class 99\n",
    "        \n",
    "      target_ids.append(target_id)\n",
    "        \n",
    "      # - Set sequence features\n",
    "      mjd      = np.array(g[1]['mjd'],      dtype='float32')\n",
    "      mjd -= mjd[0]\n",
    "      mjd /= 100 # Earth time shift in day*100\n",
    "      mjd /= (z + 1) # Object time shift in day*100\n",
    "      tdiff= np.ediff1d(mjd, to_begin = [0])\n",
    "      band     = np.array(g[1]['passband'], dtype='int32')\n",
    "      flux     = np.array(g[1]['flux'],     dtype='float32')\n",
    "      flux_err = np.array(g[1]['flux_err'], dtype='float32')\n",
    "      flux_max = np.max(flux)\n",
    "      flux_min = np.min(flux)\n",
    "      flux_norm= flux_max - flux_min \n",
    "      flux_pow = math.log2(flux_norm)\n",
    "      detected = np.array(g[1]['detected_bool'], dtype='float32')\n",
    "      received_wavelength = passbands[band] # Earth wavelength in nm\n",
    "      received_freq = 300000 / received_wavelength # Earth frequency in THz\n",
    "      source_wavelength = received_wavelength / (z + 1) # Object wavelength in nm\n",
    "      received_freq/= 1000.\n",
    "      source_wavelength/= 1000.\n",
    "        \n",
    "      #print(\"mjd.shape\")  \n",
    "      #print(mjd.shape)\n",
    "      seq_size= mjd.shape[0]\n",
    "      if seq_size>seq_max_size:\n",
    "        seq_max_size= seq_size      \n",
    "      if seq_size<seq_min_size:\n",
    "        seq_min_size= seq_size \n",
    "        \n",
    "      features_seq= np.zeros( (seq_size, 4), dtype = 'float32')\n",
    "      features_seq[:,0]= tdiff\n",
    "      features_seq[:,1]= flux/flux_norm\n",
    "      features_seq[:,2]= flux_err/flux_norm\n",
    "      features_seq[:,3]= source_wavelength\n",
    "      ##features_seq[:,4]= detected\n",
    "    \n",
    "      features_seq_all.append(features_seq)\n",
    "        \n",
    "    \n",
    "      # - Set metadata features\n",
    "      features_meta= np.zeros(5, dtype = 'float32')\n",
    "      features_meta[0]= ddf\n",
    "      features_meta[1]= z\n",
    "      features_meta[2]= z_err\n",
    "      features_meta[3]= mwebv\n",
    "      features_meta[4]= flux_pow / 10\n",
    "      \n",
    "      features_meta_all.append(features_meta)\n",
    "        \n",
    "    \n",
    "      if len(features_seq_all) % 1000 == 0:\n",
    "        print('Converting data {0}'.format(len(features_seq_all)), end='\\r')\n",
    "\n",
    "      if self.nmax!=-1 and len(features_seq_all) >= self.nmax:\n",
    "        print(f'Reached data sample limit {self.nmax}...stop reading data')  \n",
    "        break\n",
    "        \n",
    "    print(f\"#{len(features_seq_all)} data added with seq range ({seq_min_size}, {seq_max_size})...\")\n",
    "    \n",
    "    # - Find sequence truncation point (assuming a power of 2 larger than max seq size) \n",
    "    seq_size_opt= 2 ** math.ceil(math.log2(seq_max_size))\n",
    "    print(f\"Padding sequence data to a size of {seq_size_opt} ...\")\n",
    "    features_seq_padded= pad_sequences_numpy(features_seq_all, maxlen=seq_size_opt)\n",
    "    print(\"features_seq_padded.shape\")\n",
    "    print(features_seq_padded.shape)\n",
    "    \n",
    "    # - Convert data to tensors\n",
    "    self.X_seq= torch.from_numpy(features_seq_padded)\n",
    "    self.X_meta= torch.from_numpy(np.array(features_meta_all))\n",
    "    Y_target= torch.from_numpy(np.array(target_ids))\n",
    "    #self.Y= F.one_hot(Y_target, num_classes=-1)\n",
    "    self.Y= F.one_hot(Y_target, num_classes=len(self.classes))\n",
    "    \n",
    "    print(\"self.X_seq.shape\")\n",
    "    print(self.X_seq.shape)\n",
    "    print(\"self.X_meta.shape\")\n",
    "    print(self.X_meta.shape)\n",
    "    print(\"self.Y.shape\")\n",
    "    print(self.Y.shape)\n",
    "    \n",
    "    # - Compute wtable\n",
    "    self.wtable, self.class_weights= self.__compute_wtable()\n",
    "    print(\"self.wtable\")\n",
    "    print(self.wtable)\n",
    "    print(\"self.class_weights\")\n",
    "    print(self.class_weights)\n",
    "    \n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.X_seq)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.X_seq[idx], self.X_meta[idx], self.Y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02378e7",
   "metadata": {},
   "source": [
    "Create train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75eb747",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train= PlasticcDataset(\n",
    "  data_df_train, \n",
    "  meta_df_train,\n",
    "  augment=False,\n",
    "  use_specz=False,\n",
    "  extragalactic=None,  \n",
    "  nmax=-1\n",
    ")\n",
    "\n",
    "print(\"--> wtable\")\n",
    "print(wtable)\n",
    "print(\"--> wtable (dataset)\")\n",
    "print(dataset_train.wtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b550ee9c",
   "metadata": {},
   "source": [
    "Load validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ac007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val= PlasticcDataset(\n",
    "  data_df_val, \n",
    "  meta_df_val,\n",
    "  augment=False,\n",
    "  use_specz=False,\n",
    "  extragalactic=None,  \n",
    "  nmax=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfb7340",
   "metadata": {},
   "source": [
    "### Augmentate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa8310e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08077a2",
   "metadata": {},
   "source": [
    "### Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b1d4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 64\n",
    "dl_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "dl_val   = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fe24fa",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59df3034",
   "metadata": {},
   "source": [
    "### Define the classifier model\n",
    "Define the RNN model. It takes two inputs:\n",
    "\n",
    "- `seq`: Time series, shape (batch_size, time_steps, n_features)\n",
    "- `meta`: Meta data parameters, shape (batch_size, n_meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f89b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlasticcClassifier(torch.nn.Module):\n",
    "  def __init__(\n",
    "    self, \n",
    "    n_features, \n",
    "    n_meta_features, \n",
    "    hidden_size=64, \n",
    "    num_layers=2, \n",
    "    num_classes=15, \n",
    "    bidirectional=False, \n",
    "    dropout=0.5,\n",
    "    fc_hidden_size=128  \n",
    "  ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "    self.gru = torch.nn.GRU(\n",
    "      input_size=n_features,\n",
    "      hidden_size=hidden_size,\n",
    "      num_layers=num_layers,\n",
    "      batch_first=True,\n",
    "      bidirectional=bidirectional,\n",
    "      dropout=dropout if num_layers > 1 else 0.0\n",
    "    )\n",
    "\n",
    "    self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    self.classifier = torch.nn.Sequential(\n",
    "      torch.nn.Linear(hidden_size * self.num_directions + n_meta_features, fc_hidden_size),\n",
    "      torch.nn.ReLU(),\n",
    "      torch.nn.Dropout(dropout),\n",
    "      torch.nn.Linear(fc_hidden_size, num_classes),\n",
    "    )\n",
    "\n",
    "  def forward(self, x_seq, x_meta):\n",
    "    rnn_out, _ = self.gru(x_seq)                      # (batch, seq_len, hidden*2)\n",
    "    pooled, _ = torch.max(rnn_out, dim=1)             # (batch, hidden*2)\n",
    "    x = torch.cat([pooled, x_meta], dim=1)            # (batch, hidden*2 + meta_features)\n",
    "    logits = self.classifier(x)\n",
    "    probs = F.softmax(logits, dim=1)                  # one-hot output\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b1a9ce",
   "metadata": {},
   "source": [
    "Create classifier instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55502b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Create model\n",
    "n_features= dataset_train.X_seq.shape[2]\n",
    "n_meta_features= dataset_train.X_meta.shape[1]\n",
    "num_classes= dataset_train.Y.shape[1]\n",
    "seq_length= dataset_train.X_seq.shape[1]\n",
    "print(f\"seq_length={seq_length}\")\n",
    "print(f\"n_features={n_features}\")\n",
    "print(f\"n_meta_features={n_meta_features}\")\n",
    "print(f\"num_classes={num_classes}\")\n",
    "\n",
    "model = PlasticcClassifier(\n",
    "  n_features=n_features,\n",
    "  n_meta_features=n_meta_features,\n",
    "  num_classes=num_classes,\n",
    "  fc_hidden_size=128,\n",
    "  bidirectional=True  \n",
    ")\n",
    "\n",
    "# - Print model structure\n",
    "summary(\n",
    "  model, \n",
    "  #input_data=[\n",
    "  #  torch.randn(batch_size, seq_length, n_features),  # x_seq: (batch, seq_len, n_features)\n",
    "  #  torch.randn(batch_size, n_meta_features)       # x_meta: (batch, n_meta_features)\n",
    "  #]\n",
    "  input_size=[(batch_size, seq_length, n_features), (batch_size, n_meta_features)] \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808e0666",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec6ebda",
   "metadata": {},
   "source": [
    "Define the loss function as required by the PLASTICC challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffecbfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_weighted_logloss(y_true, y_pred, class_weights):\n",
    "  \"\"\"\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    Adapted from TF version: https://www.kaggle.com/ogrellier\n",
    "  \n",
    "    - y_true/y_pred are one-hot encoded\n",
    "    - class_weights=[2,2,1,1,1,1,1,2,1,1,1,1,1,2,2] (see challenge result paper)\n",
    "  \"\"\" \n",
    "    \n",
    "  # - Normalize rows and limit y_preds to eps, 1-eps    \n",
    "  eps = 1e-15\n",
    "  preds = torch.clamp(y_pred, min=eps, max=1 - eps)\n",
    "\n",
    "  # - Transform to log\n",
    "  log_preds = torch.log(y_pred)  # (N, M)\n",
    "\n",
    "  # (1) Per-class weighted log loss: sum across samples\n",
    "  y_log_ones = torch.sum(y_true * log_preds, dim=0)              # (M,)\n",
    "  nb_pos = torch.sum(y_true, dim=0)                              # (M,)\n",
    "  nb_pos = torch.where(nb_pos == 0, torch.ones_like(nb_pos), nb_pos)\n",
    "\n",
    "  y_w = y_log_ones * class_weights / nb_pos                       # (M,)\n",
    "\n",
    "  loss = -torch.sum(y_w) / torch.sum(class_weights)\n",
    "  loss = loss / y_pred.shape[0]  # normalize by batch size\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b963e163",
   "metadata": {},
   "source": [
    "Define a function to initialize weights before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc327ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "  \"\"\" Applies custom weight initialization to layers in the model \"\"\"\n",
    "  for m in model.modules():\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "      torch.nn.init.xavier_uniform_(m.weight)  # or kaiming_uniform_\n",
    "      if m.bias is not None:\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "    elif isinstance(m, torch.nn.GRU):\n",
    "      for name, param in m.named_parameters():\n",
    "        if 'weight_ih' in name:\n",
    "          torch.nn.init.xavier_uniform_(param.data)\n",
    "        elif 'weight_hh' in name:\n",
    "          torch.nn.init.orthogonal_(param.data)\n",
    "        elif 'bias' in name:\n",
    "          torch.nn.init.zeros_(param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5969e5fb",
   "metadata": {},
   "source": [
    "Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2cf728",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "  def __init__(self):\n",
    "    self.reset()\n",
    "\n",
    "  def reset(self):\n",
    "    self.sum = 0\n",
    "    self.count = 0\n",
    "\n",
    "  def update(self, value, n=1):\n",
    "    self.sum += value * n\n",
    "    self.count += n\n",
    "\n",
    "  @property\n",
    "  def avg(self):\n",
    "    return self.sum / self.count if self.count > 0 else 0\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    dl_train,\n",
    "    dl_val=None,\n",
    "    num_epochs=1,\n",
    "    lr=1e-3,\n",
    "    checkpoint_path=\"model_checkpoint.pth\",\n",
    "    class_weights=None,\n",
    "    clip_grad=False,\n",
    "    max_grad_norm=5\n",
    "):\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  model.to(device)\n",
    "\n",
    "  # - Check class weights  \n",
    "  if class_weights is None:\n",
    "    raise ValueError(\"class_weights must be provided\")\n",
    "\n",
    "  class_weights = class_weights.to(device)\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "  # - Init metrics\n",
    "  loss_meter = AverageMeter()\n",
    "  acc_metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes).to(device)\n",
    "  f1_metric = torchmetrics.F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\").to(device)\n",
    "  confusion_matrix_metric = torchmetrics.ConfusionMatrix(task=\"multiclass\", num_classes=num_classes, normalize=\"true\").to(device)\n",
    "    \n",
    "  val_acc_metric = val_f1_metric = None\n",
    "  if dl_val is not None:\n",
    "    val_loss_meter = AverageMeter()    \n",
    "    val_acc_metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes).to(device)\n",
    "    val_f1_metric = torchmetrics.F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\").to(device)\n",
    "    val_confusion_matrix_metric = torchmetrics.ConfusionMatrix(task=\"multiclass\", num_classes=num_classes, normalize=\"true\").to(device)\n",
    "    \n",
    "  history = {\n",
    "    \"loss_train\": [],\n",
    "    \"acc_train\": [],\n",
    "    \"f1score_train\": [],\n",
    "    \"cm_train\": None,\n",
    "    \"cm_metric_train\": None,  \n",
    "    \"loss_val\": [],\n",
    "    \"acc_val\": [],\n",
    "    \"f1score_val\": [],\n",
    "    \"cm_val\": None,\n",
    "    \"cm_metric_val\": None,   \n",
    "  }\n",
    "\n",
    "  # - Start training loop\n",
    "  for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # - Reset avg metrics\n",
    "    progress = tqdm(dl_train, desc=f\"Epoch {epoch+1} [Train]\", leave=False)\n",
    "    loss_meter.reset()\n",
    "    acc_metric.reset()\n",
    "    f1_metric.reset()\n",
    "    confusion_matrix_metric.reset()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    y_true_all = []\n",
    "    y_pred_all = []\n",
    "\n",
    "    # - Run batch loop\n",
    "    for x_seq, x_meta, y in progress:    \n",
    "      x_seq, x_meta, y = x_seq.to(device), x_meta.to(device), y.to(device)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      y_pred = model(x_seq, x_meta)\n",
    "\n",
    "      loss = multi_weighted_logloss(y, y_pred, class_weights)\n",
    "      \n",
    "      # ✅ Check 1: Loss is finite (not NaN or Inf)\n",
    "      if not torch.isfinite(loss):\n",
    "        print(\"⚠️ Warning: loss is NaN or Inf. Skipping this batch.\")\n",
    "        #continue  # skip backprop for this batch\n",
    "        \n",
    "      loss.backward()\n",
    "    \n",
    "      # ✅ Check 2: Optional: Print max gradient norm for debugging\n",
    "      total_norm = 0\n",
    "      for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "          param_norm = p.grad.data.norm(2)\n",
    "          total_norm += param_norm.item() ** 2\n",
    "      total_norm = total_norm ** 0.5\n",
    "      if total_norm > 1e3:\n",
    "        print(f\"⚠️ High gradient norm: {total_norm:.2f}\")\n",
    "\n",
    "      # ✅ Check 3: Clip gradients to prevent explosion\n",
    "      if clip_grad:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)\n",
    "    \n",
    "      #if clip_grad:  \n",
    "      #  #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "      #  torch.nn.utils.clip_grad_norm_(model.parameters(), gradclip)\n",
    "      #  for p in model.parameters():\n",
    "      #    p.data.add_(p.grad, alpha=-lr)  \n",
    "    \n",
    "      optimizer.step() \n",
    "        \n",
    "      # - Update loss and metrics \n",
    "      target_pred = y_pred.argmax(dim=1)\n",
    "      target_true= y.argmax(dim=1)\n",
    "      loss_meter.update(loss.item(), x_seq.size(0))\n",
    "      acc_metric.update(target_pred, target_true)\n",
    "      f1_metric.update(target_pred, target_true)\n",
    "      confusion_matrix_metric.update(target_pred, target_true)  \n",
    "        \n",
    "      total_loss += loss.item()\n",
    "      y_true_all.append(target_true.cpu())\n",
    "      y_pred_all.append(target_pred.cpu())  \n",
    "    \n",
    "      # - Update progress bar\n",
    "      progress.set_postfix({\n",
    "        \"loss\": f\"{loss_meter.avg:.4f}\",\n",
    "        \"acc\": f\"{acc_metric.compute().item():.4f}\",\n",
    "        \"f1\": f\"{f1_metric.compute().item():.4f}\"\n",
    "      })  \n",
    "\n",
    "    # - Compute average metrics (v1)\n",
    "    avg_train_loss = total_loss / len(dl_train)\n",
    "    y_true_all = torch.cat(y_true_all)\n",
    "    y_pred_all = torch.cat(y_pred_all)\n",
    "    train_acc = (y_true_all == y_pred_all).float().mean().item()\n",
    "    train_f1 = f1_score(y_true_all, y_pred_all, average='macro', zero_division=0)\n",
    "\n",
    "    # - Compute average metrics (v2)\n",
    "    avg_train_loss_v2 = loss_meter.avg\n",
    "    train_acc_v2 = acc_metric.compute().item()\n",
    "    train_f1_v2 = f1_metric.compute().item()\n",
    "    confusion_matrix= confusion_matrix_metric.compute().cpu().numpy()\n",
    "    history[\"loss_train\"].append(avg_train_loss_v2)\n",
    "    history[\"acc_train\"].append(train_acc_v2)\n",
    "    history[\"f1score_train\"].append(train_f1_v2) \n",
    "    history[\"cm_train\"]= confusion_matrix\n",
    "    history[\"cm_metric_train\"]= confusion_matrix_metric\n",
    "        \n",
    "    #print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f}\", end='')\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]: loss={avg_train_loss:.4f}, {avg_train_loss_v2:.4f} | acc={train_acc:.4f}, {train_acc_v2:.4f} | f1={train_f1:.4f}, {train_f1_v2:.4f}\", end='')\n",
    "\n",
    "    \n",
    "    if dl_val is not None:\n",
    "      model.eval()\n",
    "    \n",
    "      # - Init val metrics\n",
    "      val_loss_meter.reset()\n",
    "      val_acc_metric.reset()\n",
    "      val_f1_metric.reset()\n",
    "      val_confusion_matrix_metric.reset()\n",
    "      val_progress = tqdm(dl_val, desc=f\"Epoch {epoch+1} [Val]\", leave=False)\n",
    "      val_loss = 0.0\n",
    "      y_true_val = []\n",
    "      y_pred_val = []\n",
    "\n",
    "      with torch.no_grad():\n",
    "        for x_seq, x_meta, y in val_progress:    \n",
    "          x_seq, x_meta, y = x_seq.to(device), x_meta.to(device), y.to(device)\n",
    "          y_pred = model(x_seq, x_meta)\n",
    "\n",
    "          loss = multi_weighted_logloss(y, y_pred, class_weights)\n",
    "        \n",
    "          # - Update loss and accuracy  \n",
    "          val_loss_meter.update(loss.item(), x_seq.size(0))\n",
    "          target_pred = y_pred.argmax(dim=1)\n",
    "          target_true = y.argmax(dim=1)  \n",
    "          val_acc_metric.update(target_pred, target_true)\n",
    "          val_f1_metric.update(target_pred, target_true)\n",
    "          val_confusion_matrix_metric.update(target_pred, target_true) \n",
    "          val_loss += loss.item()\n",
    "\n",
    "          y_true_val.append(y.argmax(dim=1).cpu())\n",
    "          y_pred_val.append(y_pred.argmax(dim=1).cpu())\n",
    "  \n",
    "          # - Update progress bar\n",
    "          val_progress.set_postfix({\"loss\": f\"{val_loss_meter.avg:.4f}\"})\n",
    "        \n",
    "      # - Compute average metrics (v1)    \n",
    "      avg_val_loss = val_loss / len(dl_val)\n",
    "      y_true_val = torch.cat(y_true_val)\n",
    "      y_pred_val = torch.cat(y_pred_val)\n",
    "      val_acc = (y_true_val == y_pred_val).float().mean().item()\n",
    "      val_f1 = f1_score(y_true_val, y_pred_val, average='macro', zero_division=0)\n",
    "\n",
    "      # - Compute average metrics (v2)    \n",
    "      avg_val_loss_v2 = val_loss_meter.avg\n",
    "      val_acc_v2 = val_acc_metric.compute().item()\n",
    "      val_f1_v2 = val_f1_metric.compute().item()\n",
    "      val_confusion_matrix= val_confusion_matrix_metric.compute().cpu().numpy()  \n",
    "      history[\"loss_val\"].append(avg_val_loss_v2)\n",
    "      history[\"acc_val\"].append(val_acc_v2)\n",
    "      history[\"f1score_val\"].append(val_f1_v2)  \n",
    "      history[\"cm_val\"]= val_confusion_matrix\n",
    "      history[\"cm_metric_val\"]= val_confusion_matrix_metric\n",
    "    \n",
    "      print(f\" | Val Loss: {avg_val_loss:.4f}, {avg_val_loss_v2:.4f} | Val Acc: {val_acc:.4f}, {val_acc_v2:.4f} | Val F1: {val_f1:.4f}, {val_f1_v2:.4f}\")\n",
    "    else:\n",
    "      print()\n",
    "\n",
    "  # - Save final model\n",
    "  print(f\"\\n✅ Model checkpoint saved to: {checkpoint_path}\")\n",
    "  torch.save(model.state_dict(), checkpoint_path)  \n",
    "  #torch.save(model, outfile_model)\n",
    "\n",
    "  print(\"Training complete.\")\n",
    "  return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c848e25",
   "metadata": {},
   "source": [
    "Initialize weights and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7466ae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Initialize weights\n",
    "torch.manual_seed(10)\n",
    "initialize_weights(model)\n",
    "\n",
    "# - Run train\n",
    "nepochs= 100\n",
    "lr= 1e-4\n",
    "class_weights= torch.from_numpy(dataset_train.class_weight_factors)\n",
    "outfile_weights= os.path.join(rundir, \"model_checkpoint.pth\")\n",
    "\n",
    "metric_hist= train_model(\n",
    "  model, \n",
    "  dl_train=dl_train, \n",
    "  dl_val=dl_val, \n",
    "  num_epochs=nepochs,\n",
    "  lr=lr,\n",
    "  checkpoint_path=outfile_weights,\n",
    "  class_weights=class_weights,\n",
    "  clip_grad=False,\n",
    "  max_grad_norm=5  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51670c14",
   "metadata": {},
   "source": [
    "Let’s plot the training and validation metrics after the training run is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8f32e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_metric_hist(metric_hist):\n",
    "  \n",
    "  epochs = np.arange(1, len(metric_hist[\"loss_train\"]) + 1)\n",
    "  fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "  # - Plot train/val loss\n",
    "  ax1 = fig.add_subplot(1, 2, 1)\n",
    "  ax1.plot(epochs, metric_hist[\"loss_train\"], '-o', label='Train Loss')\n",
    "  ax1.plot(epochs, metric_hist[\"loss_val\"], '--<', label='Validation Loss')\n",
    "  ax1.set_title(\"Loss Over Epochs\", fontsize=14)\n",
    "  ax1.set_xlabel(\"Epoch\", fontsize=12)\n",
    "  ax1.set_ylabel(\"Loss\", fontsize=12)\n",
    "  ax1.legend(fontsize=11)\n",
    "  ax1.grid(True) \n",
    "    \n",
    "  # - Plot acc/f1score\n",
    "  ax2 = fig.add_subplot(1, 2, 2)\n",
    "  ax2.set_ylim(0, 1)\n",
    "  ax2.plot(epochs, metric_hist[\"acc_train\"], '-o', label='Train Accuracy')\n",
    "  ax2.plot(epochs, metric_hist[\"acc_val\"], '--<', label='Validation Accuracy')\n",
    "  ax2.plot(epochs, metric_hist[\"f1score_train\"], '-*', label='Train F1-score')\n",
    "  ax2.plot(epochs, metric_hist[\"f1score_val\"], '-->', label='Validation F1-score')\n",
    "  ax2.set_title(\"Accuracy and F1-score\", fontsize=14)\n",
    "  ax2.set_xlabel(\"Epoch\", fontsize=12)\n",
    "  ax2.set_ylabel(\"Score\", fontsize=12)\n",
    "  ax2.legend(fontsize=11)\n",
    "  ax2.grid(True)\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "# - Print & plot metrics\n",
    "print(\"== Training Metrics ==\")\n",
    "#print(metric_hist)\n",
    "\n",
    "draw_metric_hist(metric_hist)\n",
    "\n",
    "# - Draw confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "metric_hist[\"cm_metric_train\"].plot(ax=ax)\n",
    "#fig_, ax_ = metric_hist[\"cm_metric_train\"][-1].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf82999",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usc8-ai-workshop",
   "language": "python",
   "name": "usc8-ai-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
