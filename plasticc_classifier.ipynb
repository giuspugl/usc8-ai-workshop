{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8db6bff7",
   "metadata": {},
   "source": [
    "# Outline\n",
    "This tutorial demonstrates how to build, train, and evaluate a Recurrent Neural Network (RNN) model for classifying astronomical time series data using PyTorch.\n",
    "\n",
    "- `Dataset`: PLAsTiCC — simulated time-series observations of astronomical transients using LSST-like photometric data, observed in five optical bands\n",
    "- `Model`: ResNet and a custom architecture\n",
    "- `Objective`: Classify input time series into one of 15 object transient classes\n",
    "\n",
    "The tutorial will guide you through the following steps:\n",
    "\n",
    "1) Setting up the environment\n",
    "2) Downloading the dataset and preparing data loaders with appropriate transformations and augmentations\n",
    "3) Building a RNN classifier with a custom architecture\n",
    "4) Training the model\n",
    "5) Evaluating the model on the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e050949",
   "metadata": {},
   "source": [
    "# Configuring the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ec8d7d",
   "metadata": {},
   "source": [
    "## Module installation\n",
    "We’ll begin by installing the necessary Python modules for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b966059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# - Install modules from requirements.txt if present\n",
    "if os.path.isfile(\"requirements_plasticc.txt\"):\n",
    "  print(\"Installing modules from local requirements_plasticc.txt file ...\")\n",
    "  %pip install -q -r requirements_plasticc.txt\n",
    "else:\n",
    "  print(\"Installing modules ...\")  \n",
    "\n",
    "  %pip install -q pandas                                                       # Data analysis modules                     \n",
    "  %pip install -q torch torchvision torchmetrics torchinfo torchtune torchao   # ML modules\n",
    "  %pip install -q sh gdown matplotlib tqdm                                     # Plot/util modules\n",
    "    \n",
    "  # - Create requirements file\n",
    "  %pip freeze > requirements_plasticc.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06a734e",
   "metadata": {},
   "source": [
    "## Import modules\n",
    "Next, we import the essential modules needed throughout the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be88dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "##   STANDARD MODULES\n",
    "###########################\n",
    "import os\n",
    "import math\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import gdown\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import json\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "from itertools import islice\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import urllib.request\n",
    "from sh import gunzip\n",
    "import copy\n",
    "\n",
    "###########################\n",
    "##   DATA/TORCH MODULES\n",
    "###########################\n",
    "# - Data analysis\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# - Torch modules\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, Subset, random_split, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchmetrics\n",
    "import torchinfo\n",
    "from torchinfo import summary\n",
    "from torchtune.training import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535f6790",
   "metadata": {},
   "source": [
    "## Project folders\n",
    "We create a working directory `rundir` to run the tutorial in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155fba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "topdir= os.getcwd()\n",
    "rundir= os.path.join(topdir, \"run-plasticc_classifier\")\n",
    "path = Path(rundir)\n",
    "path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1a621a",
   "metadata": {},
   "source": [
    "# 📚 Dataset\n",
    "For this tutorial, we will use the [**PLAsTiCC dataset**](https://plasticc.org) dataset.\n",
    "The PLAsTiCC (Photometric LSST Astronomical Time Series Classification Challenge) dataset provides simulated time-series observations of astronomical transients for classification tasks using LSST-like photometric data.\n",
    "It was originally part of a [**Kaggle competition**](https://www.kaggle.com/competitions/PLAsTiCC-2018/overview) in 2018. \n",
    "\n",
    "For this tutorial we are going to use the [**unblinded data collection**](https://zenodo.org/records/2539456), released after the competition closure, that provides classication information for the full test set (previously undisclosed to the participants).\n",
    "\n",
    "The dataset contains two types of files, in CSV format:\n",
    "\n",
    "- Metadata files: containing object identifiers, class type and physical parameters (e.g. redshift, etc)\n",
    "- Lightcurve files: containing time series of object fluxed and other variables per each observing band\n",
    "\n",
    "Below, we report some examples of light curve data for three different objects (taken from dataset description paper):\n",
    "\n",
    "<img src=\"media/plasticc_sample1.png\" style=\"display: block; margin-left: 0; width: 500px;\" />\n",
    "<img src=\"media/plasticc_sample2.png\" style=\"display: block; margin-left: 0; width: 500px;\" />\n",
    "<img src=\"media/plasticc_sample3.png\" style=\"display: block; margin-left: 0; width: 500px;\" />\n",
    "\n",
    "Each time series is labeled as one of the following 15 classes:\n",
    "\n",
    "<img src=\"media/plasticc_classes.jpg\" style=\"display: block; margin-left: 0; width: 800px;\" />\n",
    "<img src=\"media/plasticc_classes_counts.jpg\" style=\"display: block; margin-left: 0; width: 800px;\" />\n",
    "\n",
    "More details on the data format are provided below.\n",
    "\n",
    "\n",
    "## 🔸 Metadata Files\n",
    "\n",
    "Each row corresponds to a unique transient object and contains both observational and model parameters.\n",
    "\n",
    "- **File names:**\n",
    "  - `plasticc_train_metadata.csv.gz`\n",
    "  - `plasticc_test_metadata.csv.gz`\n",
    "\n",
    "- **Columns (subset):**\n",
    "  - `object_id`: Unique identifier\n",
    "  - `ra`, `decl`: Sky coordinates\n",
    "  - `ddf_bool`: Deep Drilling Field flag (1 = DDF, 0 = WFD)\n",
    "  - `hostgal_specz`: Spectroscopic redshift (partial)\n",
    "  - `hostgal_photoz`, `hostgal_photoz_err`: Photometric redshift and error\n",
    "  - `distmod`: Distance modulus\n",
    "  - `mwebv`: Galactic extinction\n",
    "  - `target`: Challenge class label (only in training set)\n",
    "  - `true_target`: Actual class label (post-challenge)\n",
    "  - `true_submodel`: Sub-model variant ID (for some classes)\n",
    "  - `true_z`, `true_distmod`, `true_lensdmu`: Redshift-related physical quantities\n",
    "  - `tflux_[u,g,r,i,z,y]`: Template fluxes in each LSST band\n",
    "\n",
    "## 🔸 Lightcurve Files\n",
    "\n",
    "Contain photometric time-series data for each object. Each row corresponds to one measurement at a given time and band.\n",
    "\n",
    "- **Training file:**\n",
    "  - `plasticc_training_lightcurves.csv`\n",
    "\n",
    "- **Test files (split into 11 subsets):**\n",
    "  - `plasticc_test_lightcurves_01.csv.gz` to `plasticc_test_lightcurves_11.csv.gz`\n",
    "\n",
    "- **Columns:**\n",
    "  - `object_id`: Match to metadata\n",
    "  - `mjd`: Observation time (Modified Julian Date)\n",
    "  - `passband`: LSST filter index (0 = u, ..., 5 = y)\n",
    "  - `flux`: Observed flux (corrected for Galactic extinction)\n",
    "  - `flux_err`: Flux uncertainty\n",
    "  - `detected_bool`: Detection flag from image subtraction\n",
    "  \n",
    " ## 🔸 References\n",
    " More details are available in the these references:\n",
    " \n",
    " - `https://plasticc.org`\n",
    "- `https://www.kaggle.com/competitions/PLAsTiCC-2018/overview`\n",
    "- `R. Hložek et al, 2023, ApJS, 267, 25`\n",
    "- `https://arxiv.org/pdf/1809.11145`\n",
    "- `https://arxiv.org/pdf/1810.00001`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08970dd9",
   "metadata": {},
   "source": [
    "## Dataset Download\n",
    "Next, we download the dataset from Google Drive and unzip it in the main folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1daff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(url, data_path, destdir):\n",
    "  \"\"\" Download data \"\"\"\n",
    "  data_fullpath= os.path.join(destdir, data_path)\n",
    "    \n",
    "  print(\"Downloading file from url %s ...\" % (url))\n",
    "  urllib.request.urlretrieve(url, data_path)  \n",
    "  print(\"DONE!\")\n",
    "\n",
    "  print(\"Moving file %s to dir %s ...\" % (data_path, destdir))\n",
    "  shutil.move(data_path, destdir)\n",
    "\n",
    "  print(\"Unzipping dataset file %s ...\" % (data_fullpath))\n",
    "  gunzip(data_fullpath)\n",
    "\n",
    "\n",
    "# - Download train metadata\n",
    "train_metadata_url= \"https://zenodo.org/records/2539456/files/plasticc_train_metadata.csv.gz?download=1\"\n",
    "train_metadata_gz_path= 'plasticc_train_metadata.csv.gz'\n",
    "train_metadata_gz_fullpath= os.path.join(rundir, train_metadata_gz_path)\n",
    "train_metadata_fullpath= os.path.join(rundir, 'plasticc_train_metadata.csv')\n",
    "if not os.path.isfile(train_metadata_fullpath):\n",
    "  download_data(train_metadata_url, train_metadata_gz_path, rundir)\n",
    "\n",
    "# - Download train data\n",
    "train_data_url= \"https://zenodo.org/records/2539456/files/plasticc_train_lightcurves.csv.gz?download=1\"\n",
    "train_data_gz_path= 'plasticc_train_lightcurves.csv.gz'\n",
    "train_data_gz_fullpath= os.path.join(rundir, train_data_gz_path)\n",
    "train_data_fullpath= os.path.join(rundir, 'plasticc_train_lightcurves.csv')\n",
    "if not os.path.isfile(train_data_fullpath):\n",
    "  download_data(train_data_url, train_data_gz_path, rundir)\n",
    "\n",
    "# - Download test metadata\n",
    "test_metadata_url= \"https://zenodo.org/records/2539456/files/plasticc_test_metadata.csv.gz?download=1\"\n",
    "test_metadata_gz_path= 'plasticc_test_metadata.csv.gz'\n",
    "test_metadata_gz_fullpath= os.path.join(rundir, test_metadata_gz_path)\n",
    "test_metadata_fullpath= os.path.join(rundir, 'plasticc_test_metadata.csv')\n",
    "if not os.path.isfile(test_metadata_fullpath):\n",
    "  download_data(test_metadata_url, test_metadata_gz_path, rundir)\n",
    "\n",
    "# - Download test data (part 1)\n",
    "test_data_url= \"https://zenodo.org/records/2539456/files/plasticc_test_lightcurves_01.csv.gz?download=1\"\n",
    "test_data_gz_path= 'plasticc_test_lightcurves_01.csv.gz'\n",
    "test_data_gz_fullpath= os.path.join(rundir, test_data_gz_path)\n",
    "test_data_fullpath= os.path.join(rundir, 'plasticc_test_lightcurves_01.csv')\n",
    "if not os.path.isfile(test_data_fullpath):\n",
    "  download_data(test_data_url, test_data_gz_path, rundir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64be565b",
   "metadata": {},
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5bd5c2",
   "metadata": {},
   "source": [
    "### Loading train metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d6ca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Read train metadata as panda data frame\n",
    "train_metadata= pd.read_csv(train_metadata_fullpath)\n",
    "print(\"--> Train metadata\")\n",
    "print(train_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f17e47",
   "metadata": {},
   "source": [
    "### Loading train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6279c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Load data\n",
    "print(f\"Loading train data from file {train_data_fullpath} ...\")\n",
    "train_data = pd.read_csv(train_data_fullpath)\n",
    "print(\"train_data\")\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3130c327",
   "metadata": {},
   "source": [
    "### Splitting train/val sets\n",
    "Let's reserve a small portion (10%) of the training dataset for validation scopes. Below, we split the original training dataset into train and validation data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd42252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Split metadata in train/val sets\n",
    "random_state= 42\n",
    "test_size= 0.1\n",
    "meta_df_train, meta_df_val = train_test_split(\n",
    "  train_metadata,\n",
    "  test_size=test_size,\n",
    "  random_state=random_state,\n",
    "  shuffle=True\n",
    ")\n",
    "\n",
    "# - Get object_ids of train/test splits\n",
    "train_object_ids = meta_df_train['object_id'].unique()\n",
    "val_object_ids = meta_df_val['object_id'].unique()\n",
    "\n",
    "# - Use object IDs to split data_df\n",
    "data_df_train = train_data[train_data['object_id'].isin(train_object_ids)]\n",
    "data_df_val  = train_data[train_data['object_id'].isin(val_object_ids)]\n",
    "\n",
    "print(f\"#{len(meta_df_train)}/{len(meta_df_val)} data entries in train/val sets ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f33979d",
   "metadata": {},
   "source": [
    "### Loading test metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008fd083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Read train metadata as panda data frame\n",
    "meta_df_test= pd.read_csv(test_metadata_fullpath)\n",
    "print(\"--> Test metadata\")\n",
    "print(meta_df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def7ec47",
   "metadata": {},
   "source": [
    "### Loading test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15ed83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Load data\n",
    "print(f\"Loading test data from file {test_data_fullpath} ...\")\n",
    "data_df_test = pd.read_csv(test_data_fullpath)\n",
    "print(\"test_data\")\n",
    "print(data_df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e7be3b",
   "metadata": {},
   "source": [
    "### Create PyTorch datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6038d6b",
   "metadata": {},
   "source": [
    "Define PyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6425cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences_numpy(sequences, maxlen=None, dtype='float32', padding='post', truncating='post', value=0.0):\n",
    "  \"\"\"\n",
    "    Pads a list of 2D numpy arrays (sequence_len_i, n_features) to shape (N, maxlen, n_features).\n",
    "    \n",
    "    Parameters:\n",
    "        sequences : list of np.ndarray of shape (Ti, D)\n",
    "        maxlen    : int or None, length to pad/truncate to. If None, use max sequence length.\n",
    "        dtype     : data type of output array\n",
    "        padding   : 'pre' or 'post'\n",
    "        truncating: 'pre' or 'post'\n",
    "        value     : value used for padding\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray of shape (N, maxlen, D)\n",
    "  \"\"\"\n",
    "  num_samples = len(sequences)\n",
    "  feature_dim = sequences[0].shape[1]\n",
    "  lengths = [seq.shape[0] for seq in sequences]\n",
    "\n",
    "  if maxlen is None:\n",
    "    maxlen = max(lengths)\n",
    "\n",
    "  padded = np.full((num_samples, maxlen, feature_dim), value, dtype=dtype)\n",
    "\n",
    "  for i, seq in enumerate(sequences):\n",
    "    if truncating == 'pre':\n",
    "      trunc = seq[-maxlen:]\n",
    "    else:\n",
    "      trunc = seq[:maxlen]\n",
    "\n",
    "    if padding == 'pre':\n",
    "      padded[i, -len(trunc):] = trunc\n",
    "    else:\n",
    "      padded[i, :len(trunc)] = trunc\n",
    "\n",
    "  return padded\n",
    "\n",
    "\n",
    "def pad_single_sequence_numpy(sequence, maxlen, dtype='float32', padding='post', truncating='post', value=0.0):\n",
    "    \"\"\"\n",
    "    Pads or truncates a 2D array (seq_len, n_features) to shape (maxlen, n_features).\n",
    "\n",
    "    Parameters:\n",
    "        sequence  : np.ndarray of shape (seq_len, n_features)\n",
    "        maxlen    : int, the target sequence length\n",
    "        dtype     : data type of the output array\n",
    "        padding   : 'pre' or 'post' — where to pad (default: 'post')\n",
    "        truncating: 'pre' or 'post' — where to truncate (default: 'post')\n",
    "        value     : value to use for padding\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray of shape (maxlen, n_features)\n",
    "    \"\"\"\n",
    "    seq_len, n_features = sequence.shape\n",
    "    result = np.full((maxlen, n_features), value, dtype=dtype)\n",
    "\n",
    "    if seq_len >= maxlen:\n",
    "        if truncating == 'pre':\n",
    "            trunc = sequence[-maxlen:]\n",
    "        else:  # 'post'\n",
    "            trunc = sequence[:maxlen]\n",
    "    else:\n",
    "        trunc = sequence\n",
    "\n",
    "    if padding == 'pre':\n",
    "        result[-len(trunc):] = trunc\n",
    "    else:\n",
    "        result[:len(trunc)] = trunc\n",
    "\n",
    "    return result\n",
    "\n",
    "#####################################\n",
    "##      DATASET\n",
    "#####################################\n",
    "class PlasticcDataset(Dataset):\n",
    "  def __init__(\n",
    "    self,\n",
    "    data_df, # Pandas data frame\n",
    "    meta_df, # Pandas data frame\n",
    "    use_specz=False,\n",
    "    extragalactic=None,  \n",
    "    nmax=-1,\n",
    "    max_seq_size=256,\n",
    "    auto_set_max_seq_size=True,\n",
    "    do_augmentation=False, # replace original data with augmented version with augment_prob\n",
    "    augment_prob= 0.5,  \n",
    "    augment_data=False, # increase size of original size by augment_factor\n",
    "    augment_factor=25,\n",
    "    drop_rate= 0.3\n",
    "  ):\n",
    "    # - Set options\n",
    "    self.meta_df= meta_df\n",
    "    self.data_df= data_df\n",
    "    self.metadata= []\n",
    "    self.data= []\n",
    "    self.X_seq = None    # shape: (N, seq_len, 4)\n",
    "    self.X_meta = None   # shape: (N, num_features)\n",
    "    self.Y= None  # one-hot labels: (N, num_classes)\n",
    "    self.Y_target= None # labels: (N, 1)\n",
    "    self.wtable= None\n",
    "    self.class_weights= None\n",
    "    self.nmax= nmax\n",
    "    self.use_specz= use_specz\n",
    "    self.extragalactic= extragalactic\n",
    "    self.classes = np.array([6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95, 99], dtype='int32')\n",
    "    self.class_names = ['class_6','class_15','class_16','class_42','class_52','class_53','class_62','class_64','class_65','class_67','class_88','class_90','class_92','class_95','class_99']\n",
    "    self.classid2label= {\n",
    "      6: \"PS-MULENS\",# Point_source_mu-lensing\n",
    "      15: \"TDE\", # Tidal disruption event\n",
    "      16: \"EBE\", # Eclipsing binary event\n",
    "      42: \"SN-II\", # Core-collapse supernova Type II\n",
    "      52: \"SN-Iax\", # Supernova Type Ia-x\n",
    "      53: \"MIRA\", # Mira variable\n",
    "      62: \"SN-Ibc\", # Core-collapse supernova Type Ibc\n",
    "      64: \"KN\", # Kilonova\n",
    "      65: \"M-DWARF\", # M dwarf\n",
    "      67: \"SN-Ia-91bg\", # Supernova Type Ia-91bg\n",
    "      88: \"AGN\", # Active galactic nucleus\n",
    "      90: \"SN-Ia\", # Supernova Type Ia\n",
    "      92: \"RR-LY\", # RR Lyrae\n",
    "      95: \"SLSN\", # Superluminous supernova\n",
    "      99: \"OTHER\", # Other class\n",
    "    }\n",
    "    \n",
    "    self.classid_remap= {\n",
    "      6: 6,# Point_source_mu-lensing\n",
    "      15: 15, # Tidal disruption event\n",
    "      16: 16, # Eclipsing binary event\n",
    "      42: 42, # Core-collapse supernova Type II\n",
    "      52: 52, # Supernova Type Ia-x\n",
    "      53: 53, # Mira variable\n",
    "      62: 62, # Core-collapse supernova Type Ibc\n",
    "      64: 64, # Kilonova\n",
    "      65: 65, # M dwarf\n",
    "      67: 67, # Supernova Type Ia-91bg\n",
    "      88: 88, # Active galactic nucleus\n",
    "      90: 90, # Supernova Type Ia\n",
    "      92: 92, # RR Lyrae\n",
    "      95: 95, # Superluminous supernova\n",
    "      991: 99, # Microlens from binary lens\n",
    "      992: 99, # Intermediate luminous optical transient\n",
    "      993: 99, # Calcium-rich transient\n",
    "      994: 99, # Pair instability SN\n",
    "    }    \n",
    "        \n",
    "    self.nclasses= len(self.classes)\n",
    "    self.class_weight_factors= np.array([2,2,1,1,1,1,1,2,1,1,1,1,1,2,2], dtype='float32')\n",
    "    \n",
    "    # LSST passbands (nm)  u    g    r    i    z    y      \n",
    "    self.passbands = np.array([357, 477, 621, 754, 871, 1004], dtype='float32')\n",
    "    \n",
    "    self.max_seq_size= max_seq_size\n",
    "    self.auto_set_max_seq_size= auto_set_max_seq_size\n",
    "    self.seq_sizes= []\n",
    "    \n",
    "    self.augment_data= augment_data\n",
    "    self.augment_factor= augment_factor\n",
    "    self.drop_rate = drop_rate\n",
    "    self.do_augmentation= do_augmentation\n",
    "    self.augment_prob= augment_prob\n",
    "    \n",
    "    # - Read data\n",
    "    self.__read_data()\n",
    "    \n",
    "    # - Load data\n",
    "    self.__load_data()\n",
    "    \n",
    "  def __compute_wtable(self):\n",
    "    \"\"\"\n",
    "      Compute:\n",
    "        - wtable: class frequencies (N_class,)\n",
    "        - class_weights: inverse frequency, normalized to sum to num_classes\n",
    "\n",
    "      Returns:\n",
    "        wtable (torch.Tensor), class_weights (torch.Tensor)\n",
    "    \"\"\"\n",
    "    class_counts = self.Y.sum(dim=0)  # sum over all samples (across rows)\n",
    "    total_samples = self.Y.shape[0]\n",
    "    wtable = class_counts / total_samples\n",
    "    wtable[self.nclasses-1]= 1.0\n",
    "    \n",
    "    # Inverse frequency as weight (avoid divide-by-zero)\n",
    "    class_weights = 1.0 / (wtable + 1e-8)\n",
    "    class_weights = class_weights * (len(wtable) / class_weights.sum())  # normalize to mean 1\n",
    "\n",
    "    return wtable, class_weights\n",
    "\n",
    "  def __get_sequence_data(self, inputdata, inputmetadata, augment=False):\n",
    "    \"\"\" Return sequence data from raw plasticc data \"\"\"    \n",
    "\n",
    "    # - Copy input data\n",
    "    data= copy.deepcopy(inputdata)\n",
    "    meta= copy.deepcopy(inputmetadata)\n",
    "    \n",
    "    # - Retrieve original meta data\n",
    "    z_photo= meta['hostgal_photoz'].iloc[0]         # photometric host-redshift (float32)\n",
    "    zerr_photo= meta['hostgal_photoz_err'].iloc[0]  # uncertainty on photometric host-redshift\n",
    "    ddf= meta['ddf_bool'].iloc[0]                   # boolean flag: 1 for DDF, 0 for WFD\n",
    "    mwebv= meta['mwebv'].iloc[0]                    # Galactic E(B-V) extinction\n",
    "    z_spec= meta['hostgal_specz'].iloc[0]           # accurate spectroscopic-redshift for small subset\n",
    "    z= z_spec if self.use_specz else z_photo\n",
    "    z_err= 0.0 if self.use_specz else zerr_photo\n",
    "    \n",
    "    # - Retrieve original seq data \n",
    "    mjd      = np.array(data['mjd'],      dtype='float32')\n",
    "    band     = np.array(data['passband'], dtype='int32')\n",
    "    flux     = np.array(data['flux'],     dtype='float32')\n",
    "    flux_err = np.array(data['flux_err'], dtype='float32')\n",
    "    detected = np.array(data['detected_bool'], dtype='float32')\n",
    "    seq_size= mjd.shape[0]\n",
    "    \n",
    "    # - Augment features by dropping some records?\n",
    "    if augment:\n",
    "      # - Drop randomly some sequency observations  \n",
    "      mjd_aug= []\n",
    "      band_aug= []  \n",
    "      flux_aug= []\n",
    "      flux_err_aug= []\n",
    "      for k in range(seq_size):\n",
    "        if random.uniform(0, 1) >= self.drop_rate:\n",
    "          mjd_aug.append(mjd[k])\n",
    "          band_aug.append(band[k])\n",
    "          flux_aug.append(flux[k])\n",
    "          flux_err_aug.append(flux_err[k])\n",
    "       \n",
    "      mjd_aug= np.array(mjd_aug, dtype='float32')\n",
    "      band_aug= np.array(band_aug, dtype='int32')\n",
    "      flux_aug= np.array(flux_aug, dtype='float32')\n",
    "      flux_err_aug= np.array(flux_err_aug, dtype='float32') \n",
    "      \n",
    "      # - Randomize redshift\n",
    "      z_aug = random.normalvariate(z, z_err / 1.5)\n",
    "      z_aug = max(z_aug, 0)\n",
    "      z_aug = min(z_aug, 5)\n",
    "      \n",
    "      # - Randomize flux\n",
    "      flux_aug = np.random.normal(flux_aug, flux_err_aug / 1.5)\n",
    "    \n",
    "      # - Override original data with augmented versions\n",
    "      z= z_aug\n",
    "      mjd= mjd_aug\n",
    "      band= band_aug  \n",
    "      flux= flux_aug\n",
    "      flux_err= flux_err_aug\n",
    "        \n",
    "    # - Scale/modify feature data\n",
    "    #   1) Convert time at observer (t) to time at source (t0): t0= t/(1+z)\n",
    "    #   2) Convert wavelength at observer (l) to wavelength at source (l0): l0=l/(1+z)\n",
    "    seq_size= mjd.shape[0]\n",
    "    mjd -= mjd[0]\n",
    "    mjd /= 100 # Earth time shift in day*100\n",
    "    mjd /= (z + 1) # Object time shift in day*100\n",
    "    tdiff= np.ediff1d(mjd, to_begin = [0])\n",
    "    flux_max = np.max(flux)\n",
    "    flux_min = np.min(flux)\n",
    "    flux_norm= flux_max - flux_min \n",
    "    flux_pow = math.log2(flux_norm)   \n",
    "    received_wavelength = self.passbands[band] # Earth wavelength in nm\n",
    "    source_wavelength = received_wavelength / (z + 1) # Object wavelength in nm\n",
    "    source_wavelength/= 1000.\n",
    "    \n",
    "    # - Set sequence data features\n",
    "    features_seq= np.zeros( (seq_size, 4), dtype = 'float32')\n",
    "    features_seq[:,0]= tdiff\n",
    "    features_seq[:,1]= flux/flux_norm\n",
    "    features_seq[:,2]= flux_err/flux_norm\n",
    "    features_seq[:,3]= source_wavelength\n",
    "    ##features_seq[:,4]= detected\n",
    "       \n",
    "    # - Set metadata features\n",
    "    features_meta= np.zeros(5, dtype = 'float32')\n",
    "    features_meta[0]= ddf\n",
    "    features_meta[1]= z\n",
    "    features_meta[2]= z_err\n",
    "    features_meta[3]= mwebv\n",
    "    features_meta[4]= flux_pow / 10\n",
    "    \n",
    "    # - Check for NaNs\n",
    "    if np.any(np.isnan(features_seq)):\n",
    "      print(\"WARN: features_seq has NaNs values!\")   \n",
    "    if np.any(np.isnan(features_meta)):\n",
    "      print(\"WARN: features_meta has NaNs values!\") \n",
    "    \n",
    "    return features_seq, features_meta\n",
    "    \n",
    "    \n",
    "  def __read_data(self):\n",
    "    \"\"\" Read data/metadata from input data frames \"\"\"\n",
    "    \n",
    "    # - Group data by object_id\n",
    "    groups= self.data_df.groupby('object_id')\n",
    "    print(f\"Reading {len(groups)} data entries ...\")\n",
    "    \n",
    "    # - Read data and apply selection\n",
    "    self.metadata= []\n",
    "    self.data= []\n",
    "                       \n",
    "    for idx, g in enumerate(groups):\n",
    "      nentries= idx+1  \n",
    "      if idx % 1000 == 0:\n",
    "        print('Reading data {0}'.format(idx), end='\\r') \n",
    "        \n",
    "      if self.nmax!=-1 and nentries >= self.nmax:\n",
    "        print(f'Reached data sample limit {self.nmax}...stop reading data')  \n",
    "        break\n",
    "        \n",
    "      # - Find data with object_id\n",
    "      id = g[0]\n",
    "      data_entry= g[1]\n",
    "      meta = self.meta_df.loc[self.meta_df['object_id'] == id]  \n",
    "      z_photo= meta['hostgal_photoz'].iloc[0] \n",
    "    \n",
    "      # - Skip source with invalid redshift?\n",
    "      if self.extragalactic == True and z_photo==0:\n",
    "        continue\n",
    "\n",
    "      if self.extragalactic == False and z_photo>0:\n",
    "        continue\n",
    "        \n",
    "      # - Store selected data\n",
    "      self.metadata.append(meta)\n",
    "      self.data.append(data_entry)\n",
    "    \n",
    "    print(f'{len(self.data)} data samples read...')  \n",
    "    \n",
    "    \n",
    "  def __load_data_item(self, idx, augment=False, random_augment=False):  \n",
    "    \"\"\" Load data item \"\"\"\n",
    "    \n",
    "    meta_item= self.metadata[idx]\n",
    "    data_item= self.data[idx]\n",
    "    \n",
    "    # - Get target id\n",
    "    if 'target' in meta_item:\n",
    "      class_id= int(meta_item['target'].iloc[0])\n",
    "      if class_id==0:\n",
    "        class_id= int(meta_item['true_target'].iloc[0])\n",
    "      class_id= self.classid_remap[class_id] # remap id (done for the 99x other classes)\n",
    "      target_id= np.where(self.classes == class_id)[0][0]\n",
    "    else:\n",
    "      target_id= len(self.classes) - 1  # interpret as class 99\n",
    "    \n",
    "    # - Get sequence & meta pars\n",
    "    augment_sample= False\n",
    "    if augment:\n",
    "      if random_augment:\n",
    "        augment_sample= (random.uniform(0, 1) >= self.augment_prob)\n",
    "      else:\n",
    "        augment_sample= True     \n",
    "    features_seq, features_meta= self.__get_sequence_data(data_item, meta_item, augment=augment_sample)\n",
    "    seq_size= features_seq.shape[0]\n",
    "    \n",
    "    # - Pad sequence data to desired max length\n",
    "    features_seq_padded= pad_single_sequence_numpy(features_seq, maxlen=self.max_seq_size)\n",
    "    \n",
    "    return features_seq_padded, features_meta, target_id\n",
    "    \n",
    "    \n",
    "  def __load_data(self):  \n",
    "    \"\"\" Load data/metadata from files \"\"\"\n",
    "    \n",
    "    features_seq_all= []\n",
    "    features_meta_all= []\n",
    "    target_ids= []\n",
    "    \n",
    "    for i in range(len(self.data)):\n",
    "      meta_item= self.metadata[i]\n",
    "      data_item= self.data[i]\n",
    "      \n",
    "      # - Get target id\n",
    "      if 'target' in meta_item:\n",
    "        class_id= int(meta_item['target'].iloc[0])\n",
    "        if class_id==0:\n",
    "          class_id= int(meta_item['true_target'].iloc[0])\n",
    "        class_id= self.classid_remap[class_id] # remap id (done for the 99x other classes)\n",
    "        target_id= np.where(self.classes == class_id)[0][0]\n",
    "      else:\n",
    "        target_id= len(self.classes) - 1  # interpret as class 99\n",
    "      \n",
    "      # - Read original data \n",
    "      features_seq, features_meta= self.__get_sequence_data(data_item, meta_item, augment=False)\n",
    "      seq_size= features_seq.shape[0]\n",
    "      features_seq_all.append(features_seq)\n",
    "      features_meta_all.append(features_meta)\n",
    "      self.seq_sizes.append(seq_size)\n",
    "      target_ids.append(target_id) \n",
    "    \n",
    "      # - Augment data, e.g. produce new samples by randomly dropping observations\n",
    "      if self.augment_data:\n",
    "        for j in range(self.augment_factor):\n",
    "          features_seq_aug, features_meta_aug= self.__get_sequence_data(data_item, meta_item, augment=True)\n",
    "          seq_size_aug= features_seq_aug.shape[0]\n",
    "          features_seq_all.append(features_seq_aug)\n",
    "          features_meta_all.append(features_meta_aug)\n",
    "          self.seq_sizes.append(seq_size_aug)\n",
    "          target_ids.append(target_id) \n",
    "       \n",
    "    seq_min_size= np.min(self.seq_sizes)\n",
    "    seq_max_size= np.max(self.seq_sizes)\n",
    "    print(f\"#{len(features_seq_all)} data added with seq range ({seq_min_size}, {seq_max_size})...\")\n",
    "    \n",
    "    # - Find sequence truncation point (assuming a power of 2 larger than max seq size) \n",
    "    if self.auto_set_max_seq_size:\n",
    "      seq_size_opt= 2 ** math.ceil(math.log2(seq_max_size))\n",
    "    else:\n",
    "      seq_size_opt= self.max_seq_size\n",
    "    \n",
    "    print(f\"Padding sequence data to a size of {seq_size_opt} ...\")\n",
    "    features_seq_padded= pad_sequences_numpy(features_seq_all, maxlen=seq_size_opt)\n",
    "    print(\"features_seq_padded.shape\")\n",
    "    print(features_seq_padded.shape)\n",
    "    \n",
    "    # - Convert data to tensors\n",
    "    self.X_seq= torch.from_numpy(features_seq_padded)\n",
    "    self.X_meta= torch.from_numpy(np.array(features_meta_all))\n",
    "    self.Y_target= torch.from_numpy(np.array(target_ids))\n",
    "    #self.Y= F.one_hot(self.Y_target, num_classes=-1)\n",
    "    self.Y= F.one_hot(self.Y_target, num_classes=len(self.classes))\n",
    "    \n",
    "    print(\"self.X_seq.shape\")\n",
    "    print(self.X_seq.shape)\n",
    "    print(\"self.X_meta.shape\")\n",
    "    print(self.X_meta.shape)\n",
    "    print(\"self.Y.shape\")\n",
    "    print(self.Y.shape)\n",
    "    \n",
    "    # - Compute wtable\n",
    "    self.wtable, self.class_weights= self.__compute_wtable()\n",
    "    print(\"self.wtable\")\n",
    "    print(self.wtable)\n",
    "    print(\"self.class_weights\")\n",
    "    print(self.class_weights)\n",
    "    \n",
    "    \n",
    "  def __load_data_old(self):\n",
    "    \"\"\" Load data/metadata from files \"\"\"      \n",
    "\n",
    "    # - Group data by object_id\n",
    "    groups = self.data_df.groupby('object_id')\n",
    "    print(f\"Reading {len(groups)} data entries ...\")\n",
    "    \n",
    "    features_seq_all= []\n",
    "    features_meta_all= []\n",
    "    target_ids= []\n",
    "    seq_min_size= 1.e+99\n",
    "    seq_max_size= -1\n",
    "    \n",
    "    for idx, g in enumerate(groups):\n",
    "      # - Check max number of reads  \n",
    "      nentries= idx+1  \n",
    "      if idx % 1000 == 0:\n",
    "        print('Converting data {0}'.format(idx), end='\\r') \n",
    "        \n",
    "      if self.nmax!=-1 and nentries >= self.nmax:\n",
    "        print(f'Reached data sample limit {self.nmax}...stop reading data')  \n",
    "        break  \n",
    "        \n",
    "      # - Find data with object_id\n",
    "      id = g[0]\n",
    "      meta = self.meta_df.loc[self.meta_df['object_id'] == id]\n",
    "      \n",
    "      z_photo= meta['hostgal_photoz'].iloc[0]         # photometric host-redshift (float32)\n",
    "      #zerr_photo= meta['hostgal_photoz_err'].iloc[0]  # uncertainty on photometric host-redshift\n",
    "      #ddf= meta['ddf_bool'].iloc[0]                   # boolean flag: 1 for DDF, 0 for WFD\n",
    "      #mwebv= meta['mwebv'].iloc[0]                    # Galactic E(B-V) extinction\n",
    "      #z_spec= meta['hostgal_specz'].iloc[0]           # accurate spectroscopic-redshift for small subset\n",
    "      #z= z_spec if self.use_specz else z_photo\n",
    "      #z_err= 0.0 if self.use_specz else zerr_photo\n",
    "    \n",
    "      # - Skip source with invalid redshift?\n",
    "      if self.extragalactic == True and z_photo==0:\n",
    "        continue\n",
    "\n",
    "      if self.extragalactic == False and z_photo>0:\n",
    "        continue\n",
    "\n",
    "      # - Get target id\n",
    "      if 'target' in meta:\n",
    "        class_id= int(meta['target'].iloc[0])\n",
    "        if class_id==0:\n",
    "          class_id= int(meta['true_target'].iloc[0])\n",
    "        class_id= self.classid_remap[class_id] # remap id (done for the 99x other classes)\n",
    "        #print(f\"class_id={class_id}\")\n",
    "        target_id= np.where(self.classes == class_id)[0][0]\n",
    "      else:\n",
    "        target_id= len(self.classes) - 1  # interpret as class 99\n",
    "        \n",
    "      target_ids.append(target_id)\n",
    "        \n",
    "      # - Get sequence & meta pars\n",
    "      features_seq, features_meta= self.__get_sequence_data(g[1], meta, augment=False)\n",
    "      seq_size= features_seq.shape[0]\n",
    "      features_seq_all.append(features_seq)\n",
    "      features_meta_all.append(features_meta)\n",
    "      self.seq_sizes.append(seq_size)\n",
    "        \n",
    "      # - Augment data, e.g. produce new samples by randomly dropping observations\n",
    "      if self.augment_data:\n",
    "        for j in range(self.augment_factor):\n",
    "          features_seq_aug, features_meta_aug= self.__get_sequence_data(g[1], meta, augment=True)\n",
    "          seq_size_aug= features_seq_aug.shape[0]\n",
    "          features_seq_all.append(features_seq_aug)\n",
    "          features_meta_all.append(features_meta_aug)\n",
    "          self.seq_sizes.append(seq_size_aug)\n",
    "          target_ids.append(target_id)\n",
    "      \n",
    "    \n",
    "    seq_min_size= np.min(self.seq_sizes)\n",
    "    seq_max_size= np.max(self.seq_sizes)\n",
    "    print(f\"#{len(features_seq_all)} data added with seq range ({seq_min_size}, {seq_max_size})...\")\n",
    "    \n",
    "    # - Find sequence truncation point (assuming a power of 2 larger than max seq size) \n",
    "    if self.auto_set_max_seq_size:\n",
    "      seq_size_opt= 2 ** math.ceil(math.log2(seq_max_size))\n",
    "    else:\n",
    "      seq_size_opt= self.max_seq_size\n",
    "    \n",
    "    print(f\"Padding sequence data to a size of {seq_size_opt} ...\")\n",
    "    features_seq_padded= pad_sequences_numpy(features_seq_all, maxlen=seq_size_opt)\n",
    "    print(\"features_seq_padded.shape\")\n",
    "    print(features_seq_padded.shape)\n",
    "    \n",
    "    # - Convert data to tensors\n",
    "    self.X_seq= torch.from_numpy(features_seq_padded)\n",
    "    self.X_meta= torch.from_numpy(np.array(features_meta_all))\n",
    "    self.Y_target= torch.from_numpy(np.array(target_ids))\n",
    "    #self.Y= F.one_hot(self.Y_target, num_classes=-1)\n",
    "    self.Y= F.one_hot(self.Y_target, num_classes=len(self.classes))\n",
    "    \n",
    "    print(\"self.X_seq.shape\")\n",
    "    print(self.X_seq.shape)\n",
    "    print(\"self.X_meta.shape\")\n",
    "    print(self.X_meta.shape)\n",
    "    print(\"self.Y.shape\")\n",
    "    print(self.Y.shape)\n",
    "    \n",
    "    # - Compute wtable\n",
    "    self.wtable, self.class_weights= self.__compute_wtable()\n",
    "    print(\"self.wtable\")\n",
    "    print(self.wtable)\n",
    "    print(\"self.class_weights\")\n",
    "    print(self.class_weights)\n",
    "    \n",
    "    \n",
    "  def __len__(self):\n",
    "    #return len(self.X_seq)\n",
    "    return len(self.data) \n",
    "    \n",
    "  def __getitem__(self, idx):\n",
    "    #return self.X_seq[idx], self.X_meta[idx], self.Y[idx]\n",
    "    \n",
    "    # - Load item\n",
    "    features_seq, features_meta, target_id= self.__load_data_item(\n",
    "      idx, \n",
    "      augment=self.do_augmentation, \n",
    "      random_augment=True\n",
    "    )\n",
    "    \n",
    "    # - Convert data to tensors\n",
    "    X_seq= torch.from_numpy(features_seq)\n",
    "    X_meta= torch.from_numpy(np.array(features_meta))\n",
    "    Y_target= torch.from_numpy(np.array(target_id))\n",
    "    #Y= F.one_hot(Y_target, num_classes=-1)\n",
    "    Y= F.one_hot(Y_target, num_classes=len(self.classes))\n",
    "    \n",
    "    return X_seq, X_meta, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f281eec9",
   "metadata": {},
   "source": [
    "Create train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb46e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train= PlasticcDataset(\n",
    "  data_df_train, \n",
    "  meta_df_train,\n",
    "  use_specz=False,\n",
    "  extragalactic=None,  \n",
    "  nmax=-1,\n",
    "  max_seq_size=352,\n",
    "  auto_set_max_seq_size=False,\n",
    "  do_augmentation=True,\n",
    "  augment_prob= 0.5,  \n",
    "  augment_data=False,\n",
    "  augment_factor=25,\n",
    "  drop_rate= 0.3\n",
    ")\n",
    "\n",
    "#print(\"--> wtable\")\n",
    "#print(wtable)\n",
    "#print(\"--> wtable (dataset)\")\n",
    "#print(dataset_train.wtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9829745",
   "metadata": {},
   "source": [
    "Plot the distribution of time series lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c89b01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lengths= dataset_train.seq_sizes\n",
    "targets= dataset_train.Y_target\n",
    "seq_min_size= np.min(seq_lengths)\n",
    "seq_max_size= np.max(seq_lengths)\n",
    "seq_mean_size= np.mean(seq_lengths)\n",
    "seq_median_size= np.median(seq_lengths)\n",
    "seq_std_size= np.std(seq_lengths)\n",
    "print(f\"seq stats: min/max={seq_min_size}/{seq_max_size}, mean/median={seq_mean_size}/{seq_median_size}, std={seq_std_size}\")\n",
    "\n",
    "plt.scatter(targets, seq_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de27189c",
   "metadata": {},
   "source": [
    "Load validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fa7f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val= PlasticcDataset(\n",
    "  data_df_val, \n",
    "  meta_df_val,\n",
    "  use_specz=False,\n",
    "  extragalactic=None,  \n",
    "  nmax=-1,\n",
    "  max_seq_size=352,\n",
    "  auto_set_max_seq_size=False,\n",
    "  do_augmentation=False,  \n",
    "  augment_prob=0.5,  \n",
    "  augment_data=False,\n",
    "  augment_factor=1,\n",
    "  drop_rate= 0.3  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4524eb",
   "metadata": {},
   "source": [
    "Load test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22177760",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test= PlasticcDataset(\n",
    "  data_df_test, \n",
    "  meta_df_test,\n",
    "  use_specz=False,\n",
    "  extragalactic=None,  \n",
    "  nmax=-1,\n",
    "  max_seq_size=352,\n",
    "  auto_set_max_seq_size=False,\n",
    "  do_augmentation=False,\n",
    "  augment_prob=0.5,\n",
    "  augment_data=False,\n",
    "  augment_factor=1,\n",
    "  drop_rate= 0.3  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63680e3",
   "metadata": {},
   "source": [
    "### Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e273ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 64\n",
    "dl_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "dl_val   = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)\n",
    "dl_test   = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fe24fa",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59df3034",
   "metadata": {},
   "source": [
    "### Define the classifier model\n",
    "Define the RNN model. It takes two inputs:\n",
    "\n",
    "- `seq`: Time series, shape (batch_size, time_steps, n_features)\n",
    "- `meta`: Meta data parameters, shape (batch_size, n_meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b199ffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlasticcClassifier(torch.nn.Module):\n",
    "  def __init__(\n",
    "    self, \n",
    "    n_features, \n",
    "    n_meta_features, \n",
    "    hidden_size=64, \n",
    "    num_layers=2, \n",
    "    num_classes=15, \n",
    "    bidirectional=False, \n",
    "    dropout=0.5,\n",
    "    fc_hidden_size=128  \n",
    "  ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "    self.gru = torch.nn.GRU(\n",
    "      input_size=n_features,\n",
    "      hidden_size=hidden_size,\n",
    "      num_layers=num_layers,\n",
    "      batch_first=True,\n",
    "      bidirectional=bidirectional,\n",
    "      dropout=dropout if num_layers > 1 else 0.0\n",
    "    )\n",
    "\n",
    "    self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    self.classifier = torch.nn.Sequential(\n",
    "      torch.nn.Linear(hidden_size * self.num_directions + n_meta_features, fc_hidden_size),\n",
    "      torch.nn.ReLU(),\n",
    "      torch.nn.Dropout(dropout),\n",
    "      torch.nn.Linear(fc_hidden_size, num_classes),\n",
    "    )\n",
    "\n",
    "  def forward(self, x_seq, x_meta):\n",
    "    rnn_out, _ = self.gru(x_seq)                      # (batch, seq_len, hidden*2)\n",
    "    pooled, _ = torch.max(rnn_out, dim=1)             # (batch, hidden*2)\n",
    "    x = torch.cat([pooled, x_meta], dim=1)            # (batch, hidden*2 + meta_features)\n",
    "    logits = self.classifier(x)\n",
    "    probs = F.softmax(logits, dim=1)                  # one-hot output\n",
    "    \n",
    "    return probs, logits\n",
    "    #return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8de007",
   "metadata": {},
   "source": [
    "Create classifier instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbabde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Create model\n",
    "n_features= dataset_train.X_seq.shape[2]\n",
    "n_meta_features= dataset_train.X_meta.shape[1]\n",
    "num_classes= dataset_train.Y.shape[1]\n",
    "seq_length= dataset_train.X_seq.shape[1]\n",
    "print(f\"seq_length={seq_length}\")\n",
    "print(f\"n_features={n_features}\")\n",
    "print(f\"n_meta_features={n_meta_features}\")\n",
    "print(f\"num_classes={num_classes}\")\n",
    "\n",
    "model = PlasticcClassifier(\n",
    "  n_features=n_features,\n",
    "  n_meta_features=n_meta_features,\n",
    "  hidden_size=128,\n",
    "  num_layers=2,\n",
    "  num_classes=num_classes,\n",
    "  bidirectional=True,\n",
    "  dropout=0.5,\n",
    "  fc_hidden_size=128  \n",
    ")\n",
    "\n",
    "# - Print model structure\n",
    "summary(\n",
    "  model, \n",
    "  #input_data=[\n",
    "  #  torch.randn(batch_size, seq_length, n_features),  # x_seq: (batch, seq_len, n_features)\n",
    "  #  torch.randn(batch_size, n_meta_features)       # x_meta: (batch, n_meta_features)\n",
    "  #]\n",
    "  input_size=[(batch_size, seq_length, n_features), (batch_size, n_meta_features)] \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a1704",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fe19c9",
   "metadata": {},
   "source": [
    "#### PLAsTiCC Weighted Log-Loss Metric\n",
    "\n",
    "The PLAsTiCC challenge uses a **weighted multi-class logarithmic loss** to evaluate classification performance. This metric penalizes incorrect predictions and accounts for **class imbalance** by assigning each class a weight.\n",
    "\n",
    "The loss is defined as:\n",
    "\n",
    "$$\n",
    "L = -\\frac{\\sum_{i=1}^{M} w_i \\cdot \\frac{1}{N_i} \\sum_{j=1}^{N_i} y_{ij}\\log(p_{ij})}{\\sum_{i=1}^{M} w_i} \n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $N_i$ is the number of objects with class $i$\n",
    "- $w_i$ is the weight for class $i$\n",
    "- $y_{ij}$ is 1 if observation $i$ belongs to class $j$ and 0 otherwise\n",
    "- $p_{ij}$ is the predicted probability that observation $i$ belongs to class $j$\n",
    "\n",
    "**Notes**\n",
    "- The loss is computed per class, then **averaged across all classes**, weighted by the importance $w_i$ (= 1 for most classes, 2 for rare objects). This ensures that rare but important classes contribute proportionally.\n",
    "- Predictions are clipped to the range $([1e-15, 1 - 1e-15])$ to prevent $\\log(0)$.\n",
    "- The sum of class weights is used to normalize the final score.\n",
    "- During training, class 99 (the \"none of the above\" class) was excluded and evaluated separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58200e1",
   "metadata": {},
   "source": [
    "Define the loss function as required by the PLASTICC challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb4e225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_weighted_logloss(y_true, y_pred, class_weights, normalize_batch=True):\n",
    "  \"\"\"\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    Adapted from TF version: https://www.kaggle.com/ogrellier\n",
    "  \n",
    "    - y_true/y_pred are one-hot encoded\n",
    "    - class_weights=[2,2,1,1,1,1,1,2,1,1,1,1,1,2,2] (see challenge result paper)\n",
    "  \"\"\" \n",
    "    \n",
    "  # - Normalize rows and limit y_preds to eps, 1-eps    \n",
    "  eps = 1e-15\n",
    "  preds = torch.clamp(y_pred, min=eps, max=1 - eps)\n",
    "\n",
    "  # - Transform to log\n",
    "  log_preds = torch.log(y_pred)  # (N, M)\n",
    "\n",
    "  # (1) Per-class weighted log loss: sum across samples\n",
    "  y_log_ones = torch.sum(y_true * log_preds, dim=0)              # (M,)\n",
    "  nb_pos = torch.sum(y_true, dim=0)                              # (M,)\n",
    "  nb_pos = torch.where(nb_pos == 0, torch.ones_like(nb_pos), nb_pos)\n",
    "\n",
    "  y_w = y_log_ones * class_weights / nb_pos                       # (M,)\n",
    "\n",
    "  loss = -torch.sum(y_w) / torch.sum(class_weights)\n",
    "  if normalize_batch:\n",
    "    loss = loss / y_pred.shape[0]  # normalize by batch size\n",
    "    \n",
    "  if not torch.isfinite(loss):\n",
    "    print(\"⚠️ Warning: loss is NaN or Inf\")\n",
    "    print(\"y_pred finite? \")\n",
    "    print(torch.all(torch.isfinite(y_pred)))\n",
    "    print(\"y_true finite? \")\n",
    "    print(torch.all(torch.isfinite(y_true)))\n",
    "    print(\"log_preds finite? \")\n",
    "    print(torch.all(torch.isfinite(log_preds)))\n",
    "    print(\"y_log_ones finite? \")\n",
    "    print(torch.all(torch.isfinite(y_log_ones)))\n",
    "    print(\"y_w finite? \")\n",
    "    print(torch.all(torch.isfinite(y_w)))\n",
    "    \n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206ae0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_weighted_logloss_stable(y_true, logits, class_weights, normalize_batch=True):\n",
    "    \"\"\"\n",
    "    Numerically stable PLAsTiCC log-loss using log_softmax.\n",
    "\n",
    "    Args:\n",
    "        y_true: one-hot encoded targets (N, num_classes)\n",
    "        logits: raw output of model before softmax (N, num_classes)\n",
    "        class_weights: tensor of class weights (num_classes,)\n",
    "\n",
    "    Returns:\n",
    "        Scalar loss (float)\n",
    "    \"\"\"\n",
    "    # - Use log_softmax for numerical stability\n",
    "    log_probs = F.log_softmax(logits, dim=1)  # (N, num_classes)\n",
    "\n",
    "    # - Multiply by one-hot true labels → picks correct class log-prob\n",
    "    y_log_ones = torch.sum(y_true * log_probs, dim=0)  # (num_classes,)\n",
    "\n",
    "    # - Number of positive examples per class\n",
    "    nb_pos = torch.sum(y_true, dim=0)\n",
    "    nb_pos = torch.where(nb_pos == 0, torch.ones_like(nb_pos), nb_pos)  # avoid divide-by-zero\n",
    "\n",
    "    # - Weighted log-loss per class\n",
    "    y_w = y_log_ones * class_weights / nb_pos\n",
    "\n",
    "    loss = -torch.sum(y_w) / torch.sum(class_weights)\n",
    "    if normalize_batch:\n",
    "      loss = loss / y_pred.shape[0]  # normalize by batch size\n",
    "    \n",
    "    if not torch.isfinite(loss):\n",
    "      print(\"⚠️ Warning: loss is NaN or Inf\")\n",
    "      print(\"logits finite? \")\n",
    "      print(torch.all(torch.isfinite(logits)))\n",
    "      print(\"y_true finite? \")\n",
    "      print(torch.all(torch.isfinite(y_true)))\n",
    "      print(\"log_probs finite? \")\n",
    "      print(torch.all(torch.isfinite(log_probs)))\n",
    "      print(\"y_log_ones finite? \")\n",
    "      print(torch.all(torch.isfinite(y_log_ones)))\n",
    "      print(\"y_w finite? \")\n",
    "      print(torch.all(torch.isfinite(y_w)))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385dd3e6",
   "metadata": {},
   "source": [
    "Define a function to initialize weights before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135ef378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "  \"\"\" Applies custom weight initialization to layers in the model \"\"\"\n",
    "  for m in model.modules():\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "      torch.nn.init.xavier_uniform_(m.weight)  # or kaiming_uniform_\n",
    "      if m.bias is not None:\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "    elif isinstance(m, torch.nn.GRU):\n",
    "      for name, param in m.named_parameters():\n",
    "        if 'weight_ih' in name:\n",
    "          torch.nn.init.xavier_uniform_(param.data)\n",
    "        elif 'weight_hh' in name:\n",
    "          torch.nn.init.orthogonal_(param.data)\n",
    "        elif 'bias' in name:\n",
    "          torch.nn.init.zeros_(param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a17d42f",
   "metadata": {},
   "source": [
    "Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4534ccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "  def __init__(self):\n",
    "    self.reset()\n",
    "\n",
    "  def reset(self):\n",
    "    self.sum = 0\n",
    "    self.count = 0\n",
    "\n",
    "  def update(self, value, n=1):\n",
    "    self.sum += value * n\n",
    "    self.count += n\n",
    "\n",
    "  @property\n",
    "  def avg(self):\n",
    "    return self.sum / self.count if self.count > 0 else 0\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    dl_train,\n",
    "    dl_val=None,\n",
    "    num_epochs=1,\n",
    "    lr=1e-3,\n",
    "    use_lr_scheduler=False,\n",
    "    warmup_ratio=0.1,\n",
    "    class_weights=None,\n",
    "    clip_grad=False,\n",
    "    max_grad_norm=5,\n",
    "    outfile_model= \"model.pth\",\n",
    "    outfile_weights= \"model_weights.pth\"\n",
    "):\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  model.to(device)\n",
    "\n",
    "  # - Check class weights  \n",
    "  if class_weights is None:\n",
    "    raise ValueError(\"class_weights must be provided\")\n",
    "\n",
    "  class_weights = class_weights.to(device)\n",
    "  \n",
    "  # - Init optimizer\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "  scheduler = None\n",
    "  if use_lr_scheduler:\n",
    "    num_training_steps = num_epochs * len(dl_train)\n",
    "    num_warmup_steps = int(warmup_ratio * num_training_steps)\n",
    "    print(f\"--> #steps warmup/tot: {num_warmup_steps}/{num_training_steps}\")\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "      optimizer=optimizer,\n",
    "      num_warmup_steps=num_warmup_steps,\n",
    "      num_training_steps=num_training_steps\n",
    "    )  \n",
    "\n",
    "  # - Init metrics\n",
    "  loss_meter = AverageMeter()\n",
    "  acc_metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes).to(device)\n",
    "  f1_metric = torchmetrics.F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\").to(device)\n",
    "  confusion_matrix_metric = torchmetrics.ConfusionMatrix(task=\"multiclass\", num_classes=num_classes, normalize=\"true\").to(device)\n",
    "    \n",
    "  val_acc_metric = val_f1_metric = None\n",
    "  if dl_val is not None:\n",
    "    val_loss_meter = AverageMeter()    \n",
    "    val_acc_metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes).to(device)\n",
    "    val_f1_metric = torchmetrics.F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\").to(device)\n",
    "    val_confusion_matrix_metric = torchmetrics.ConfusionMatrix(task=\"multiclass\", num_classes=num_classes, normalize=\"true\").to(device)\n",
    "    \n",
    "  history = {\n",
    "    \"loss_train\": [],\n",
    "    \"acc_train\": [],\n",
    "    \"f1score_train\": [],\n",
    "    \"cm_train\": None,\n",
    "    \"cm_metric_train\": None,  \n",
    "    \"loss_val\": [],\n",
    "    \"acc_val\": [],\n",
    "    \"f1score_val\": [],\n",
    "    \"cm_val\": None,\n",
    "    \"cm_metric_val\": None,   \n",
    "  }\n",
    "\n",
    "  # - Start training loop\n",
    "  for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # - Reset avg metrics\n",
    "    progress = tqdm(dl_train, desc=f\"Epoch {epoch+1} [Train]\", leave=False)\n",
    "    loss_meter.reset()\n",
    "    acc_metric.reset()\n",
    "    f1_metric.reset()\n",
    "    confusion_matrix_metric.reset()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    y_true_all = []\n",
    "    y_pred_all = []\n",
    "\n",
    "    # - Run batch loop\n",
    "    for x_seq, x_meta, y in progress:    \n",
    "      x_seq, x_meta, y = x_seq.to(device), x_meta.to(device), y.to(device)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      #y_pred = model(x_seq, x_meta)\n",
    "      y_pred, logits = model(x_seq, x_meta)    \n",
    "\n",
    "      #loss = multi_weighted_logloss(y, y_pred, class_weights, normalize_batch=False)\n",
    "      loss = multi_weighted_logloss_stable(y, logits, class_weights, normalize_batch=False)\n",
    "      \n",
    "      # ✅ Check 1: Loss is finite (not NaN or Inf)\n",
    "      if not torch.isfinite(loss):\n",
    "        print(\"⚠️ Warning: loss is NaN or Inf. Skipping this batch.\")\n",
    "        print(\"x_seq finite? \")\n",
    "        print(torch.all(torch.isfinite(x_seq)))\n",
    "        print(\"x_meta finite? \")\n",
    "        print(torch.all(torch.isfinite(x_meta)))  \n",
    "        continue  # skip backprop for this batch\n",
    "        \n",
    "      loss.backward()\n",
    "    \n",
    "      # ✅ Check 2: Optional: Print max gradient norm for debugging\n",
    "      total_norm = 0\n",
    "      for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "          param_norm = p.grad.data.norm(2)\n",
    "          total_norm += param_norm.item() ** 2\n",
    "      total_norm = total_norm ** 0.5\n",
    "      if total_norm > 1e3:\n",
    "        print(f\"⚠️ High gradient norm: {total_norm:.2f}\")\n",
    "\n",
    "      # ✅ Check 3: Clip gradients to prevent explosion\n",
    "      if clip_grad:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)\n",
    "    \n",
    "      #if clip_grad:  \n",
    "      #  #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "      #  torch.nn.utils.clip_grad_norm_(model.parameters(), gradclip)\n",
    "      #  for p in model.parameters():\n",
    "      #    p.data.add_(p.grad, alpha=-lr)  \n",
    "    \n",
    "      optimizer.step()\n",
    "      if scheduler:\n",
    "        scheduler.step()  # 🔁 per-step LR update\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "      else:\n",
    "        current_lr= lr\n",
    "        \n",
    "      # - Update loss and metrics \n",
    "      target_pred = y_pred.argmax(dim=1)\n",
    "      target_true= y.argmax(dim=1)\n",
    "      loss_meter.update(loss.item(), x_seq.size(0))\n",
    "      acc_metric.update(target_pred, target_true)\n",
    "      f1_metric.update(target_pred, target_true)\n",
    "      confusion_matrix_metric.update(target_pred, target_true)  \n",
    "        \n",
    "      total_loss += loss.item()\n",
    "      y_true_all.append(target_true.cpu())\n",
    "      y_pred_all.append(target_pred.cpu())  \n",
    "    \n",
    "      # - Update progress bar\n",
    "      progress.set_postfix({\n",
    "        \"lr\": f\"{current_lr:.6f}\",\n",
    "        \"loss\": f\"{loss_meter.avg:.4f}\",\n",
    "        \"acc\": f\"{acc_metric.compute().item():.4f}\",\n",
    "        \"f1\": f\"{f1_metric.compute().item():.4f}\"\n",
    "      })  \n",
    "\n",
    "    # - Compute average metrics (v1)\n",
    "    avg_train_loss = total_loss / len(dl_train)\n",
    "    y_true_all = torch.cat(y_true_all)\n",
    "    y_pred_all = torch.cat(y_pred_all)\n",
    "    train_acc = (y_true_all == y_pred_all).float().mean().item()\n",
    "    train_f1 = f1_score(y_true_all, y_pred_all, average='macro', zero_division=0)\n",
    "\n",
    "    # - Compute average metrics (v2)\n",
    "    avg_train_loss_v2 = loss_meter.avg\n",
    "    train_acc_v2 = acc_metric.compute().item()\n",
    "    train_f1_v2 = f1_metric.compute().item()\n",
    "    confusion_matrix= confusion_matrix_metric.compute().cpu().numpy()\n",
    "    history[\"loss_train\"].append(avg_train_loss_v2)\n",
    "    history[\"acc_train\"].append(train_acc_v2)\n",
    "    history[\"f1score_train\"].append(train_f1_v2) \n",
    "    history[\"cm_train\"]= confusion_matrix\n",
    "    history[\"cm_metric_train\"]= confusion_matrix_metric\n",
    "        \n",
    "    #print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f}\", end='')\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]: lr={current_lr:.6f}, loss={avg_train_loss:.4f}, {avg_train_loss_v2:.4f} | acc={train_acc:.4f}, {train_acc_v2:.4f} | f1={train_f1:.4f}, {train_f1_v2:.4f}\", end='')\n",
    "\n",
    "    \n",
    "    if dl_val is not None:\n",
    "      model.eval()\n",
    "    \n",
    "      # - Init val metrics\n",
    "      val_loss_meter.reset()\n",
    "      val_acc_metric.reset()\n",
    "      val_f1_metric.reset()\n",
    "      val_confusion_matrix_metric.reset()\n",
    "      val_progress = tqdm(dl_val, desc=f\"Epoch {epoch+1} [Val]\", leave=False)\n",
    "      val_loss = 0.0\n",
    "      y_true_val = []\n",
    "      y_pred_val = []\n",
    "\n",
    "      with torch.no_grad():\n",
    "        for x_seq, x_meta, y in val_progress:    \n",
    "          x_seq, x_meta, y = x_seq.to(device), x_meta.to(device), y.to(device)\n",
    "          #y_pred = model(x_seq, x_meta)\n",
    "          y_pred, logits = model(x_seq, x_meta)  \n",
    "\n",
    "          #loss = multi_weighted_logloss(y, y_pred, class_weights, normalize_batch=False)\n",
    "          loss = multi_weighted_logloss_stable(y, logits, class_weights, normalize_batch=False)\n",
    "        \n",
    "          # - Update loss and accuracy  \n",
    "          val_loss_meter.update(loss.item(), x_seq.size(0))\n",
    "          target_pred = y_pred.argmax(dim=1)\n",
    "          target_true = y.argmax(dim=1)  \n",
    "          val_acc_metric.update(target_pred, target_true)\n",
    "          val_f1_metric.update(target_pred, target_true)\n",
    "          val_confusion_matrix_metric.update(target_pred, target_true) \n",
    "          val_loss += loss.item()\n",
    "\n",
    "          y_true_val.append(y.argmax(dim=1).cpu())\n",
    "          y_pred_val.append(y_pred.argmax(dim=1).cpu())\n",
    "  \n",
    "          # - Update progress bar\n",
    "          val_progress.set_postfix({\"loss\": f\"{val_loss_meter.avg:.4f}\"})\n",
    "        \n",
    "      # - Compute average metrics (v1)    \n",
    "      avg_val_loss = val_loss / len(dl_val)\n",
    "      y_true_val = torch.cat(y_true_val)\n",
    "      y_pred_val = torch.cat(y_pred_val)\n",
    "      val_acc = (y_true_val == y_pred_val).float().mean().item()\n",
    "      val_f1 = f1_score(y_true_val, y_pred_val, average='macro', zero_division=0)\n",
    "\n",
    "      # - Compute average metrics (v2)    \n",
    "      avg_val_loss_v2 = val_loss_meter.avg\n",
    "      val_acc_v2 = val_acc_metric.compute().item()\n",
    "      val_f1_v2 = val_f1_metric.compute().item()\n",
    "      val_confusion_matrix= val_confusion_matrix_metric.compute().cpu().numpy()  \n",
    "      history[\"loss_val\"].append(avg_val_loss_v2)\n",
    "      history[\"acc_val\"].append(val_acc_v2)\n",
    "      history[\"f1score_val\"].append(val_f1_v2)  \n",
    "      history[\"cm_val\"]= val_confusion_matrix\n",
    "      history[\"cm_metric_val\"]= val_confusion_matrix_metric\n",
    "    \n",
    "      print(f\" | Val Loss: {avg_val_loss:.4f}, {avg_val_loss_v2:.4f} | Val Acc: {val_acc:.4f}, {val_acc_v2:.4f} | Val F1: {val_f1:.4f}, {val_f1_v2:.4f}\")\n",
    "    else:\n",
    "      print()\n",
    "\n",
    "  # - Save final model\n",
    "  print(f\"\\n✅ Model checkpoint saved to: {outfile_weights}\")\n",
    "  torch.save(model.state_dict(), outfile_weights)  \n",
    "  torch.save(model, outfile_model)\n",
    "\n",
    "  print(\"Training complete.\")\n",
    "  return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b10cdce",
   "metadata": {},
   "source": [
    "Initialize weights and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a508420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Initialize weights\n",
    "torch.manual_seed(10)\n",
    "initialize_weights(model)\n",
    "\n",
    "# - Run train\n",
    "nepochs= 100\n",
    "lr= 1e-3\n",
    "warmup_ratio= 0.1\n",
    "class_weights= torch.from_numpy(dataset_train.class_weight_factors)\n",
    "outfile_weights= os.path.join(rundir, \"model_weights.pth\")\n",
    "outfile_model= os.path.join(rundir, \"model.pth\")\n",
    "\n",
    "metric_hist= train_model(\n",
    "  model, \n",
    "  dl_train=dl_train, \n",
    "  dl_val=dl_val, \n",
    "  num_epochs=nepochs,\n",
    "  lr=lr,\n",
    "  use_lr_scheduler=False,  \n",
    "  warmup_ratio=warmup_ratio,\n",
    "  class_weights=class_weights,\n",
    "  clip_grad=False,\n",
    "  max_grad_norm=5,\n",
    "  outfile_model=outfile_model,  \n",
    "  outfile_weights=outfile_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e81816c",
   "metadata": {},
   "source": [
    "Let’s plot the training and validation metrics after the training run is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b5e6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_metric_hist(metric_hist):\n",
    "  \n",
    "  epochs = np.arange(1, len(metric_hist[\"loss_train\"]) + 1)\n",
    "  fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "  # - Plot train/val loss\n",
    "  ax1 = fig.add_subplot(1, 2, 1)\n",
    "  ax1.plot(epochs, metric_hist[\"loss_train\"], '-o', label='Train Loss')\n",
    "  ax1.plot(epochs, metric_hist[\"loss_val\"], '--<', label='Validation Loss')\n",
    "  ax1.set_title(\"Loss Over Epochs\", fontsize=14)\n",
    "  ax1.set_xlabel(\"Epoch\", fontsize=12)\n",
    "  ax1.set_ylabel(\"Loss\", fontsize=12)\n",
    "  ax1.legend(fontsize=11)\n",
    "  ax1.grid(True) \n",
    "    \n",
    "  # - Plot acc/f1score\n",
    "  ax2 = fig.add_subplot(1, 2, 2)\n",
    "  ax2.set_ylim(0, 1)\n",
    "  ax2.plot(epochs, metric_hist[\"acc_train\"], '-o', label='Train Accuracy')\n",
    "  ax2.plot(epochs, metric_hist[\"acc_val\"], '--<', label='Validation Accuracy')\n",
    "  ax2.plot(epochs, metric_hist[\"f1score_train\"], '-*', label='Train F1-score')\n",
    "  ax2.plot(epochs, metric_hist[\"f1score_val\"], '-->', label='Validation F1-score')\n",
    "  ax2.set_title(\"Accuracy and F1-score\", fontsize=14)\n",
    "  ax2.set_xlabel(\"Epoch\", fontsize=12)\n",
    "  ax2.set_ylabel(\"Score\", fontsize=12)\n",
    "  ax2.legend(fontsize=11)\n",
    "  ax2.grid(True)\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "# - Print & plot metrics\n",
    "print(\"== Training Metrics ==\")\n",
    "#print(metric_hist)\n",
    "\n",
    "draw_metric_hist(metric_hist)\n",
    "\n",
    "# - Draw confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "metric_hist[\"cm_metric_train\"].plot(ax=ax)\n",
    "#fig_, ax_ = metric_hist[\"cm_metric_train\"][-1].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbac6abe",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "We now evaluate the trained model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29da9dc",
   "metadata": {},
   "source": [
    "Load the model from the saved checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9873c33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Load model\n",
    "#torch.serialization.add_safe_globals([PlasticcClassifier])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_trained= torch.load(outfile_model, weights_only=False)\n",
    "model.load_state_dict(torch.load(outfile_weights, weights_only=True))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ee82d",
   "metadata": {},
   "source": [
    "Run model prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962d2609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(model, dl_test, class_weights):\n",
    "    \"\"\"\n",
    "    Perform inference on a test set.\n",
    "\n",
    "    Args:\n",
    "        model: Trained AstroMultiInputClassifier\n",
    "        dl_test: DataLoader for the test set\n",
    "\n",
    "    Returns:\n",
    "        all_probs: Tensor of predicted class probabilities (N, num_classes)\n",
    "        all_preds: Tensor of predicted class indices (N,)\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    progress = tqdm(dl_test, desc=\"[Test]\", leave=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_seq, x_meta, y in progress:\n",
    "            x_seq = x_seq.to(device)\n",
    "            x_meta = x_meta.to(device)\n",
    "            y= y.to(device)\n",
    "\n",
    "            #y_pred = model(x_seq, x_meta)  # (batch, num_classes)\n",
    "            y_pred, logits = model(x_seq, x_meta)  # (batch, num_classes)\n",
    "            all_probs.append(y_pred.cpu())\n",
    "            all_preds.append(y_pred.argmax(dim=1).cpu())\n",
    "            all_targets.append(y.argmax(dim=1).cpu())  # from one-hot\n",
    "\n",
    "    probs = torch.cat(all_probs, dim=0)\n",
    "    preds = torch.cat(all_preds, dim=0)\n",
    "    targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "    acc = accuracy_score(targets, preds)\n",
    "    f1 = f1_score(targets, preds, average='macro', zero_division=0)\n",
    "\n",
    "    # One-hot targets again for PLAsTiCC loss\n",
    "    targets_onehot = torch.nn.functional.one_hot(targets, num_classes=probs.shape[1]).float()\n",
    "    class_weights = class_weights.to(probs.device)\n",
    "    metric = multi_weighted_logloss(targets_onehot, probs, class_weights, normalize_batch=False)\n",
    "    y_true= targets_onehot\n",
    "\n",
    "    print(f\"✅ Inference Metrics:\")\n",
    "    print(f\"  Accuracy        : {acc:.4f}\")\n",
    "    print(f\"  Macro F1-score  : {f1:.4f}\")\n",
    "    print(f\"  PLAsTiCC metric : {metric:.4f}\")\n",
    "\n",
    "    return y_true, probs, preds, acc, f1, metric\n",
    "\n",
    "# - Run prediction on train\n",
    "print(\"Run prediction on train dataset ...\")\n",
    "y_true_train, probs_train, preds_train, acc_train, f1_train, metric_train = predict_model(model_trained, dl_train, class_weights)\n",
    "\n",
    "# - Run prediction on test\n",
    "print(\"Run prediction on test dataset ...\")\n",
    "y_true_test, probs_test, preds_test, acc_test, f1_test, metric_test = predict_model(model_trained, dl_test, class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cda532",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights= dataset_test.class_weight_factors\n",
    "print(\"weights\")\n",
    "print(weights)\n",
    "\n",
    "def compute_plasticc_metric(y_true, probs, weights):\n",
    "  # - Weights sum\n",
    "  wsum= np.sum(weights)\n",
    "  print(\"wsum\")\n",
    "  print(wsum)\n",
    "  \n",
    "  # - Normalize rows and limit y_preds to eps, 1-eps    \n",
    "  eps = 1e-15\n",
    "  probs = np.clip(probs, min=eps, max=1 - eps)\n",
    "\n",
    "  # - Transform to log\n",
    "  log_probs = np.log(probs)  # (N, M)\n",
    "\n",
    "  # (1) Per-class weighted log loss: sum across samples\n",
    "  y_log_ones = np.sum(y_true * log_probs, axis=0)              # (M,)\n",
    "  nb_pos = np.sum(y_true, axis=0)                              # (M,)\n",
    "  print(\"nb_pos\")\n",
    "  print(nb_pos)  \n",
    "  nb_pos = np.where(nb_pos == 0, np.ones_like(nb_pos), nb_pos)\n",
    "  print(\"nb_pos\")\n",
    "  print(nb_pos)  \n",
    "\n",
    "  y_w = y_log_ones * weights / nb_pos                       # (M,)\n",
    "\n",
    "  metric = -np.sum(y_w) / wsum\n",
    "  return metric\n",
    "\n",
    "#####################\n",
    "## TRAIN METRICS\n",
    "#####################\n",
    "print(\"type(y_true_train)\")\n",
    "print(type(y_true_train))\n",
    "\n",
    "metric_train= compute_plasticc_metric(y_true_train.numpy(), probs_train.numpy(), weights)    \n",
    "print(\"== Plasticc metric (TRAIN)\")\n",
    "print(metric_train)\n",
    "\n",
    "#####################\n",
    "## TEST METRICS\n",
    "#####################\n",
    "\n",
    "metric_test= compute_plasticc_metric(y_true_test.numpy(), probs_test.numpy(), weights)    \n",
    "print(\"== Plasticc metric (TEST)\")\n",
    "print(metric_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7811dfee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usc8-ai-workshop",
   "language": "python",
   "name": "usc8-ai-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
