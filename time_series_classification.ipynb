{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Astronomical Time Series Classification with PyTorch RNNs\n",
    "\n",
    "\n",
    "\n",
    " **Goal:** Classify synthetic light curves (time series) into different categories using Recurrent Neural Networks (RNNs) with PyTorch.\n",
    "\n",
    "\n",
    "\n",
    " **Context:** In astronomy, light curves (measurements of an object's brightness over time) are crucial for understanding the nature of celestial objects. Different types of variability exist (periodic stars, transient events, eclipsing binaries, etc.), and automatically classifying them is an important task. This notebook focuses on classifying three specific types: Cepheids, RR Lyrae, and Eclipsing Binaries, using the synthetic data generators provided.\n",
    "\n",
    "\n",
    "\n",
    " **Learning Approach:**\n",
    "\n",
    " 1.  We will use **synthetic** light curve data for 7 classes (Cepheid, RR Lyrae, Eclipsing Binary, LPV/Mira, Flare Star and Rotational Modulation), generated using functions similar to those in astronomical research contexts. The dataset is designed to be *non-trivial*, moderately challenging simple RNN models.\n",
    "\n",
    " 2.  We will implement and train basic **LSTM (Long Short-Term Memory)** and **GRU (Gated Recurrent Unit)** models (unidirectional, single-layer). We will observe their performance, which will likely be *improvable*.\n",
    "\n",
    " 3.  **Main Exercise:** Participants will modify and enhance the basic LSTM/GRU architectures by introducing techniques like:\n",
    "\n",
    "     *   Multiple Layers (`num_layers`)\n",
    "\n",
    "     *   Bidirectionality (`bidirectional=True`)\n",
    "\n",
    "     *   Dropout\n",
    "\n",
    "     *   (Optional advanced: Attention, 1D Convolutions)\n",
    "\n",
    " 4.  We will evaluate and compare the performance of the basic models against the enhanced models.\n",
    "\n",
    "\n",
    "\n",
    " **Notebook Outline:**\n",
    "\n",
    " 1.  **Environment Setup:** Install and import libraries.\n",
    "\n",
    " 2.  **Synthetic Dataset Generation and Preparation:** Create light curves using provided functions, split into train/validation/test, visualize, and create PyTorch DataLoaders.\n",
    "\n",
    " 3.  **Base LSTM Model:** Define, train, and evaluate a simple LSTM.\n",
    "\n",
    " 4.  **Base GRU Model:** Define, train, and evaluate a simple GRU.\n",
    "\n",
    " 5.  **Exercise: Enhance RNN Architectures:** Modify the base models to achieve better performance.\n",
    "\n",
    " 6.  **Exercise Solution (Example):** A sample implementation of an enhanced model.\n",
    "\n",
    " 7.  **Comparison and Conclusions:** Comparative analysis of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio numpy matplotlib scikit-learn torchmetrics seaborn pandas -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchmetrics\n",
    "import requests\n",
    "\n",
    "# Device setup (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Settings for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories (optional)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Synthetic Dataset Generation and Preparation\n",
    "\n",
    "\n",
    "\n",
    " We will create a synthetic dataset using the provided functions to generate light curves for three classes:\n",
    "\n",
    "\n",
    "\n",
    " 1.  **Cepheid:** Pulsating variable stars with a characteristic asymmetric light curve shape.\n",
    "\n",
    " 2.  **RR Lyrae:** Another type of pulsating variable star, typically with shorter periods and lower amplitudes than Cepheids, often with a sharp rise and slower decline.\n",
    "\n",
    " 3.  **Eclipsing Binary:** Systems of two stars orbiting each other, where eclipses cause periodic dips in brightness.\n",
    "\n",
    " 4.  **Delta Scuti (Î´ Sct):** Pulsating stars with very short periods (hours), often exhibiting multiple pulsation frequencies simultaneously (multi-periodic), resulting in complex light curve shapes with small amplitudes.\n",
    "\n",
    " 5.  **LPV/Mira:** Long Period Variables, typically red giants, with very long pulsation periods (100s to 1000s of days), large amplitudes, often asymmetric shapes, and noticeable variations from one cycle to the next.\n",
    "\n",
    " 6.  **Flare Star:** Stars exhibiting a relatively constant baseline brightness punctuated by sudden, sharp, short-lived, high-amplitude brightening events (flares).\n",
    "\n",
    " 7.  **Rotational Modulation:** Stars showing quasi-periodic variations caused by features (like starspots) rotating in and out of view. The shape and amplitude can evolve over time as the features change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LOADING THE PRE-GENERATED DATASET ---\n",
    "\n",
    "dataset_filename_npz = \"synthetic_time_series_dataset.npz\"\n",
    "GITHUB_RAW_URL = \"https://raw.githubusercontent.com/simoneriggi/usc8-ai-workshop/main/synthetic_time_series_dataset.npz\"\n",
    "\n",
    "if not os.path.exists(dataset_filename_npz):\n",
    "    print(f\"Downloading dataset {dataset_filename_npz} from GitHub...\")\n",
    "    try:\n",
    "        response = requests.get(GITHUB_RAW_URL)\n",
    "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "        with open(dataset_filename_npz, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(\"Download complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during download from GitHub: {e}\")\n",
    "        raise # Re-raise the exception to stop execution if download fails\n",
    "\n",
    "else:\n",
    "    print(f\"File {dataset_filename_npz} already exists. Skipping download.\")\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    loaded_data = np.load(dataset_filename_npz)\n",
    "    series_data = loaded_data['series_data']\n",
    "    labels_data = loaded_data['labels_data']\n",
    "    print(f\"Dataset loaded: series_data shape {series_data.shape}, labels_data shape {labels_data.shape}\")\n",
    "\n",
    "    # Reassign global constants that might have been defined based on the original generation parameters.\n",
    "    # This ensures consistency if these variables are used later.\n",
    "    TIME_SERIES_LENGTH = series_data.shape[1]\n",
    "    NUM_CLASSES = len(np.unique(labels_data))\n",
    "    # CLASS_NAMES might need to be redefined here, or ensure it's defined before this cell,\n",
    "    # based on the loaded NUM_CLASSES. If CLASS_NAMES was already defined (e.g., hardcoded)\n",
    "    # and matches the new NUM_CLASSES, it should be fine.\n",
    "    print(f\"Inferred TIME_SERIES_LENGTH: {TIME_SERIES_LENGTH}\")\n",
    "    print(f\"Inferred NUM_CLASSES: {NUM_CLASSES}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File {dataset_filename_npz} not found after download attempt.\")\n",
    "    print(\"The notebook cannot continue without the dataset. Check the download/generation section.\")\n",
    "    # At this point, the notebook execution should ideally stop or fall back to the generation code (if enabled).\n",
    "    raise SystemExit(\"Dataset not found, execution stopped.\")\n",
    "except KeyError as e:\n",
    "    print(f\"ERROR: Missing key in .npz file: {e}. The file might be corrupt or not in the expected format.\")\n",
    "    raise SystemExit(\"Error in dataset format, execution stopped.\")\n",
    "\n",
    "# --- END OF PRE-GENERATED DATASET LOADING ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Light Curve Generation Functions (from the provided notebook) ---\n",
    "\n",
    "def generate_cepheid(n_points=128, period=10, amplitude=1.5, noise_level=0.1):\n",
    "    \"\"\"Generate a Cepheid-like light curve with asymmetric variation\"\"\"\n",
    "    time = np.sort(np.random.uniform(0, 100, n_points)) # Generate time points first\n",
    "    phase = (time / period) % 1.0\n",
    "\n",
    "    # Asymmetric sawtooth-like pattern\n",
    "    # Flux calculation based on phase\n",
    "    flux = 10 + amplitude * (0.5 - 0.5 * np.cos(2 * np.pi * phase) -\n",
    "                           0.2 * np.sin(4 * np.pi * phase))\n",
    "\n",
    "    # Add noise\n",
    "    flux_err = np.ones_like(flux) * noise_level\n",
    "    flux += np.random.normal(0, noise_level, size=len(time))\n",
    "\n",
    "    # Return only flux and label for classification task\n",
    "    return flux, 0 # Label 0 for Cepheid\n",
    "\n",
    "def generate_rr_lyrae(n_points=128, period=0.6, amplitude=0.8, noise_level=0.1):\n",
    "    \"\"\"Generate an RR Lyrae-like light curve\"\"\"\n",
    "    time = np.sort(np.random.uniform(0, 100, n_points))\n",
    "    phase = (time / period) % 1.0\n",
    "\n",
    "    # Sharper rise, slower decline\n",
    "    flux = 10 + amplitude * (0.4 - 0.5 * np.cos(2 * np.pi * phase) -\n",
    "                           0.3 * np.sin(2 * np.pi * phase))\n",
    "\n",
    "    # Add noise\n",
    "    flux_err = np.ones_like(flux) * noise_level\n",
    "    flux += np.random.normal(0, noise_level, size=len(time))\n",
    "\n",
    "    return flux, 1 # Label 1 for RR Lyrae\n",
    "\n",
    "def generate_eclipsing_binary(n_points=128, period=3.2, primary_depth=0.5,\n",
    "                               secondary_depth=0.2, noise_level=0.1):\n",
    "    \"\"\"Generate an eclipsing binary light curve with primary and secondary eclipses\"\"\"\n",
    "    time = np.sort(np.random.uniform(0, 50, n_points)) # Use shorter time baseline typical for EB studies\n",
    "    phase = (time / period) % 1.0\n",
    "\n",
    "    # Generate flux\n",
    "    flux = np.ones_like(time) * 10  # Base flux\n",
    "\n",
    "    # Add primary eclipse (sharper dip)\n",
    "    primary_mask = np.abs(phase - 0.0) < 0.05\n",
    "    primary_phase_centered = phase[primary_mask] / 0.05 # Scale phase within eclipse duration\n",
    "    primary_profile = primary_depth * (1 - primary_phase_centered**2) # Simple quadratic dip\n",
    "    flux[primary_mask] -= primary_profile[primary_profile>0] # Ensure positive profile values\n",
    "\n",
    "\n",
    "    # Add secondary eclipse (shallower dip)\n",
    "    secondary_mask = np.abs(phase - 0.5) < 0.05\n",
    "    secondary_phase_centered = (phase[secondary_mask] - 0.5) / 0.05 # Center phase at 0.5\n",
    "    secondary_profile = secondary_depth * (1 - secondary_phase_centered**2)\n",
    "    flux[secondary_mask] -= secondary_profile[secondary_profile>0]\n",
    "\n",
    "    # Add noise\n",
    "    flux_err = np.ones_like(flux) * noise_level\n",
    "    flux += np.random.normal(0, noise_level, size=len(time))\n",
    "\n",
    "    return flux, 2 # Label 2 for Eclipsing Binary\n",
    "\n",
    "def generate_delta_scuti(n_points=128, baseline_flux=10,\n",
    "                         periods_hours=[1.5, 1.8, 2.1], # Tipici periodi in ore\n",
    "                         amplitudes=[0.03, 0.02, 0.015], # Piccole ampiezze (in unitÃ  di flusso relativo o magnitudini)\n",
    "                         noise_level=0.005):\n",
    "\n",
    "    if len(periods_hours) != len(amplitudes):\n",
    "        raise ValueError(\"Le liste periods_hours e amplitudes devono avere la stessa lunghezza.\")\n",
    "\n",
    "    # Genera tempi - per pulsatori rapidi, un intervallo piÃ¹ breve ma denso Ã¨ comune\n",
    "    # Simuliamo osservazioni su ~2 giorni (48 ore)\n",
    "    time_hours = np.sort(np.random.uniform(0, 48, n_points))\n",
    "\n",
    "    total_signal = np.zeros(n_points)\n",
    "\n",
    "    # Somma le componenti sinusoidali per ogni periodo\n",
    "    for period_h, amp in zip(periods_hours, amplitudes):\n",
    "        # Aggiungi una fase casuale per ciascuna componente\n",
    "        phase_shift = np.random.uniform(0, 2 * np.pi)\n",
    "        # Calcola il segnale per questo periodo\n",
    "        signal = amp * np.sin(2 * np.pi * time_hours / period_h + phase_shift)\n",
    "        total_signal += signal\n",
    "\n",
    "    # Flusso finale = baseline + segnale multi-periodico + rumore\n",
    "    flux = baseline_flux + total_signal + np.random.normal(0, noise_level, n_points)\n",
    "\n",
    "    # Definisci un'etichetta numerica per questa classe\n",
    "    label = 3 # Assegniamo 5 per Delta Scuti (assicurati che sia univoca)\n",
    "\n",
    "    return flux, label\n",
    "\n",
    "def generate_lpv_mira(n_points=128, baseline_flux=10,\n",
    "                      period_days=300, amplitude=2.0, # Periodo lungo, ampiezza grande\n",
    "                      asymmetry_param=0.3, # Controlla l'asimmetria (0 = sinusoidale, >0 salita piÃ¹ ripida)\n",
    "                      cycle_variation_amp=0.3, # Frazione di variazione ampiezza ciclo-per-ciclo\n",
    "                      cycle_variation_shape=0.1, # Variazione forma/asimmetria ciclo-per-ciclo\n",
    "                      noise_level=0.1):\n",
    "\n",
    "    # Genera tempi su un intervallo lungo per coprire diversi cicli\n",
    "    max_time = period_days * np.random.uniform(3, 5) # Osserva per 3-5 periodi medi\n",
    "    time_days = np.sort(np.random.uniform(0, max_time, n_points))\n",
    "\n",
    "    # Calcola la fase *media*\n",
    "    phase = (time_days / period_days) % 1.0\n",
    "\n",
    "    # Simula variazioni ciclo-per-ciclo (lente)\n",
    "    # Usiamo una sinusoide a lungo periodo per modulare i parametri\n",
    "    long_period_mod = period_days * np.random.uniform(2.5, 4.0) # Periodo della modulazione\n",
    "    amp_mod = 1.0 + cycle_variation_amp * np.sin(2 * np.pi * time_days / long_period_mod + np.random.uniform(0, np.pi))\n",
    "    shape_mod = asymmetry_param * (1.0 + cycle_variation_shape * np.sin(2 * np.pi * time_days / long_period_mod + np.random.uniform(np.pi, 2*np.pi))) # Modula asimmetria\n",
    "\n",
    "    current_amplitude = amplitude * amp_mod\n",
    "    current_asymmetry = shape_mod\n",
    "\n",
    "    # Crea il segnale base usando armoniche per l'asimmetria\n",
    "    # Esempio: sin(x) + asymmetry * sin(2*x + phase_shift)\n",
    "    signal = current_amplitude * (np.sin(2 * np.pi * phase) +\n",
    "                                current_asymmetry * np.sin(4 * np.pi * phase + np.pi / 2)) # Aggiunge seconda armonica per asimmetria\n",
    "\n",
    "    # Aggiusta la forma per renderla piÃ¹ simile a una Mira (es. salita piÃ¹ ripida)\n",
    "    # Potrebbe richiedere modelli piÃ¹ complessi, ma questo dÃ  un'idea\n",
    "    # Per una salita piÃ¹ ripida, si potrebbero usare funzioni a tratti o template\n",
    "\n",
    "    # Flusso finale = baseline + segnale + rumore\n",
    "    flux = baseline_flux + signal + np.random.normal(0, noise_level * (1 + signal / baseline_flux / 2), n_points) # Rumore leggermente dipendente dal flusso?\n",
    "\n",
    "    # Etichetta\n",
    "    label = 4 # Assegniamo 6 per LPV/Mira\n",
    "\n",
    "    return flux, label\n",
    "\n",
    "def generate_flare_star(n_points=128, baseline_flux=10, flare_rate=0.05,\n",
    "                       flare_amp_mean=3.0, flare_decay_time=0.5, noise_level=0.1):\n",
    "    \"\"\"Generate a flare star light curve.\"\"\"\n",
    "    time = np.linspace(0, 50, n_points) # Shorter baseline often used for flare studies\n",
    "    dt = time[1] - time[0]\n",
    "    flux = np.random.normal(baseline_flux, noise_level*0.5, n_points) # Baseline with some noise\n",
    "\n",
    "    # Add flares (Poisson process approximation)\n",
    "    n_flares = np.random.poisson(flare_rate * (time[-1] - time[0]))\n",
    "    flare_times = np.random.uniform(time[0], time[-1], n_flares)\n",
    "\n",
    "    for t_flare in flare_times:\n",
    "        # Flare shape: Instant rise, exponential decay\n",
    "        flare_amp = np.random.exponential(flare_amp_mean)\n",
    "        flare_profile = flare_amp * np.exp(-(time - t_flare) / flare_decay_time)\n",
    "        # Apply only after the flare time and add to flux\n",
    "        flux[time >= t_flare] += flare_profile[time >= t_flare]\n",
    "\n",
    "    # Add general noise\n",
    "    flux += np.random.normal(0, noise_level, n_points)\n",
    "    return flux, 5 # Label 3 for Flare Star\n",
    "\n",
    "def generate_rotational_modulation(n_points=128, period=4.0, amplitude=0.3,\n",
    "                                  amp_variation_frac=0.3, noise_level=0.1):\n",
    "    \"\"\"Generate a quasi-periodic light curve simulating rotational modulation.\"\"\"\n",
    "    time = np.sort(np.random.uniform(0, 100, n_points))\n",
    "    base_flux = 10\n",
    "\n",
    "    # Slowly varying amplitude\n",
    "    long_period = np.random.uniform(30, 60) # Period of amplitude variation\n",
    "    current_amplitude = amplitude * (1 + amp_variation_frac * np.sin(2 * np.pi * time / long_period))\n",
    "\n",
    "    # Main periodic signal (rotation)\n",
    "    phase = (time / period) % 1.0\n",
    "    # Use a non-perfect sine wave, e.g., add a harmonic\n",
    "    signal = current_amplitude * (np.sin(2 * np.pi * phase) + 0.3 * np.sin(4 * np.pi * phase + np.pi/3))\n",
    "\n",
    "    flux = base_flux + signal\n",
    "    # Add noise\n",
    "    flux += np.random.normal(0, noise_level, n_points)\n",
    "    return flux, 6 # Label 4 for Rotational Modulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate the full dataset ---\n",
    "\n",
    "N_SAMPLES_PER_CLASS = 1000\n",
    "TIME_SERIES_LENGTH = 128 # Should match n_points in generators\n",
    "NOISE_LEVEL_GLOBAL = 0.3 # Global noise level adjustment if needed\n",
    "NUM_CLASSES = 7\n",
    "CLASS_NAMES = ['Cepheid', 'RR Lyrae', 'Eclipsing Binary', 'Delta Scuti', 'LPV', 'Flare Star', 'Rotational Mod']\n",
    "\n",
    "all_series = []\n",
    "all_labels = []\n",
    "\n",
    "print(\"Generating synthetic data...\")\n",
    "for i in range(N_SAMPLES_PER_CLASS):\n",
    "    # Cepheid - Vary parameters slightly for diversity\n",
    "    cep_period = np.random.uniform(8, 12)\n",
    "    cep_amp = np.random.uniform(1.3, 1.7)\n",
    "    cep_flux, cep_label = generate_cepheid(n_points=TIME_SERIES_LENGTH, period=cep_period, amplitude=cep_amp, noise_level=NOISE_LEVEL_GLOBAL)\n",
    "    all_series.append(cep_flux)\n",
    "    all_labels.append(cep_label)\n",
    "\n",
    "    # RR Lyrae - Vary parameters\n",
    "    rr_period = np.random.uniform(0.5, 0.7)\n",
    "    rr_amp = np.random.uniform(0.7, 0.9)\n",
    "    rr_flux, rr_label = generate_rr_lyrae(n_points=TIME_SERIES_LENGTH, period=rr_period, amplitude=rr_amp, noise_level=NOISE_LEVEL_GLOBAL*0.8) # Slightly less noisy?\n",
    "    all_series.append(rr_flux)\n",
    "    all_labels.append(rr_label)\n",
    "\n",
    "    # Eclipsing Binary - Vary parameters\n",
    "    eb_period = np.random.uniform(2.8, 3.6)\n",
    "    eb_pdepth = np.random.uniform(0.4, 0.6)\n",
    "    eb_sdepth = np.random.uniform(0.1, 0.3)\n",
    "    eb_flux, eb_label = generate_eclipsing_binary(n_points=TIME_SERIES_LENGTH, period=eb_period, primary_depth=eb_pdepth, secondary_depth=eb_sdepth, noise_level=NOISE_LEVEL_GLOBAL*1.2) # Slightly more noisy?\n",
    "    all_series.append(eb_flux)\n",
    "    all_labels.append(eb_label)\n",
    "    if (i+1) % (N_SAMPLES_PER_CLASS // 10) == 0:\n",
    "        print(f\"  Generated {3*(i+1)} samples...\")\n",
    "    \n",
    "    # Delta Scuti\n",
    "    ds_periods = sorted(np.random.uniform(0.5, 4.0, np.random.randint(2, 5))) # 2-4 random periods in hours\n",
    "    ds_amps = sorted(np.random.exponential(0.02, len(ds_periods)), reverse=True) # Esponential amplitudes \n",
    "    ds_noise = np.random.uniform(0.003, 0.008)\n",
    "    ds_flux, ds_label = generate_delta_scuti(n_points=TIME_SERIES_LENGTH, periods_hours=ds_periods, amplitudes=ds_amps, noise_level=ds_noise)\n",
    "    all_series.append(ds_flux)\n",
    "    all_labels.append(ds_label)\n",
    "\n",
    "    # LPV/Mira\n",
    "    lpv_period = np.random.uniform(200, 500)\n",
    "    lpv_amp = np.random.uniform(1.5, 3.0)\n",
    "    lpv_asym = np.random.uniform(0.2, 0.5)\n",
    "    lpv_noise = np.random.uniform(0.08, 0.2)\n",
    "    lpv_flux, lpv_label = generate_lpv_mira(n_points=TIME_SERIES_LENGTH, period_days=lpv_period, amplitude=lpv_amp, asymmetry_param=lpv_asym, noise_level=lpv_noise)\n",
    "    all_series.append(lpv_flux)\n",
    "    all_labels.append(lpv_label)\n",
    "\n",
    "    # Flare Star\n",
    "    fs_amp = np.random.uniform(2.0, 4.0)\n",
    "    fs_decay = np.random.uniform(0.3, 0.7)\n",
    "    fs_flux, fs_label = generate_flare_star(n_points=TIME_SERIES_LENGTH, flare_amp_mean=fs_amp, flare_decay_time=fs_decay, noise_level=NOISE_LEVEL_GLOBAL*0.7)\n",
    "    all_series.append(fs_flux)\n",
    "    all_labels.append(fs_label)\n",
    "\n",
    "    # Rotational Modulation\n",
    "    rm_period = np.random.uniform(3.0, 5.0)\n",
    "    rm_amp = np.random.uniform(0.2, 0.4)\n",
    "    rm_flux, rm_label = generate_rotational_modulation(n_points=TIME_SERIES_LENGTH, period=rm_period, amplitude=rm_amp, noise_level=NOISE_LEVEL_GLOBAL)\n",
    "    all_series.append(rm_flux)\n",
    "    all_labels.append(rm_label)\n",
    "\n",
    "    # Update progress print statement if needed (depends on N_SAMPLES_PER_CLASS)\n",
    "    if (i+1) % (N_SAMPLES_PER_CLASS // 10) == 0:\n",
    "         print(f\"  Generated {NUM_CLASSES*(i+1)} samples...\") # Use NUM_CLASSES\n",
    "\n",
    "\n",
    "# Convert to NumPy arrays and shuffle\n",
    "all_series = np.array(all_series)\n",
    "all_labels = np.array(all_labels)\n",
    "indices = np.arange(len(all_labels))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "series_data = all_series[indices]\n",
    "labels_data = all_labels[indices]\n",
    "\n",
    "print(f\"\\nGenerated data shape: {series_data.shape}\")\n",
    "print(f\"Generated labels shape: {labels_data.shape}\")\n",
    "print(f\"Unique labels: {np.unique(labels_data)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Visualization ---\n",
    "plt.figure(figsize=(25, 16)) # Increased height\n",
    "time_axis = np.arange(TIME_SERIES_LENGTH)\n",
    "\n",
    "for i, class_name in enumerate(CLASS_NAMES):\n",
    "    # >>>>> Use 2 rows, 3 columns <<<<<\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    idxs = np.where(labels_data == i)[0][:3] # Show 3 examples\n",
    "    if len(idxs) > 0: # Check if examples exist for this class\n",
    "        for idx in idxs:\n",
    "            plt.plot(time_axis, series_data[idx], alpha=0.7)\n",
    "        plt.title(f\"Class {i}: {class_name}\")\n",
    "    else:\n",
    "        plt.title(f\"Class {i}: {class_name}\\n(No samples found?)\") # Handle case if a class wasn't generated properly\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Flux\")\n",
    "    plt.grid(True, alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train/Validation/Test Split ---\n",
    "# Stratify to maintain class proportions\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    series_data, labels_data, test_size=0.15, random_state=SEED, stratify=labels_data\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.15 / 0.85, random_state=SEED, stratify=y_train_val # ~15% of total for validation\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}, {y_test.shape}\")\n",
    "print(f\"Label distribution (Train): {np.bincount(y_train)}\")\n",
    "print(f\"Label distribution (Validation): {np.bincount(y_val)}\")\n",
    "print(f\"Label distribution (Test): {np.bincount(y_test)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Preprocessing: Standardization ---\n",
    "# Scaling is important for neural networks.\n",
    "# Fit the scaler ONLY on the training data.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Reshape for scaler: Needs 2D [n_samples * seq_len, 1] or [n_samples, seq_len]\n",
    "# Let's scale each time series independently first, then maybe standardize across the dataset?\n",
    "# Simpler approach: Treat each sample's sequence as features for the scaler.\n",
    "# Fit on flattened training data\n",
    "scaler.fit(X_train.reshape(-1, 1))\n",
    "\n",
    "# Transform train, val, test\n",
    "X_train_scaled = scaler.transform(X_train.reshape(-1, 1)).reshape(X_train.shape)\n",
    "X_val_scaled = scaler.transform(X_val.reshape(-1, 1)).reshape(X_val.shape)\n",
    "X_test_scaled = scaler.transform(X_test.reshape(-1, 1)).reshape(X_test.shape)\n",
    "\n",
    "\n",
    "# Add the feature dimension (required by PyTorch RNNs/CNNs: [batch, seq_len, features])\n",
    "X_train_scaled = X_train_scaled[:, :, np.newaxis]\n",
    "X_val_scaled = X_val_scaled[:, :, np.newaxis]\n",
    "X_test_scaled = X_test_scaled[:, :, np.newaxis]\n",
    "\n",
    "print(f\"\\nScaled train shape with feature dim: {X_train_scaled.shape}\")\n",
    "print(f\"Mean after scaling (Train sample 0): {X_train_scaled[0].mean():.4f}\") # Should be close to 0\n",
    "print(f\"Std after scaling (Train sample 0): {X_train_scaled[0].std():.4f}\")   # Should be close to 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PyTorch Dataset and DataLoader Creation ---\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for time series.\"\"\"\n",
    "    def __init__(self, series, labels, transform=None):\n",
    "        # Ensure data is float32 for PyTorch tensors\n",
    "        self.series = torch.tensor(series, dtype=torch.float32)\n",
    "        # Ensure labels are long for CrossEntropyLoss\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.transform = transform # Optional: for any further transformations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.series[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        # PyTorch RNNs expect [seq_len, features] or [batch, seq_len, features]\n",
    "        # Our data is already [seq_len, features] per sample\n",
    "        return sample, label\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = TimeSeriesDataset(X_train_scaled, y_train)\n",
    "val_dataset = TimeSeriesDataset(X_val_scaled, y_val)\n",
    "test_dataset = TimeSeriesDataset(X_test_scaled, y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True) # os.cpu_count() // 2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True) # os.cpu_count() // 2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True) # os.cpu_count() // 2, pin_memory=True)\n",
    "\n",
    "# Verify a batch\n",
    "try:\n",
    "    data_iter = iter(train_loader)\n",
    "    sample_batch, label_batch = next(data_iter)\n",
    "    print(f\"\\nSample batch shape: {sample_batch.shape}\") # [batch_size, seq_len, num_features]\n",
    "    print(f\"Label batch shape: {label_batch.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError fetching batch, check DataLoader setup: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. Base LSTM Model\n",
    "\n",
    "\n",
    "\n",
    " We define a very simple LSTM classifier:\n",
    "\n",
    " *   A single, unidirectional LSTM layer.\n",
    "\n",
    " *   A fully-connected linear layer for the final classification.\n",
    "\n",
    "\n",
    "\n",
    " We expect decent but not optimal performance, given the dataset's design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMBaseClassifier(nn.Module):\n",
    "    \"\"\"Base (Simple) LSTM Classifier.\"\"\"\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=1, num_classes=NUM_CLASSES, dropout_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM Layer\n",
    "        # batch_first=True means input is (batch, seq_len, feature)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                            batch_first=True, dropout=dropout_prob, bidirectional=False)\n",
    "\n",
    "        # Linear layer for classification\n",
    "        self.fc = nn.Linear(hidden_size, num_classes) # Only hidden_size because unidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states (optional, defaults to zeros if not provided)\n",
    "        # h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        # c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Pass through LSTM\n",
    "        # lstm_out contains output for each timestep\n",
    "        # h_n is the final hidden state, c_n is the final cell state\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x) #, (h0, c0))\n",
    "\n",
    "        # We use the output of the last hidden state of the last layer for classification\n",
    "        # h_n has shape [num_layers * num_directions, batch, hidden_size]\n",
    "        # We want the last layer -> h_n[-1] for unidirectional\n",
    "        last_hidden_state = h_n[-1]\n",
    "\n",
    "        # Pass through the linear layer\n",
    "        out = self.fc(last_hidden_state)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions for Training and Evaluation ---\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, metrics_calculator):\n",
    "    \"\"\"Runs one training epoch.\"\"\"\n",
    "    model.train() # Set model to training mode\n",
    "    metrics_calculator.reset()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for data, target in dataloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update metrics and loss\n",
    "        total_loss += loss.item() * data.size(0) # Accumulate loss for the epoch\n",
    "        # Detach output before passing to metrics to prevent gradient tracking issues\n",
    "        metrics_calculator.update(output.detach(), target)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    metrics = metrics_calculator.compute()\n",
    "    return avg_loss, metrics\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device, metrics_calculator):\n",
    "    \"\"\"Runs one validation epoch.\"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    metrics_calculator.reset()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculation\n",
    "        for data, target in dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            metrics_calculator.update(output, target)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    metrics = metrics_calculator.compute()\n",
    "    return avg_loss, metrics\n",
    "\n",
    "def run_train(model, train_loader, val_loader, criterion, optimizer, device, num_epochs, model_name=\"best_model.pth\", patience=5):\n",
    "    \"\"\"Main training loop with early stopping.\"\"\"\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # Initialize metrics calculators (per epoch)\n",
    "    # Using Accuracy and F1-Score (macro average suitable for potentially imbalanced classes)\n",
    "    metrics_train = torchmetrics.MetricCollection({\n",
    "        'accuracy': torchmetrics.Accuracy(task=\"multiclass\", num_classes=NUM_CLASSES),\n",
    "        'f1': torchmetrics.F1Score(task=\"multiclass\", num_classes=NUM_CLASSES, average='macro')\n",
    "    }).to(device)\n",
    "\n",
    "    metrics_val = torchmetrics.MetricCollection({\n",
    "        'accuracy': torchmetrics.Accuracy(task=\"multiclass\", num_classes=NUM_CLASSES),\n",
    "        'f1': torchmetrics.F1Score(task=\"multiclass\", num_classes=NUM_CLASSES, average='macro')\n",
    "    }).to(device)\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss, train_metrics = train_epoch(model, train_loader, criterion, optimizer, device, metrics_train)\n",
    "        train_losses.append(train_loss)\n",
    "        # Convert tensor metric to Python float\n",
    "        train_acc = train_metrics['accuracy'].item()\n",
    "        train_f1 = train_metrics['f1'].item()\n",
    "        train_accs.append(train_acc)\n",
    "\n",
    "\n",
    "        # Validation\n",
    "        val_loss, val_metrics = validate_epoch(model, val_loader, criterion, device, metrics_val)\n",
    "        val_losses.append(val_loss)\n",
    "        val_acc = val_metrics['accuracy'].item()\n",
    "        val_f1 = val_metrics['f1'].item()\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        # Early Stopping and Best Model Saving (based on validation loss)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "            print(f\"   -> Validation loss decreased. Saved best model to {model_name}\")\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"   -> Validation loss did not improve for {epochs_no_improve} epoch(s).\")\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch + 1} epochs.\")\n",
    "            break\n",
    "\n",
    "    history = {'train_loss': train_losses, 'val_loss': val_losses,\n",
    "               'train_acc': train_accs, 'val_acc': val_accs}\n",
    "    return history\n",
    "\n",
    "def run_test(model, dataloader, criterion, device, class_names, model_path=\"best_model.pth\"):\n",
    "    \"\"\"Evaluates the model on the test set.\"\"\"\n",
    "    try:\n",
    "        # Load the best saved model\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        model.eval()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Model file not found at {model_path}. Cannot evaluate.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model state: {e}. Cannot evaluate.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    # Metrics for the test set\n",
    "    test_metrics = torchmetrics.MetricCollection({\n",
    "        'accuracy': torchmetrics.Accuracy(task=\"multiclass\", num_classes=len(class_names)).to(device),\n",
    "        'f1_macro': torchmetrics.F1Score(task=\"multiclass\", num_classes=len(class_names), average='macro').to(device),\n",
    "        'f1_weighted': torchmetrics.F1Score(task=\"multiclass\", num_classes=len(class_names), average='weighted').to(device),\n",
    "        'conf_matrix': torchmetrics.ConfusionMatrix(task=\"multiclass\", num_classes=len(class_names)).to(device)\n",
    "    }).to(device)\n",
    "    test_metrics.reset()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            preds = torch.argmax(output, dim=1)\n",
    "            test_metrics.update(preds, target) # Note: torchmetrics expects (preds, target)\n",
    "\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    final_metrics = test_metrics.compute()\n",
    "\n",
    "    # Extract metrics safely, converting to CPU and Python float/numpy array\n",
    "    accuracy = final_metrics['accuracy'].cpu().item()\n",
    "    f1_macro = final_metrics['f1_macro'].cpu().item()\n",
    "    f1_weighted = final_metrics['f1_weighted'].cpu().item()\n",
    "    conf_matrix_tensor = final_metrics['conf_matrix'].cpu() # Keep as tensor for now\n",
    "    conf_matrix_np = conf_matrix_tensor.numpy() # Convert to numpy for display\n",
    "\n",
    "    print(\"\\n--- Test Set Evaluation ---\")\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test F1 Score (Macro): {f1_macro:.4f}\")\n",
    "    print(f\"Test F1 Score (Weighted): {f1_weighted:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    # Ensure target_names matches the number of classes inferred from labels\n",
    "    unique_labels = sorted(np.unique(all_targets))\n",
    "    report_class_names = [class_names[i] for i in unique_labels]\n",
    "    print(classification_report(all_targets, all_preds, target_names=report_class_names, digits=4))\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    plt.figure(figsize=(len(class_names)*1.8, len(class_names)*1.5))\n",
    "    sns.heatmap(conf_matrix_np, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=report_class_names, yticklabels=report_class_names)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Confusion Matrix - Test Set\")\n",
    "    plt.show()\n",
    "\n",
    "    # Return metrics including the confusion matrix tensor\n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': final_metrics['accuracy'], # Keep as tensor\n",
    "        'f1_macro': final_metrics['f1_macro'], # Keep as tensor\n",
    "        'f1_weighted': final_metrics['f1_weighted'], # Keep as tensor\n",
    "        'conf_matrix': conf_matrix_tensor\n",
    "    }\n",
    "\n",
    "def draw_metric_hist(history, title='Model Training History'):\n",
    "    \"\"\"Draws learning curves.\"\"\"\n",
    "    if not history or 'train_loss' not in history or not history['train_loss']:\n",
    "        print(\"History object is empty or missing data. Cannot plot.\")\n",
    "        return\n",
    "\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history['train_loss'], 'bo-', label='Training Loss')\n",
    "    if 'val_loss' in history and history['val_loss']:\n",
    "      plt.plot(epochs, history['val_loss'], 'ro-', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history['train_acc'], 'bo-', label='Training Accuracy')\n",
    "    if 'val_acc' in history and history['val_acc']:\n",
    "      plt.plot(epochs, history['val_acc'], 'ro-', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust for global title\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train Base LSTM ---\n",
    "\n",
    "# Instantiate Model, Loss, Optimizer\n",
    "lstm_base_model = LSTMBaseClassifier(input_size=1, hidden_size=64, num_layers=1, num_classes=NUM_CLASSES).to(device)\n",
    "criterion = nn.CrossEntropyLoss() # Suitable for multi-class classification\n",
    "optimizer_lstm_base = optim.Adam(lstm_base_model.parameters(), lr=0.0001)\n",
    "\n",
    "print(\"--- Training Base LSTM Model ---\")\n",
    "# Reduce epochs for faster workshop execution, add patience for early stopping\n",
    "NUM_EPOCHS = 15 # ADJUST FOR WORKSHOP TIME (e.g., 5-15)\n",
    "PATIENCE = 3 # Stop after 3 epochs with no validation loss improvement\n",
    "\n",
    "lstm_base_history = run_train(lstm_base_model, train_loader, val_loader, criterion, optimizer_lstm_base,\n",
    "                              device, num_epochs=NUM_EPOCHS, model_name=\"lstm_base_best.pth\", patience=PATIENCE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot Base LSTM Learning Curves ---\n",
    "draw_metric_hist(lstm_base_history, title='Base LSTM Training History')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluate Base LSTM on Test Set ---\n",
    "lstm_base_test_metrics = run_test(lstm_base_model, test_loader, criterion, device,\n",
    "                                  class_names=CLASS_NAMES, model_path=\"lstm_base_best.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Base LSTM Comments:** Observe the metrics (accuracy, F1, confusion matrix). The model likely learned something, but there might be significant errors, especially between classes with subtle differences or requiring understanding of longer-term patterns or specific event shapes (like eclipses). This indicates room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4. Base GRU Model\n",
    "\n",
    "\n",
    "\n",
    " Now, let's implement a base GRU model, structurally similar to the base LSTM (single layer, unidirectional), to see if GRU performs differently on this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUBaseClassifier(nn.Module):\n",
    "    \"\"\"Base (Simple) GRU Classifier.\"\"\"\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=1, num_classes=NUM_CLASSES, dropout_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # GRU Layer\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers,\n",
    "                          batch_first=True, dropout=dropout_prob, bidirectional=False)\n",
    "\n",
    "        # Linear layer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through GRU\n",
    "        # GRU output: output, h_n (h_n is the last hidden state)\n",
    "        gru_out, h_n = self.gru(x)\n",
    "\n",
    "        # Use the last hidden state of the last layer\n",
    "        last_hidden_state = h_n[-1]\n",
    "\n",
    "        # Pass through the linear layer\n",
    "        out = self.fc(last_hidden_state)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train Base GRU ---\n",
    "gru_base_model = GRUBaseClassifier(input_size=1, hidden_size=64, num_layers=1, num_classes=NUM_CLASSES).to(device)\n",
    "criterion = nn.CrossEntropyLoss() # Reuse the same loss function\n",
    "optimizer_gru_base = optim.Adam(gru_base_model.parameters(), lr=0.0001)\n",
    "\n",
    "print(\"\\n--- Training Base GRU Model ---\")\n",
    "gru_base_history = run_train(gru_base_model, train_loader, val_loader, criterion, optimizer_gru_base,\n",
    "                             device, num_epochs=NUM_EPOCHS, model_name=\"gru_base_best.pth\", patience=PATIENCE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot Base GRU Learning Curves ---\n",
    "draw_metric_hist(gru_base_history, title='Base GRU Training History')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluate Base GRU on Test Set ---\n",
    "gru_base_test_metrics = run_test(gru_base_model, test_loader, criterion, device,\n",
    "                                 class_names=CLASS_NAMES, model_path=\"gru_base_best.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Base GRU Comments:** Compare the GRU base performance with the LSTM base. Often, GRU trains slightly faster and might perform better or worse depending on the dataset. Again, we should likely see potential for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5. Exercise: Enhance RNN Architectures\n",
    "\n",
    "\n",
    "\n",
    " **Your Task:** Now it's time to enhance the RNN models! The base models are intentionally simple (unidirectional, 1 layer). This limits their ability to:\n",
    "\n",
    " *   Capture hierarchical temporal dependencies (addressable with `num_layers > 1`).\n",
    "\n",
    " *   Use information from both past and future context at a given time step (addressable with `bidirectional=True`).\n",
    "\n",
    " *   Manage overfitting in more complex models (mitigated with `dropout`).\n",
    "\n",
    "\n",
    "\n",
    " **Goal:** Modify the `LSTMBaseClassifier` or `GRUBaseClassifier` classes (or create new ones, e.g., `GRUlassifier`) to include **at least one** of the following techniques:\n",
    "\n",
    "\n",
    "\n",
    " 1.  **Multiple Layers:** Set `num_layers` to 2 or more in the `nn.LSTM` or `nn.GRU` initialization.\n",
    "\n",
    "     *   *Note:* The hidden state output `h_n` will have shape `[num_layers * num_directions, batch, hidden_size]`. To use only the last state of the *last layer*, you need to select it correctly (e.g., `h_n[-1]` if unidirectional, or combine `h_n[-1]` and `h_n[-2]` if bidirectional and taking the last layer's forward and backward states).\n",
    "\n",
    " 2.  **Bidirectionality:** Set `bidirectional=True`.\n",
    "\n",
    "     *   *Important:* The RNN output (e.g., `gru_out`) will now have shape `[batch, seq_len, hidden_size * 2]`.\n",
    "\n",
    "     *   *Important:* The hidden state `h_n` will have shape `[num_layers * 2, batch, hidden_size]`.\n",
    "\n",
    "     *   *Important:* The subsequent linear layer (`nn.Linear`) **must** now accept an input dimension of `hidden_size * 2` if you use the concatenated hidden states (e.g., `torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)`) or the output of the last time step (`lstm_out[:, -1, :]`). Using `lstm_out[:, -1, :]` is often simplest with `batch_first=True`.\n",
    "\n",
    " 3.  **Dropout:**\n",
    "\n",
    "     *   Add the `dropout=dropout_prob` argument (with `dropout_prob > 0`, e.g., 0.2-0.5) to `nn.LSTM` or `nn.GRU`. This applies dropout *between* RNN layers (so it only has an effect if `num_layers > 1`).\n",
    "\n",
    "     *   Add `nn.Dropout(p=dropout_prob)` layers *after* the RNN layer and/or *before* the final linear layer to regularize the output features.\n",
    "\n",
    "\n",
    "\n",
    " **Tips:**\n",
    "\n",
    " *   Start by modifying just one aspect (e.g., add `bidirectional=True`) and observe the impact.\n",
    "\n",
    " *   Try combining techniques (e.g., `num_layers=2`, `bidirectional=True`, `dropout=0.3`).\n",
    "\n",
    " *   You might need to adjust `hidden_size` or the optimizer's learning rate (`lr`).\n",
    "\n",
    " *   Pay close attention to tensor shapes! Print shapes (`print(tensor.shape)`) within the `forward` method if you're unsure.\n",
    "\n",
    "\n",
    "\n",
    " **Write your code in the cells below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### ADD YOUR CODE HERE ######\n",
    "# Define your enhanced classifier class (e.g., LSTMEnhancedClassifier or GRUEnhancedClassifier)\n",
    "# You can copy one of the base classes and modify it.\n",
    "\n",
    "class EnhancedClassifier(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=128, num_layers=2, num_classes=NUM_CLASSES, dropout_prob=0.3, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        # Enhanced LSTM Layer\n",
    "        self.lstm= nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                          batch_first=True,\n",
    "                          dropout=dropout_prob if num_layers > 1 else 0,\n",
    "                          bidirectional=bidirectional)\n",
    "\n",
    "        # Example: Dropout layer after RNN\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Example: Linear layer (adjust input size based on bidirectionality)\n",
    "        self.fc = nn.Linear(hidden_size * num_directions, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "\n",
    "        # Select the output for the linear layer\n",
    "        # Using the last time step's output is common and handles bidirectionality correctly here\n",
    "        last_timestep_output = lstm_out[:, -1, :]\n",
    "\n",
    "        # Apply dropout\n",
    "        out = self.dropout(last_timestep_output)\n",
    "\n",
    "        # Final classification layer\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### ADD YOUR CODE HERE ######\n",
    "# Instantiate your enhanced model\n",
    "# Example:\n",
    "# enhanced_model = EnhancedClassifier(...) # Use your class name\n",
    "enhanced_model = EnhancedClassifier( # Replace with your class and desired parameters\n",
    "    input_size=1,\n",
    "    hidden_size=96,        # Experiment with size\n",
    "    num_layers=2,          # Experiment with layers (1, 2, 3)\n",
    "    dropout_prob=0.4,      # Experiment with dropout (0.0 to 0.5)\n",
    "    bidirectional=True     # Experiment with True/False\n",
    ").to(device)\n",
    "\n",
    "# Print the model structure to verify\n",
    "print(\"Enhanced Model Structure:\")\n",
    "print(enhanced_model)\n",
    "\n",
    "# Define loss and optimizer (you might want to adjust the learning rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_enhanced = optim.Adam(enhanced_model.parameters(), lr=0.0005) # Try a different LR?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### ADD YOUR CODE HERE ######\n",
    "# Train your enhanced model\n",
    "print(\"\\n--- Training Your Enhanced Model ---\")\n",
    "NUM_EPOCHS_ENHANCED = 20 # Adjust as needed, maybe more than base models\n",
    "PATIENCE_ENHANCED = 4   # Adjust patience\n",
    "\n",
    "enhanced_history = run_train(\n",
    "    enhanced_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer_enhanced,\n",
    "    device,\n",
    "    num_epochs=NUM_EPOCHS_ENHANCED,\n",
    "    model_name=\"enhanced_model_best.pth\",\n",
    "    patience=PATIENCE_ENHANCED\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### ADD YOUR CODE HERE ######\n",
    "# Plot the learning curves for your enhanced model\n",
    "draw_metric_hist(enhanced_history, title='Your Enhanced Model Training History')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### ADD YOUR CODE HERE ######\n",
    "# Evaluate your enhanced model on the test set\n",
    "enhanced_test_metrics = run_test(\n",
    "    enhanced_model,\n",
    "    test_loader,\n",
    "    criterion,\n",
    "    device,\n",
    "    class_names=CLASS_NAMES,\n",
    "    model_path=\"enhanced_model_best.pth\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 6. Exercise Solution (Example)\n",
    "\n",
    "\n",
    "\n",
    " Below is an *example* of how you could have implemented an enhanced LSTM classifier. Remember, many different combinations of layers, bidirectionality, hidden size, and dropout could lead to improved results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example Solution: Enhanced GRU Classifier ---\n",
    "\n",
    "class GRUEnhancedSolution(nn.Module):\n",
    "    \"\"\"Enhanced GRU Classifier for the Solution.\"\"\"\n",
    "    def __init__(self, input_size=1, hidden_size=128, num_layers=2, num_classes=NUM_CLASSES, dropout_prob=0.4, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers,\n",
    "                            batch_first=True,\n",
    "                            dropout=dropout_prob if num_layers > 1 else 0, # Only apply RNN dropout if layers > 1\n",
    "                            bidirectional=bidirectional)\n",
    "\n",
    "        # Dropout layer after GRU\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Adjust linear layer input size\n",
    "        self.fc = nn.Linear(hidden_size * num_directions, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x) # We only need the output sequence here\n",
    "\n",
    "        # Get the output of the last time step (handles bidirectionality)\n",
    "        last_timestep_output = gru_out[:, -1, :]\n",
    "\n",
    "        # Apply dropout before the final layer\n",
    "        out = self.dropout(last_timestep_output)\n",
    "\n",
    "        # Final linear layer\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# --- Instantiate, Train, and Evaluate the Solution Example ---\n",
    "print(\"\\n--- Running Example Solution ---\")\n",
    "\n",
    "# Instantiate\n",
    "solution_model = GRUEnhancedSolution(\n",
    "    input_size=1,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    dropout_prob=0.4,\n",
    "    bidirectional=True\n",
    ").to(device)\n",
    "\n",
    "print(\"\\nSolution Model Structure:\")\n",
    "print(solution_model)\n",
    "\n",
    "# Optimizer\n",
    "criterion_sol = nn.CrossEntropyLoss()\n",
    "optimizer_sol = optim.Adam(solution_model.parameters(), lr=0.0005)\n",
    "\n",
    "# Train (using same parameters as exercise section for comparison)\n",
    "solution_history = run_train(\n",
    "    solution_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion_sol,\n",
    "    optimizer_sol,\n",
    "    device,\n",
    "    num_epochs=NUM_EPOCHS_ENHANCED, # Use same number of epochs as exercise\n",
    "    model_name=\"solution_model_best.pth\",\n",
    "    patience=PATIENCE_ENHANCED # Use same patience\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history\n",
    "draw_metric_hist(solution_history, title='Example Solution Model Training History')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "solution_test_metrics = run_test(\n",
    "    solution_model,\n",
    "    test_loader,\n",
    "    criterion_sol,\n",
    "    device,\n",
    "    class_names=CLASS_NAMES,\n",
    "    model_path=\"solution_model_best.pth\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 7. Comparison and Conclusions\n",
    "\n",
    "\n",
    "\n",
    " Let's compare the performance of the base models with the enhanced model you created (and optionally, the example solution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Comparison of Test Results ---\n",
    "print(\"\\n--- Performance Comparison (Test Set) ---\")\n",
    "\n",
    "def get_metrics_summary(metrics_dict, model_name):\n",
    "    \"\"\"Extracts key metrics from the results dictionary.\"\"\"\n",
    "    if metrics_dict is None:\n",
    "        print(f\"Metrics for {model_name} are not available.\")\n",
    "        return {'Accuracy': float('nan'), 'F1 Macro': float('nan'), 'F1 Weighted': float('nan')}\n",
    "\n",
    "    # Ensure metrics are tensors before converting\n",
    "    acc = metrics_dict.get('accuracy')\n",
    "    f1m = metrics_dict.get('f1_macro')\n",
    "    f1w = metrics_dict.get('f1_weighted')\n",
    "\n",
    "    return {\n",
    "        'Accuracy': acc.cpu().item() if torch.is_tensor(acc) else float('nan'),\n",
    "        'F1 Macro': f1m.cpu().item() if torch.is_tensor(f1m) else float('nan'),\n",
    "        'F1 Weighted': f1w.cpu().item() if torch.is_tensor(f1w) else float('nan')\n",
    "    }\n",
    "\n",
    "summary_lstm_base = get_metrics_summary(lstm_base_test_metrics, \"LSTM Base\")\n",
    "summary_gru_base = get_metrics_summary(gru_base_test_metrics, \"GRU Base\")\n",
    "# Use the metrics from your trained enhanced model\n",
    "summary_enhanced = get_metrics_summary(enhanced_test_metrics, \"GRU Enhanced Model\")\n",
    "# Optionally include the solution example metrics\n",
    "summary_solution = get_metrics_summary(solution_test_metrics, \"Solution Example\")\n",
    "\n",
    "\n",
    "print(\"\\nLSTM Base:\")\n",
    "for key, value in summary_lstm_base.items(): print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nGRU Base:\")\n",
    "for key, value in summary_gru_base.items(): print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nLSTM Enhanced Model:\")\n",
    "for key, value in summary_enhanced.items(): print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# Uncomment if you ran the solution example\n",
    "print(\"\\nGRU Solution Model:\")\n",
    "for key, value in summary_solution.items(): print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "\n",
    "# --- Comparison Table ---\n",
    "try:\n",
    "    comparison_data = {\n",
    "        'Model': ['LSTM Base', 'GRU Base', 'LSTM Enhanced Model', 'GRU Solution'],\n",
    "        'Accuracy': [summary_lstm_base['Accuracy'], summary_gru_base['Accuracy'], summary_enhanced['Accuracy'], summary_solution['Accuracy']],\n",
    "        'F1 Macro': [summary_lstm_base['F1 Macro'], summary_gru_base['F1 Macro'], summary_enhanced['F1 Macro'], summary_solution['F1 Macro']],\n",
    "        'F1 Weighted': [summary_lstm_base['F1 Weighted'], summary_gru_base['F1 Weighted'], summary_enhanced['F1 Weighted'], summary_solution['F1 Weighted']]\n",
    "    }\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(\"\\n--- Comparison Table ---\")\n",
    "    print(comparison_df.round(4).to_string(index=False))\n",
    "except ImportError:\n",
    "    print(\"\\nPandas not found. Cannot display comparison table.\")\n",
    "except Exception as e:\n",
    "     print(f\"\\nError creating comparison table: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Discussion:**\n",
    "\n",
    "\n",
    "\n",
    " *   **What specific changes did you make to enhance the RNN model?** (e.g., Added bidirectionality, increased layers, added dropout?)\n",
    "\n",
    " *   **How did these changes affect performance compared to the base models?** Was there a significant improvement in accuracy or F1-score? Did it generalize better (comparing train vs. validation accuracy)?\n",
    "\n",
    " *   **Looking at the confusion matrix of your enhanced model, which classes are still being confused?** This might suggest further improvements or indicate inherent difficulties in distinguishing certain light curve types with this architecture.\n",
    "\n",
    " *   **What challenges did you encounter during the enhancement process?** (e.g., Managing tensor shapes, choosing hyperparameters, interpreting results?)\n",
    "\n",
    "\n",
    "\n",
    " **Conclusions:**\n",
    "\n",
    "\n",
    "\n",
    " This workshop demonstrated how to:\n",
    "\n",
    " 1.  Generate and prepare specific types of astronomical time series data (Cepheid, RR Lyrae, Eclipsing Binary, 'Delta Scuti', 'LPV', 'Flare Star', 'Rotational Modulation') for classification with PyTorch.\n",
    "\n",
    " 2.  Implement basic RNN models (LSTM and GRU).\n",
    "\n",
    " 3.  Evaluate model performance and identify areas for improvement.\n",
    "\n",
    " 4.  Enhance RNN architectures using techniques like multiple layers, bidirectionality, and dropout to potentially improve classification accuracy.\n",
    "\n",
    "\n",
    "\n",
    " Experimentation with different architectures and hyperparameters is key in deep learning for time series analysis. Even small architectural changes can significantly impact performance, especially on complex datasets like those often found in astronomy. More advanced techniques like Attention mechanisms or combined CNN-RNN models could potentially yield further improvements for this type of task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
