{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1806e013",
      "metadata": {
        "id": "1806e013"
      },
      "source": [
        "# Outline\n",
        "This tutorial demonstrates how to build, train, and evaluate a Convolutional Neural Network (CNN) classifier for astronomical image data using PyTorch.\n",
        "\n",
        "- `Dataset`: Galaxy MNIST — images of galaxies with different morphologies, observed in three optical bands\n",
        "- `CNN Models`: ResNet and a custom architecture\n",
        "- `Objective`: Classify input images into one of four classes\n",
        "\n",
        "The tutorial will guide you through the following steps:\n",
        "\n",
        "1) Setting up the environment\n",
        "2) Downloading the dataset and preparing data loaders with appropriate transformations and augmentations\n",
        "3) Building a CNN classifier using a predefined architecture (ResNet)\n",
        "4) Training the model\n",
        "5) Evaluating the model on the test dataset\n",
        "6) Building a configurable CNN with a custom architecture, encouraging users to implement the training and evaluation steps based on the ResNet example"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10df2d01",
      "metadata": {
        "id": "10df2d01"
      },
      "source": [
        "# Configuring the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bda7e73",
      "metadata": {
        "id": "9bda7e73"
      },
      "source": [
        "## Module installation\n",
        "We’ll begin by installing the necessary Python modules for this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f608540",
      "metadata": {
        "id": "8f608540"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# - Install modules from requirements.txt if present\n",
        "if os.path.isfile(\"requirements.txt\"):\n",
        "  print(\"Installing modules from local requirements.txt file ...\")\n",
        "  %pip install -q -r requirements.txt\n",
        "else:\n",
        "  print(\"Installing modules ...\")\n",
        "\n",
        "  %pip install -q pillow opencv-python                                 # Img processing modules\n",
        "  %pip install -q torch torchvision torchmetrics torchsummary grad-cam # ML modules\n",
        "  %pip install -q gdown matplotlib tqdm                                # Plot/util modules\n",
        "\n",
        "  # - Create requirements file\n",
        "  %pip freeze > requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91353df9",
      "metadata": {
        "id": "91353df9"
      },
      "source": [
        "## Import modules\n",
        "Next, we import the essential modules needed throughout the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "540ac7db",
      "metadata": {
        "id": "540ac7db"
      },
      "outputs": [],
      "source": [
        "###########################\n",
        "##   STANDARD MODULES\n",
        "###########################\n",
        "import os\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import gdown\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import json\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
        "from itertools import islice\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "###########################\n",
        "##   IMGPROC/TORCH MODULES\n",
        "###########################\n",
        "# - Image proc\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "# - Torch modules\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset, Subset, random_split\n",
        "import torchvision\n",
        "from torchvision.datasets.vision import VisionDataset\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.transforms import ToTensor\n",
        "import torchmetrics\n",
        "from torchsummary import summary\n",
        "\n",
        "# - GradCAM\n",
        "from pytorch_grad_cam import (\n",
        "  GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus,\n",
        "  AblationCAM, XGradCAM, EigenCAM, EigenGradCAM,\n",
        "  LayerCAM, FullGrad, GradCAMElementWise, KPCA_CAM\n",
        ")\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget, ClassifierOutputSoftmaxTarget\n",
        "from pytorch_grad_cam.utils.find_layers import find_layer_types_recursive\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set random seeds\n",
        "Let's define a function to set random numpy/torch seeds to make random generation reproducible."
      ],
      "metadata": {
        "id": "nP-iJ58WE-M2"
      },
      "id": "nP-iJ58WE-M2"
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed=42):\n",
        "  \"\"\" Set random seed \"\"\"\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "\n",
        "  torch.cuda.manual_seed_all(seed) # should be safe even if CUDA not available\n",
        "  torch.backends.cudnn.deterministic= True\n",
        "  torch.backends.cudnn.benchmark= False\n",
        "\n",
        "# - Set the seed\n",
        "set_seed(1)"
      ],
      "metadata": {
        "id": "Iq2nM3xyE5HM"
      },
      "id": "Iq2nM3xyE5HM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "24eab38b",
      "metadata": {
        "id": "24eab38b"
      },
      "source": [
        "## Project folders\n",
        "We create a working directory `rundir` to run the tutorial in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc372736",
      "metadata": {
        "id": "bc372736"
      },
      "outputs": [],
      "source": [
        "topdir= os.getcwd()\n",
        "rundir= os.path.join(topdir, \"run-gmnist_classifier\")\n",
        "path = Path(rundir)\n",
        "path.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5186fc69",
      "metadata": {
        "id": "5186fc69"
      },
      "source": [
        "# Dataset\n",
        "For this tutorial, we will use the [**Galaxy MNIST dataset**](https://github.com/mwalmsley/galaxy_mnist).\n",
        "\n",
        "This dataset contains **10,000 images** of galaxies observed in three optical bands (*g*, *r*, and *z*), available at two resolutions:\n",
        "\n",
        "- **Low resolution**: `64×64×3`  \n",
        "- **High resolution**: `224×224×3`  \n",
        "\n",
        "The images are sourced from the **Dark Energy Camera Legacy Survey (DECaLS)** as part of the **Galaxy Zoo** project. DECaLS observations were conducted using the **Dark Energy Camera (DECam)** mounted on the 4-meter Blanco Telescope in Chile. Fluxes in the *grz* bands were converted to RGB colors (see Section 2.3 of the reference paper), and each galaxy image was saved in PNG format.\n",
        "\n",
        "The dataset is divided into two subsets:\n",
        "\n",
        "- **Training set**: 8,000 images  \n",
        "- **Test set**: 2,000 images  \n",
        "\n",
        "Each image is labeled as one of the following four classes:\n",
        "\n",
        "1. **SMOOTH_ROUND**  \n",
        "   Smooth, round galaxies without visible spiral features\n",
        "\n",
        "2. **SMOOTH_CIGAR**  \n",
        "   Smooth, elongated (cigar-shaped) galaxies, typically seen edge-on, without signs of spiral structure\n",
        "\n",
        "3. **EDGE_ON_DISK**  \n",
        "   Disk or spiral galaxies seen edge-on, often with visible spiral arms or structural irregularities\n",
        "\n",
        "4. **UNBARRED_SPIRAL**  \n",
        "   Face-on, unbarred spiral galaxies with visible disk and/or spiral arms\n",
        "   \n",
        "Examples of each class are shown in the image below.\n",
        "\n",
        "<img src=\"https://github.com/simoneriggi/usc8-ai-workshop/blob/main/media/gmnist.png?raw=1\" style=\"display: block; margin-left: 0; width: 400px;\" />\n",
        "\n",
        "> **Note**:  \n",
        "> The classes **SMOOTH_CIGAR** and **EDGE_ON_DISK** can appear very similar.  \n",
        "> - If the galaxy appears smooth and uniform, possibly with a small central bulge, it likely belongs to **SMOOTH_CIGAR**.  \n",
        "> - If the galaxy shows structural features or irregularities, it is more likely an **EDGE_ON_DISK** galaxy.\n",
        "\n",
        "In this tutorial, we will use the **high-resolution images** (`224×224` pixels).  \n",
        "The dataset has been **slightly modified** from its original format and is available for download from **Google Drive**.\n",
        "\n",
        "More details on the observational data and labelling are available in these references:\n",
        "\n",
        "- [Galaxy Zoo DECaLS paper](https://ui.adsabs.harvard.edu/abs/2022MNRAS.509.3966W/abstract)    \n",
        "- [Galaxy Zoo DECaLS data](https://zenodo.org/records/4573248)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edfa8bb8",
      "metadata": {
        "id": "edfa8bb8"
      },
      "source": [
        "## Dataset Download\n",
        "Next, we download the dataset from Google Drive and unzip it in the main folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e6d93d6",
      "metadata": {
        "id": "1e6d93d6"
      },
      "outputs": [],
      "source": [
        "# - Set dataset URL & paths\n",
        "dataset_name= \"galaxy_mnist-dataset\"\n",
        "dataset_dir= os.path.join(rundir, dataset_name)\n",
        "dataset_tar= 'galaxy_mnist-dataset.tar.gz'\n",
        "dataset_tar_fullpath= os.path.join(rundir, dataset_tar)\n",
        "dataset_url= 'https://drive.google.com/uc?export=download&id=1OprJ_NQIFyQSRWqjGLFQsAMumHvJ-tMB'\n",
        "\n",
        "# - Download dataset (if not previously downloaded)\n",
        "if not os.path.isfile(dataset_tar_fullpath):\n",
        "  print(\"Downloading file from url %s ...\" % (dataset_url))\n",
        "  gdown.download(dataset_url, dataset_tar, quiet=False)\n",
        "  print(\"DONE!\")\n",
        "\n",
        "# - Untar dataset\n",
        "if not os.path.isdir(dataset_dir):\n",
        "  print(\"Unzipping dataset file %s ...\" % (dataset_tar))\n",
        "  fp= tarfile.open(dataset_tar)\n",
        "  fp.extractall('.')\n",
        "  fp.close()\n",
        "  print(\"DONE!\")\n",
        "\n",
        "# - Moving data to rundir\n",
        "if not os.path.isfile(dataset_tar_fullpath):\n",
        "  print(\"Moving tar file to rundir %s ...\" % (rundir))\n",
        "  shutil.move(dataset_tar, rundir)\n",
        "\n",
        "if not os.path.isdir(dataset_dir):\n",
        "  print(\"Moving datadir to rundir %s ...\" % (rundir))\n",
        "  shutil.move(dataset_name, rundir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "761b9117",
      "metadata": {
        "id": "761b9117"
      },
      "source": [
        "The dataset includes data lists for both training and test samples, provided in JSON format. These lists are available for both 1-channel (channel-averaged) and 3-channel (RGB) image data.\n",
        "\n",
        "In this tutorial, we will use the 3-channel image data:\n",
        "\n",
        "- `train/3chan/datalist_train.json`\n",
        "- `test/3chan/datalist_test.json`\n",
        "\n",
        "Each data list follows the structure below:\n",
        "\n",
        "```json\n",
        "{    \n",
        "  \"data\": [    \n",
        "    {    \n",
        "      \"filepaths\": [\n",
        "        \"galaxy_mnist-dataset/train/3chan/train_1.png\"\n",
        "      ],\n",
        "      \"sname\": \"S1\",\n",
        "      \"id\": 1,\n",
        "      \"label\": \"smooth_cigar\"\n",
        "    },    \n",
        "    ...\n",
        "    ...\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "- `filepaths`: A list of one or more image file paths. In this case only 1 image is available per source.\n",
        "- `sname`: A short source name identifier\n",
        "- `label`/`id`: The ground-truth galaxy class label and target:      \n",
        "\n",
        "      0: \"smooth_round\"  \n",
        "      1: \"smooth_cigar\"   \n",
        "      2: \"edge_on_disk\"    \n",
        "      3: \"unbarred_spiral\"      "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e82cb76",
      "metadata": {
        "id": "8e82cb76"
      },
      "source": [
        "## Create PyTorch Dataset\n",
        "We define a custom PyTorch dataset for the Galaxy MNIST data by subclassing the `torch.utils.data.Dataset` base class. To do this, we need to override the following essential methods:\n",
        "\n",
        "`__len__`: Returns the total number of samples in the dataset.   \n",
        "`__getitem__`: Returns the i-th sample, including the image and its corresponding target label.   \n",
        "\n",
        "The custom dataset class accepts the following arguments:\n",
        "\n",
        "- `metadata_file`: Path to the JSON file containing the data list;\n",
        "- `subset`: An instance of `torch.utils.data.Subset` to create a subset of the original dataset;\n",
        "- `transform`: A `torchvision.transforms` object for applying transformations to the input images;\n",
        "- `data_path`: Path to the root dataset directory (e.g., your `rundir`).\n",
        "\n",
        "You must provide the `metadata_file` or, alternatively, the `subset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "151932d1",
      "metadata": {
        "id": "151932d1"
      },
      "outputs": [],
      "source": [
        "class GMNISTDataset(Dataset):\n",
        "  \"\"\" Galaxy MNIST dataset \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      metadata_file: Optional[Union[str, Path]] = \"\",\n",
        "      subset: Optional[Subset] = None,\n",
        "      transform: Optional[Callable] = None,\n",
        "      data_path: Optional[Union[str, Path]] = \"\",\n",
        "  ):\n",
        "\n",
        "    # - Check args\n",
        "    if not os.path.isfile(metadata_file) and subset is None:\n",
        "      raise(\"You must provide metadata_file or subset!\")\n",
        "\n",
        "    if data_path!=\"\" and not os.path.isdir(data_path):\n",
        "      raise(\"Invalid data_path given, it must be an existing path!\")\n",
        "\n",
        "    # - Read metadata\n",
        "    self.subset= subset\n",
        "    if self.subset is None:\n",
        "      print(f\"Reading dataset metadata from file {metadata_file}...\")\n",
        "      self.__read_metadata(metadata_file)\n",
        "\n",
        "    # - Set pars\n",
        "    self.data_path= data_path\n",
        "    self.transform = transform\n",
        "    self.pil2tensor = T.Compose([T.PILToTensor()])\n",
        "\n",
        "    self.target2label= {\n",
        "      0: \"smooth_round\",\n",
        "      1: \"smooth_cigar\",\n",
        "      2: \"edge_on_disk\",\n",
        "      3: \"unbarred_spiral\"\n",
        "    }\n",
        "\n",
        "  def __read_metadata(self, filename: Union[str, Path]) -> None:\n",
        "    \"\"\" Read JSON metadata \"\"\"\n",
        "\n",
        "    with open(filename, \"r\") as f:\n",
        "      self.datalist= json.load(f)[\"data\"]\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    \"\"\" Return the size of the dataset \"\"\"\n",
        "    return len(self.subset) if self.subset else len(self.datalist)\n",
        "\n",
        "  def __load_item(self, idx: int):\n",
        "    \"\"\" Load an individual sample \"\"\"\n",
        "\n",
        "    # - Read image path & class id\n",
        "    item = self.datalist[idx]\n",
        "    target= item['id']\n",
        "    img_path= item['filepaths'][0]\n",
        "    if self.data_path!=\"\":\n",
        "      img_path= os.path.join(self.data_path, img_path)\n",
        "\n",
        "    # - Read PIL image as RGB\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "    return img, target\n",
        "\n",
        "  def __getitem__(self, idx: int):\n",
        "    \"\"\"Return a dataset sample (image, target).\"\"\"\n",
        "\n",
        "    # - Load image & target\n",
        "    img, target = self.subset[idx] if self.subset else self.__load_item(idx)\n",
        "\n",
        "    # - Convert PIL image to tensor if needed\n",
        "    if isinstance(img, PIL.Image.Image):\n",
        "      img = self.pil2tensor(img)\n",
        "\n",
        "    # - Replace NaN or Inf with zeros\n",
        "    img[~torch.isfinite(img)] = 0\n",
        "\n",
        "    # - Apply transforms\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "\n",
        "    return img, target"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcce61db",
      "metadata": {
        "id": "fcce61db"
      },
      "source": [
        "## Create data custom transforms\n",
        "We define a set of custom image transformations to be used as data augmentations. To implement a custom transformation in PyTorch, simply subclass `nn.Module` and override the `forward` method, as shown in the examples below.\n",
        "\n",
        "This allows seamless integration with `torchvision.transforms.Compose` and keeps transformations modular and reusable."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef900720",
      "metadata": {
        "id": "ef900720"
      },
      "source": [
        "### Random flip\n",
        "A transformation that randomly flips the image either horizontally, vertically, or leaves it unchanged."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59cd84b1",
      "metadata": {
        "id": "59cd84b1"
      },
      "outputs": [],
      "source": [
        "class RandomFlip(torch.nn.Module):\n",
        "  \"\"\" Randomly flip the image horizontally, vertically, or leave it unchanged. \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, img: Union[Image.Image, Tensor]) -> Union[Image.Image, Tensor]:\n",
        "    op = random.choice([\"hflip\", \"vflip\", \"none\"])\n",
        "    if op == \"hflip\":\n",
        "      return TF.hflip(img)\n",
        "    elif op == \"vflip\":\n",
        "      return TF.vflip(img)\n",
        "    else:\n",
        "      return img"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "345d5d92",
      "metadata": {
        "id": "345d5d92"
      },
      "source": [
        "### Random rotate\n",
        "A transform that randomly rotate image by either 90, 180, 270 degrees or leave image unchanged."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97fa994e",
      "metadata": {
        "id": "97fa994e"
      },
      "outputs": [],
      "source": [
        "class RandomRotate90(torch.nn.Module):\n",
        "  \"\"\" Randomly rotate the image by 90°, 180°, 270°, or leave it unchanged. \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, img: Union[Image.Image, Tensor]) -> Union[Image.Image, Tensor]:\n",
        "    angle = random.choice([0, 90, 180, 270])\n",
        "    if angle != 0:\n",
        "      return TF.rotate(img, angle)\n",
        "    return img"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63266e3d",
      "metadata": {
        "id": "63266e3d"
      },
      "source": [
        "### Absolute Channel Maximum Scaling\n",
        "This transform finds, for each image, the absolute maximum, and then it scales all channels by this value, taking into account any possible band flux ratio information as sensitive classification variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08b7f300",
      "metadata": {
        "id": "08b7f300"
      },
      "outputs": [],
      "source": [
        "class AbsChanMaxScaling(torch.nn.Module):\n",
        "  \"\"\" Scales an image tensor by the absolute maximum across all channels.\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, img: Tensor) -> Tensor:\n",
        "\n",
        "    # - Compute absolute image max across channels\n",
        "    if img.ndim == 4:  # [B, C, H, W]\n",
        "      abs_max = torch.amax(img, dim=(1, 2, 3), keepdim=True)\n",
        "    elif img.ndim == 3:  # [C, H, W]\n",
        "      abs_max = torch.amax(img, dim=(0, 1, 2), keepdim=True)\n",
        "    else:\n",
        "      warnings.warn(f\"Unexpected tensor dimension ({img.ndim}). Returning input unmodified.\")\n",
        "      return img\n",
        "\n",
        "    # - Scale image by absmax, avoiding division by zero\n",
        "    abs_max = abs_max.clamp(min=1e-8)\n",
        "    return img / abs_max"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3efbb5c5",
      "metadata": {
        "id": "3efbb5c5"
      },
      "source": [
        "### My custom transform\n",
        "Think about other augmentations you could experiment with to improve model performance!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bd04ef2",
      "metadata": {
        "id": "9bd04ef2"
      },
      "outputs": [],
      "source": [
        "## DEFINE YOUR OWN TRANSFORM HERE!\n",
        "# ...\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d40348d7",
      "metadata": {
        "id": "d40348d7"
      },
      "source": [
        "### Define composite transforms\n",
        "We will define two composite transforms:\n",
        "\n",
        "- Training transform: includes both standard preprocessing and additional augmentation transforms\n",
        "- Validation/Test transform: includes only the standard preprocessing steps (no augmentation)\n",
        "\n",
        "**Standard Transforms**    \n",
        "These are applied to all data (train and test):\n",
        "\n",
        "- Image resize\n",
        "- Absolute channel max scaling\n",
        "\n",
        "**Augmentation Transforms**    \n",
        "These are applied only to the training data:\n",
        "\n",
        "- Random flipping\n",
        "- Random 90-degree rotation\n",
        "- Random crop and resize\n",
        "\n",
        "You may also consider whether it's beneficial to include the following normalization step, which is commonly used for ImageNet-pretrained models:\n",
        "\n",
        "``` python\n",
        "T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "```\n",
        "\n",
        "> ⚠️ **Note:** While this normalization is standard for natural images, it may not be optimal for astronomical data. Use with care depending on your goals (e.g., fine-tuning a pretrained model vs. training from scratch)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4831ce42",
      "metadata": {
        "id": "4831ce42"
      },
      "outputs": [],
      "source": [
        "# - Define image resize size\n",
        "img_resize= 224\n",
        "\n",
        "# - Define transforms for train data\n",
        "transform_train= T.Compose(\n",
        "  [\n",
        "    T.Resize(img_resize, interpolation=T.InterpolationMode.BICUBIC),\n",
        "    RandomFlip(),\n",
        "    RandomRotate90(),\n",
        "    T.RandomResizedCrop(\n",
        "      img_resize,\n",
        "      scale=(0.5, 1.0),\n",
        "      ratio=(1., 1.),\n",
        "      interpolation=T.InterpolationMode.BICUBIC\n",
        "    ),\n",
        "    AbsChanMaxScaling(),\n",
        "\n",
        "    # Optional: Normalize using ImageNet statistics\n",
        "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "  ]\n",
        ")\n",
        "\n",
        "# - Define transforms for val/test data\n",
        "transform_eval= T.Compose(\n",
        "  [\n",
        "    T.Resize(img_resize, interpolation=T.InterpolationMode.BICUBIC),\n",
        "    AbsChanMaxScaling(),\n",
        "\n",
        "    # Optional: Normalize using ImageNet statistics\n",
        "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "  ]\n",
        ")\n",
        "\n",
        "# - Define transforms for plotting purposes\n",
        "transform= T.Compose(\n",
        "  [\n",
        "    T.Resize(img_resize, interpolation=T.InterpolationMode.BICUBIC),\n",
        "    AbsChanMaxScaling()\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61fa5fa3",
      "metadata": {
        "id": "61fa5fa3"
      },
      "source": [
        "## Create datasets\n",
        "We will load the Galaxy MNIST training and test datasets using the `GMNISTDataset` class defined earlier.\n",
        "After loading the full training set, we will split it into two subsets:\n",
        "\n",
        "- Training set: 70% of the original training data\n",
        "- Validation set: 30% of the original training data\n",
        "\n",
        "This split allows us to validate model performance during training while keeping the test set strictly for final evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9c8e45a",
      "metadata": {
        "id": "e9c8e45a"
      },
      "outputs": [],
      "source": [
        "# - Set paths to train/test datalists\n",
        "filename_train_3chan= os.path.join(dataset_dir, \"train/3chan/datalist_train.json\")\n",
        "filename_test_3chan= os.path.join(dataset_dir, \"test/3chan/datalist_test.json\")\n",
        "\n",
        "# - Load full training (train + validation) dataset\n",
        "print(f\"Reading train-cv dataset from: {filename_train_3chan}\")\n",
        "dataset_traincv= GMNISTDataset(\n",
        "  metadata_file=filename_train_3chan,\n",
        "  transform=transform,\n",
        "  data_path=rundir\n",
        ")\n",
        "\n",
        "# - Load test dataset\n",
        "print(f\"Reading test dataset from: {filename_test_3chan}\")\n",
        "dataset_test= GMNISTDataset(\n",
        "  metadata_file=filename_test_3chan,\n",
        "  transform=transform_eval,\n",
        "  data_path=rundir\n",
        ")\n",
        "\n",
        "# - Split train-cv into 70% training and 30% validation subsets\n",
        "print(\"Splitting train-cv dataset into 70% train / 30% val subsets...\")\n",
        "generator= torch.Generator().manual_seed(42)\n",
        "subset_train, subset_val= random_split(dataset_traincv, [0.7, 0.3], generator=generator)\n",
        "\n",
        "# - Create training and validation datasets from subsets\n",
        "print(\"Creating train & val datasets from subsets ...\")\n",
        "dataset_train= GMNISTDataset(\n",
        "  subset=subset_train,\n",
        "  transform=transform_train,\n",
        "  data_path=rundir\n",
        ")\n",
        "\n",
        "dataset_val= GMNISTDataset(\n",
        "  subset=subset_val,\n",
        "  transform=transform_eval,\n",
        "  data_path=rundir\n",
        ")\n",
        "\n",
        "# - We create a test dataset with minimal transforms for plot purposes\n",
        "dataset_test_draw= GMNISTDataset(\n",
        "  metadata_file=filename_test_3chan,\n",
        "  transform=transform,\n",
        "  data_path=rundir\n",
        ")\n",
        "\n",
        "# - Print dataset sizes\n",
        "print(f\"# {len(dataset_train)} samples in training set\")\n",
        "print(f\"# {len(dataset_val)} samples in validation set\")\n",
        "print(f\"# {len(dataset_test)} samples in test set\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a9465ef",
      "metadata": {
        "id": "9a9465ef"
      },
      "source": [
        "### Draw sample images\n",
        "Let's draw some sample images from the train set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "605827ce",
      "metadata": {
        "id": "605827ce"
      },
      "outputs": [],
      "source": [
        "# - Create figure\n",
        "fig = plt.figure(figsize=(15, 15))\n",
        "\n",
        "# - Show 16 sample images from the train dataset\n",
        "for i, (tensor_image, target) in islice(enumerate(dataset_test_draw), 16):\n",
        "  label = dataset_train.target2label[target]\n",
        "  ax = fig.add_subplot(4, 4, i+1)\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "  ax.imshow(tensor_image.permute(1, 2, 0))  # CHW -> HWC\n",
        "  ax.set_title(label, fontsize=15)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8228363",
      "metadata": {
        "id": "b8228363"
      },
      "source": [
        "## Create dataloaders\n",
        "We will now create `DataLoader` objects for the training, validation, and test datasets.  \n",
        "These loaders will efficiently batch and shuffle data during training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1e9017b",
      "metadata": {
        "id": "f1e9017b"
      },
      "outputs": [],
      "source": [
        "##################################\n",
        "##     CREATE DATALOADERS\n",
        "##################################\n",
        "# - Define batch size\n",
        "batch_size = 64\n",
        "\n",
        "# - Create training DataLoader\n",
        "dataloader_train= torch.utils.data.DataLoader(\n",
        "  dataset_train,\n",
        "  batch_size=batch_size,\n",
        "  shuffle=True,\n",
        "  num_workers=1\n",
        ")\n",
        "\n",
        "# - Create validation DataLoader\n",
        "dataloader_val= torch.utils.data.DataLoader(\n",
        "  dataset_val,\n",
        "  batch_size=batch_size,\n",
        "  shuffle=False,\n",
        "  num_workers=1\n",
        ")\n",
        "\n",
        "# Create test DataLoader (smaller batch size for visualization/debugging)\n",
        "dataloader_test= torch.utils.data.DataLoader(\n",
        "  dataset_test,\n",
        "  batch_size=8,\n",
        "  shuffle=False,\n",
        "  num_workers=1\n",
        ")\n",
        "\n",
        "##################################\n",
        "##    DATA INSPECTION\n",
        "##################################\n",
        "# - Fetch one batch from test loader\n",
        "imgs, targets = next(iter(dataloader_test))\n",
        "print(f\"Type of imgs: {type(imgs)}\")\n",
        "print(f\"Shape of imgs: {imgs.shape}\")\n",
        "\n",
        "# - Compute statistics\n",
        "data_min = torch.amin(imgs, dim=(2, 3))         # per-channel min for each image\n",
        "data_max = torch.amax(imgs, dim=(2, 3))         # per-channel max for each image\n",
        "data_absmax = torch.amax(torch.abs(imgs), dim=(1, 2, 3))  # absolute max per image\n",
        "\n",
        "print(\"Per-channel min:\\n\", data_min)\n",
        "print(\"Per-channel max:\\n\", data_max)\n",
        "print(\"Per-image abs max:\\n\", data_absmax)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d37b099",
      "metadata": {
        "id": "0d37b099"
      },
      "source": [
        "# CNN classifier\n",
        "We will implement two different CNN classifiers to perform image classification on the Galaxy MNIST dataset:\n",
        "\n",
        "- ResNet architecture (predefined, widely used, and powerful)\n",
        "- Custom architecture (flexible and user-defined)\n",
        "\n",
        "We will provide a complete, end-to-end example using the ResNet model.\n",
        "For the custom architecture, we will offer partial implementations along with hints and guidelines, allowing you to complete the design and training as an exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5a65d20",
      "metadata": {
        "id": "a5a65d20"
      },
      "source": [
        "## ResNet classifier\n",
        "We will define a classifier class for Galaxy MNIST image classification using a pretrained ResNet model.\n",
        "\n",
        "To do this, we will leverage the predefined architectures available in `torchvision.models` and build our model pipeline using PyTorch’s `nn.Sequential` class for modularity and clarity.\n",
        "\n",
        "This approach allows us to reuse powerful pretrained features while adapting the final classification layer to match the Galaxy MNIST label space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31300b6b",
      "metadata": {
        "id": "31300b6b"
      },
      "outputs": [],
      "source": [
        "class ResNetClassifier():\n",
        "  \"\"\" Build a ResNet classifier \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "    self,\n",
        "    nn_arch: str = \"resnet18\",\n",
        "    pretrained_weights: Optional[str] = None,\n",
        "    num_classes: int = 4,\n",
        "    n_dense_layers: int = 1,\n",
        "    dense_layer_sizes: Union[int, List[int]] = 64,\n",
        "    add_dropout: bool = True,\n",
        "    dropout_prob: float = 0.5,\n",
        "  ):\n",
        "    \"\"\" Initialize class \"\"\"\n",
        "\n",
        "    self.model= None\n",
        "    self.device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.nn_arch= nn_arch\n",
        "    self.pretrained_weights= pretrained_weights\n",
        "    self.num_classes= num_classes\n",
        "    self.n_dense_layers= n_dense_layers\n",
        "    self.add_dropout= add_dropout\n",
        "    self.dropout_prob= dropout_prob\n",
        "\n",
        "    # - Set dense layer size per layer\n",
        "    if isinstance(dense_layer_sizes, list):\n",
        "      if len(dense_layer_sizes) != self.n_dense_layers:\n",
        "        raise ValueError(\"dense_layer_sizes list must match n_dense_layers!\")\n",
        "      self.dense_layer_sizes= dense_layer_sizes\n",
        "    else:\n",
        "      self.dense_layer_sizes= [dense_layer_sizes]*self.n_dense_layers\n",
        "\n",
        "    # - Build and move model\n",
        "    if self.__build_model()<0:\n",
        "      raise RuntimeError(\"Failed to build model!\")\n",
        "\n",
        "    print(f\"Moving model to device: {self.device}\")\n",
        "    self.model.to(self.device)\n",
        "\n",
        "  def __build_model(self) -> int:\n",
        "    \"\"\"Create network from predefined ResNet architecture.\"\"\"\n",
        "\n",
        "    print(f\"Building model with architecture: {self.nn_arch}\")\n",
        "\n",
        "    # - Load pretrained model\n",
        "    #   NB: Supported weights for resnet18/34: 'IMAGENET1K_V1'\n",
        "    #       Supported weights for resnet50/101: {'IMAGENET1K_V1','IMAGENET1K_V2'}\n",
        "    try:\n",
        "      self.model = getattr(torchvision.models, self.nn_arch)(weights=self.pretrained_weights)\n",
        "    except AttributeError:\n",
        "      print(f\"ERROR: Unsupported architecture '{self.nn_arch}'.\")\n",
        "      print(\"Available models:\")\n",
        "      print(torchvision.models.list_models(module=torchvision.models))\n",
        "      return -1\n",
        "\n",
        "    # - Build classification head\n",
        "    class_head = torch.nn.Sequential()\n",
        "\n",
        "    for i, layer_size in enumerate(self.dense_layer_sizes):\n",
        "      class_head.add_module(f\"fc{i+1}\", torch.nn.LazyLinear(layer_size))\n",
        "      class_head.add_module(f\"relu_fc{i+1}\", torch.nn.ReLU())\n",
        "      if self.add_dropout:\n",
        "        class_head.add_module(f\"dropout{i+1}\", torch.nn.Dropout(p=self.dropout_prob))\n",
        "\n",
        "    if self.n_dense_layers == 0 and self.add_dropout:\n",
        "      class_head.add_module(\"dropout\", torch.nn.Dropout(p=self.dropout_prob))\n",
        "\n",
        "    # - Final output layer\n",
        "    class_head.add_module(\"output\", torch.nn.LazyLinear(self.num_classes))\n",
        "\n",
        "    # - Replace ResNet classifier head\n",
        "    self.model.fc = class_head\n",
        "\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29acd703",
      "metadata": {
        "id": "29acd703"
      },
      "source": [
        "We'll now create an instance of the ResNet classifier using the ResNet18 architecture with ImageNet pre-trained weights.\n",
        "\n",
        "The classification head is configured as follows:\n",
        "\n",
        "- 1 dense hidden layer with 64 neurons\n",
        "- Dropout applied after the dense layer with a probability of 0.5\n",
        "\n",
        "Feel free to modify these settings (e.g., architecture, number of layers, dropout rate) to experiment with different model configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ae47d76",
      "metadata": {
        "id": "5ae47d76"
      },
      "outputs": [],
      "source": [
        "# - Define model configuration\n",
        "nn_arch= \"resnet18\"\n",
        "pretrained_weights=\"DEFAULT\"\n",
        "n_dense_layers= 1\n",
        "dense_layer_sizes= [64]\n",
        "add_dropout= True\n",
        "dropout_prob= 0.5\n",
        "\n",
        "# - Create model instance\n",
        "classifier_resnet= ResNetClassifier(\n",
        "  nn_arch=nn_arch,\n",
        "  pretrained_weights=pretrained_weights,\n",
        "  num_classes=4,\n",
        "  n_dense_layers=n_dense_layers,\n",
        "  dense_layer_sizes=dense_layer_sizes,\n",
        "  add_dropout= add_dropout,\n",
        "  dropout_prob= dropout_prob\n",
        ")\n",
        "\n",
        "# - Print model architecture\n",
        "input_shape= (imgs.shape[1], imgs.shape[2], imgs.shape[3]) # (C, H, W)\n",
        "summary(classifier_resnet.model, input_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed574e1c",
      "metadata": {
        "id": "ed574e1c"
      },
      "source": [
        "### Train model\n",
        "Below, we define a set of helper methods to handle the training loop, including model optimization, loss computation, and performance tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a696ea34",
      "metadata": {
        "id": "a696ea34"
      },
      "outputs": [],
      "source": [
        "class AverageMeter:\n",
        "  def __init__(self):\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self):\n",
        "    self.sum = 0\n",
        "    self.count = 0\n",
        "\n",
        "  def update(self, value, n=1):\n",
        "    self.sum += value * n\n",
        "    self.count += n\n",
        "\n",
        "  @property\n",
        "  def avg(self):\n",
        "    return self.sum / self.count if self.count > 0 else 0\n",
        "\n",
        "\n",
        "def run_train(\n",
        "  classifier,\n",
        "  train_dl,\n",
        "  val_dl= None,\n",
        "  num_epochs: Optional[int] = 1,\n",
        "  loss_fn= None,\n",
        "  optimizer= None,\n",
        "  lr: Optional[float] = 1e-4,\n",
        "  outfile_model=\"model.pth\",\n",
        "):\n",
        "  \"\"\" Train network \"\"\"\n",
        "\n",
        "  # - Get model and pars from classifier\n",
        "  model= classifier.model\n",
        "  device= classifier.device\n",
        "  num_classes = classifier.num_classes\n",
        "\n",
        "  # - Set loss\n",
        "  if loss_fn is None:\n",
        "    print(\"Setting default CrossEntropy loss ...\")\n",
        "    loss_fn= torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  # - Set optimizer\n",
        "  if optimizer is None:\n",
        "    print(f\"Setting default Adam optimizer with lr={lr} ...\")\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "  # - Set output model/weights filenames\n",
        "  base = os.path.splitext(os.path.basename(outfile_model))[0]\n",
        "  outdir = os.path.dirname(os.path.abspath(outfile_model))\n",
        "  outfile_model_best = os.path.join(outdir, f\"{base}_best.pth\")\n",
        "  outfile_weights = os.path.join(outdir, f\"{base}_weights.pth\")\n",
        "  outfile_weights_best = os.path.join(outdir, f\"{base}_weights_best.pth\")\n",
        "\n",
        "  # - Init metrics\n",
        "  acc_metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes).to(device)\n",
        "  f1_metric = torchmetrics.F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\").to(device)\n",
        "\n",
        "  val_acc_metric = val_f1_metric = None\n",
        "\n",
        "  if val_dl is not None:\n",
        "    val_acc_metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes).to(device)\n",
        "    val_f1_metric = torchmetrics.F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\").to(device)\n",
        "\n",
        "  best_val_acc = 0.0\n",
        "  history = {\n",
        "    \"loss_train\": [],\n",
        "    \"acc_train\": [],\n",
        "    \"f1score_train\": [],\n",
        "    \"loss_val\": [],\n",
        "    \"acc_val\": [],\n",
        "    \"f1score_val\": [],\n",
        "  }\n",
        "\n",
        "  # - Training loop\n",
        "  for epoch in range(num_epochs):\n",
        "    train_loss, train_acc, train_f1 = train_epoch(\n",
        "      model,\n",
        "      train_dl,\n",
        "      loss_fn,\n",
        "      optimizer,\n",
        "      device,\n",
        "      acc_metric,\n",
        "      f1_metric,\n",
        "      epoch\n",
        "    )\n",
        "    history[\"loss_train\"].append(train_loss)\n",
        "    history[\"acc_train\"].append(train_acc)\n",
        "    history[\"f1score_train\"].append(train_f1)\n",
        "\n",
        "    # - Run validation?\n",
        "    if val_dl is not None:\n",
        "      val_loss, val_acc, val_f1 = validate_epoch(\n",
        "        model,\n",
        "        val_dl,\n",
        "        loss_fn,\n",
        "        device,\n",
        "        val_acc_metric,\n",
        "        val_f1_metric,\n",
        "        epoch\n",
        "      )\n",
        "      history[\"loss_val\"].append(val_loss)\n",
        "      history[\"acc_val\"].append(val_acc)\n",
        "      history[\"f1score_val\"].append(val_f1)\n",
        "\n",
        "      print(f\"Epoch [{epoch+1}/{num_epochs}]: loss={train_loss:.4f} (val={val_loss:.4f}), \"\n",
        "            f\"acc={train_acc:.4f} (val={val_acc:.4f}), f1={train_f1:.4f} (val={val_f1:.4f})\")\n",
        "\n",
        "      if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        print(f\"Saving best model at epoch {epoch+1} (val acc={best_val_acc:.4f})...\")\n",
        "        torch.save(model.state_dict(), outfile_weights_best)\n",
        "        torch.save(model, outfile_model_best)\n",
        "    else:\n",
        "      print(f\"Epoch [{epoch+1}/{num_epochs}]: loss={train_loss:.4f}, acc={train_acc:.4f}, f1={train_f1:.4f}\")\n",
        "\n",
        "  # - Save final model\n",
        "  print(\"Saving final model...\")\n",
        "  torch.save(model.state_dict(), outfile_weights)\n",
        "  torch.save(model, outfile_model)\n",
        "\n",
        "  print(\"Training complete.\")\n",
        "  return history\n",
        "\n",
        "\n",
        "def train_epoch(\n",
        "  model,\n",
        "  dataloader,\n",
        "  criterion,\n",
        "  optimizer,\n",
        "  device,\n",
        "  acc_metric,\n",
        "  f1_metric,\n",
        "  epoch\n",
        "):\n",
        "  \"\"\" Train one epoch \"\"\"\n",
        "\n",
        "  # - Init metrics\n",
        "  model.train()\n",
        "  loss_meter = AverageMeter()\n",
        "  acc_metric.reset()\n",
        "  f1_metric.reset()\n",
        "  progress = tqdm(dataloader, desc=f\"Epoch {epoch+1} [Train]\", leave=False)\n",
        "\n",
        "  # - Run batch loop\n",
        "  for X, y in progress:\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    # - Compute prediction and loss\n",
        "    outputs = model(X)\n",
        "    loss = criterion(outputs, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # - Update loss and accuracy\n",
        "    loss_meter.update(loss.item(), X.size(0))\n",
        "    preds = outputs.argmax(dim=1)\n",
        "    acc_metric.update(preds, y)\n",
        "    f1_metric.update(preds, y)\n",
        "\n",
        "    progress.set_postfix({\n",
        "      \"loss\": f\"{loss_meter.avg:.4f}\",\n",
        "      \"acc\": f\"{acc_metric.compute().item():.4f}\",\n",
        "      \"f1\": f\"{f1_metric.compute().item():.4f}\"\n",
        "    })\n",
        "\n",
        "  avg_loss = loss_meter.avg\n",
        "  avg_acc = acc_metric.compute().item()\n",
        "  avg_f1 = f1_metric.compute().item()\n",
        "\n",
        "  return avg_loss, avg_acc, avg_f1\n",
        "\n",
        "\n",
        "def validate_epoch(\n",
        "  model,\n",
        "  dataloader,\n",
        "  criterion,\n",
        "  device,\n",
        "  acc_metric,\n",
        "  f1_metric,\n",
        "  epoch\n",
        "):\n",
        "  \"\"\" Run validation loop \"\"\"\n",
        "\n",
        "  # - Init metrics\n",
        "  model.eval()\n",
        "  loss_meter = AverageMeter()\n",
        "  acc_metric.reset()\n",
        "  f1_metric.reset()\n",
        "  progress = tqdm(dataloader, desc=f\"Epoch {epoch+1} [Val]\", leave=False)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in progress:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      # - Compute prediction and loss\n",
        "      outputs = model(X)\n",
        "      loss = criterion(outputs, y)\n",
        "\n",
        "      # - Update loss and accuracy\n",
        "      loss_meter.update(loss.item(), X.size(0))\n",
        "      preds = outputs.argmax(dim=1)\n",
        "      acc_metric.update(preds, y)\n",
        "      f1_metric.update(preds, y)\n",
        "\n",
        "      # - Update progress bar\n",
        "      progress.set_postfix({\"loss\": f\"{loss_meter.avg:.4f}\"})\n",
        "\n",
        "  avg_loss = loss_meter.avg\n",
        "  avg_acc = acc_metric.compute().item()\n",
        "  avg_f1 = f1_metric.compute().item()\n",
        "\n",
        "  return avg_loss, avg_acc, avg_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44fbd996",
      "metadata": {
        "id": "44fbd996"
      },
      "source": [
        "Let’s define a few key training parameters:\n",
        "\n",
        "- Learning rate\n",
        "- Number of training epochs\n",
        "- Batch size (already defined when creating the data loaders)\n",
        "- Loss function\n",
        "\n",
        "Feel free to modify these values to experiment with different training setups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a952359d",
      "metadata": {
        "id": "a952359d"
      },
      "outputs": [],
      "source": [
        "num_epochs= 5\n",
        "lr= 1e-4\n",
        "loss_fn= torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(classifier_resnet.model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaff5c51",
      "metadata": {
        "id": "aaff5c51"
      },
      "source": [
        "We now train the classifier using the methods and parameters defined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e576dce",
      "metadata": {
        "id": "0e576dce"
      },
      "outputs": [],
      "source": [
        "# - Run train\n",
        "outfile_model= os.path.join(rundir, \"resnet_model.pth\")\n",
        "\n",
        "metric_hist_resnet= run_train(\n",
        "  classifier=classifier_resnet,\n",
        "  train_dl=dataloader_train,\n",
        "  val_dl=dataloader_val,\n",
        "  num_epochs=num_epochs,\n",
        "  loss_fn=loss_fn,\n",
        "  optimizer=optimizer,\n",
        "  lr=lr,\n",
        "  outfile_model=outfile_model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76de3263",
      "metadata": {
        "id": "76de3263"
      },
      "source": [
        "Let’s plot the training and validation metrics after the training run is complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96334a0a",
      "metadata": {
        "id": "96334a0a"
      },
      "outputs": [],
      "source": [
        "def draw_metric_hist(metric_hist):\n",
        "\n",
        "  epochs = np.arange(1, len(metric_hist[\"loss_train\"]) + 1)\n",
        "  fig = plt.figure(figsize=(14, 5))\n",
        "\n",
        "  # - Plot train/val loss\n",
        "  ax1 = fig.add_subplot(1, 2, 1)\n",
        "  ax1.plot(epochs, metric_hist[\"loss_train\"], '-o', label='Train Loss')\n",
        "  ax1.plot(epochs, metric_hist[\"loss_val\"], '--<', label='Validation Loss')\n",
        "  ax1.set_title(\"Loss Over Epochs\", fontsize=14)\n",
        "  ax1.set_xlabel(\"Epoch\", fontsize=12)\n",
        "  ax1.set_ylabel(\"Loss\", fontsize=12)\n",
        "  ax1.legend(fontsize=11)\n",
        "  ax1.grid(True)\n",
        "\n",
        "  # - Plot acc/f1score\n",
        "  ax2 = fig.add_subplot(1, 2, 2)\n",
        "  ax2.set_ylim(0, 1)\n",
        "  ax2.plot(epochs, metric_hist[\"acc_train\"], '-o', label='Train Accuracy')\n",
        "  ax2.plot(epochs, metric_hist[\"acc_val\"], '--<', label='Validation Accuracy')\n",
        "  ax2.plot(epochs, metric_hist[\"f1score_train\"], '-*', label='Train F1-score')\n",
        "  ax2.plot(epochs, metric_hist[\"f1score_val\"], '-->', label='Validation F1-score')\n",
        "  ax2.set_title(\"Accuracy and F1-score\", fontsize=14)\n",
        "  ax2.set_xlabel(\"Epoch\", fontsize=12)\n",
        "  ax2.set_ylabel(\"Score\", fontsize=12)\n",
        "  ax2.legend(fontsize=11)\n",
        "  ax2.grid(True)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "# - Print & plot metrics\n",
        "print(\"== Training Metrics ==\")\n",
        "print(metric_hist_resnet)\n",
        "\n",
        "draw_metric_hist(metric_hist_resnet)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24a3cb40",
      "metadata": {
        "id": "24a3cb40"
      },
      "source": [
        "### Evaluate model\n",
        "We now evaluate the trained classifier on the test dataset, performing the following:\n",
        "\n",
        "- **Classification metrics**: accuracy, F1-score, and confusion matrix\n",
        "- **Feature map visualization**: inspecting internal feature representations\n",
        "- **Activation heatmaps**: highlighting regions of the image that contribute most to predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86ab1a3c",
      "metadata": {
        "id": "86ab1a3c"
      },
      "outputs": [],
      "source": [
        "def run_test(\n",
        "  classifier,\n",
        "  dataloader,\n",
        "  modelfile=\"\",\n",
        "  weightfile=\"\"\n",
        "):\n",
        "  \"\"\" Evaluate the model on test data and return performance metrics \"\"\"\n",
        "\n",
        "  # - Load model from file?\n",
        "  if modelfile==\"\":\n",
        "    model= classifier.model\n",
        "  else:\n",
        "    print(f\"Loading model from file: {modelfile}\")\n",
        "    model= torch.load(modelfile, weights_only=False)\n",
        "\n",
        "  # - Check for model/dataloader\n",
        "  if model is None:\n",
        "    print(\"ERROR: No model present, cannot run prediction on test set!\")\n",
        "    return None\n",
        "\n",
        "  # - Load model weights\n",
        "  if weightfile!=\"\":\n",
        "    print(f\"Loading model weights from file: {weightfile}\")\n",
        "    model.load_state_dict(torch.load(weightfile, weights_only=True))\n",
        "\n",
        "  model= model.to(classifier.device).eval()\n",
        "\n",
        "  # - Initialize metrics\n",
        "  num_classes = classifier.num_classes\n",
        "  device = classifier.device\n",
        "\n",
        "  accuracy_metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes).to(device)\n",
        "  f1score_metric = torchmetrics.F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\").to(device)\n",
        "  confusion_matrix_metric = torchmetrics.ConfusionMatrix(task=\"multiclass\", num_classes=num_classes, normalize=\"true\").to(device)\n",
        "\n",
        "  accuracy_metric.reset()\n",
        "  f1score_metric.reset()\n",
        "  confusion_matrix_metric.reset()\n",
        "\n",
        "  # - Evaluation loop\n",
        "  progress_bar = tqdm(dataloader, desc=\"[Test]\", leave=False)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X_batch, y_batch in progress_bar:\n",
        "      X_batch, y_batch = X_batch.to(classifier.device), y_batch.to(classifier.device)\n",
        "\n",
        "      # - Compute prediction and loss\n",
        "      outputs = model(X_batch)\n",
        "      preds = outputs.argmax(dim=1)\n",
        "\n",
        "      # - Update loss and accuracy\n",
        "      accuracy_metric.update(preds, y_batch)\n",
        "      f1score_metric.update(preds, y_batch)\n",
        "      confusion_matrix_metric.update(preds, y_batch)\n",
        "\n",
        "  # - Final metrics\n",
        "  avg_accuracy = accuracy_metric.compute().item()\n",
        "  avg_f1score = f1score_metric.compute().item()\n",
        "  confusion_matrix= confusion_matrix_metric.compute().cpu().numpy()\n",
        "\n",
        "  metrics= {\n",
        "    \"acc\": avg_accuracy,\n",
        "    \"avg_f1score\": avg_f1score,\n",
        "    \"cm\": confusion_matrix,\n",
        "    \"cm_metric\": confusion_matrix_metric\n",
        "  }\n",
        "\n",
        "  return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e74436f7",
      "metadata": {
        "id": "e74436f7"
      },
      "source": [
        "We now evaluate the model on the test set and compute performance metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "477b0a78",
      "metadata": {
        "id": "477b0a78"
      },
      "outputs": [],
      "source": [
        "weightfile= os.path.join(rundir, \"resnet_model_weights.pth\")\n",
        "\n",
        "metrics_resnet_test= run_test(\n",
        "  classifier_resnet,\n",
        "  dataloader=dataloader_test,\n",
        "  weightfile=weightfile\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2022c78e",
      "metadata": {
        "id": "2022c78e"
      },
      "source": [
        "#### Visualizing metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "003bc398",
      "metadata": {
        "id": "003bc398"
      },
      "outputs": [],
      "source": [
        "# - Print metrics\n",
        "print(\"== metrics (TEST) ==\")\n",
        "print(metrics_resnet_test)\n",
        "\n",
        "# - Draw confusion matrix\n",
        "fig_, ax_ = metrics_resnet_test[\"cm_metric\"].plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe2d2d47",
      "metadata": {
        "id": "fe2d2d47"
      },
      "source": [
        "#### Visualizing feature maps\n",
        "**Feature maps** (also known as activation maps) are the outputs produced by convolutional layers after applying filters to an input—such as an image or a previous feature map.\n",
        "\n",
        "Visualizing these feature maps helps us understand **what the network is learning**. Specifically:\n",
        "\n",
        "- **Early layers** typically capture **low-level features**, such as edges, corners, or fine-grained textures.\n",
        "- **Deeper layers** tend to capture **higher-level**, abstract patterns, such as shapes, structures, or class-specific regions.\n",
        "\n",
        "By inspecting the activations for a specific input image, we can gain insight into which parts of the image are being emphasized or preserved as information flows through the network."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "288761d5",
      "metadata": {
        "id": "288761d5"
      },
      "source": [
        "We define a method below to extract feature maps from a given model for a specific input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "029477bb",
      "metadata": {
        "id": "029477bb"
      },
      "outputs": [],
      "source": [
        "def extract_feature_maps(\n",
        "  classifier,\n",
        "  image,\n",
        "  modelfile=\"\",\n",
        "  weightfile=\"\"\n",
        "):\n",
        "  \"\"\" Extract a list of feature map for a model and input image \"\"\"\n",
        "\n",
        "  # - Load image on device\n",
        "  #   NB: Transforms are expected to be already applied\n",
        "  if image is None:\n",
        "    print(\"ERROR: Input image is None!\")\n",
        "    return -1\n",
        "\n",
        "  image = image.to(classifier.device)\n",
        "\n",
        "  # - Load model from file?\n",
        "  if modelfile==\"\":\n",
        "    model= classifier.model\n",
        "  else:\n",
        "    print(f\"Loading model from file: {modelfile}\")\n",
        "    model= torch.load(modelfile, weights_only=False)\n",
        "\n",
        "  # - Check for model/dataloader\n",
        "  if model is None:\n",
        "    print(\"ERROR: No model present, cannot run prediction on test set!\")\n",
        "    return None\n",
        "\n",
        "  # - Load model weights\n",
        "  if weightfile!=\"\":\n",
        "    print(f\"Loading weights from file: {weightfile}\")\n",
        "    model.load_state_dict(torch.load(weightfile, weights_only=True))\n",
        "\n",
        "  model= model.to(classifier.device).eval()\n",
        "\n",
        "  # - Extract all Conv2D layers (excluding downsampling layers)\n",
        "  print(\"Extracting convolutional layers...\")\n",
        "  conv_layers = []\n",
        "  conv_layer_names = []\n",
        "  for name, layer in model.named_modules():\n",
        "    if isinstance(layer, torch.nn.Conv2d) and \"downsample\" not in name:\n",
        "      conv_layers.append(layer)\n",
        "      conv_layer_names.append(name)\n",
        "\n",
        "  # - Register hooks to capture activations\n",
        "  activations = {}\n",
        "  def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "      activations[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "  for name, layer in zip(conv_layer_names, conv_layers):\n",
        "    layer.register_forward_hook(get_activation(name))\n",
        "\n",
        "  # - Run a forward pass to trigger the hooks\n",
        "  _ = model(image)\n",
        "\n",
        "  # - Get activations\n",
        "  feature_maps= []\n",
        "  layer_names= []\n",
        "  print(\"--> Feature Map Shapes:\")\n",
        "  for name in conv_layer_names:\n",
        "    fmap = activations[name].squeeze(0)  # Remove batch dim\n",
        "    print(f\"{name}: {tuple(fmap.shape)}\")\n",
        "\n",
        "    feature_maps.append(fmap.cpu().numpy())\n",
        "    layer_names.append(name)\n",
        "\n",
        "  return feature_maps, layer_names"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20cd2d90",
      "metadata": {
        "id": "20cd2d90"
      },
      "source": [
        "We define a method below to visualize feature maps extracted from a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42ee4d02",
      "metadata": {
        "id": "42ee4d02"
      },
      "outputs": [],
      "source": [
        "def draw_feature_maps(\n",
        "  classifier,\n",
        "  image,\n",
        "  modelfile=\"\",\n",
        "  weightfile=\"\",\n",
        "  images_per_row= 4\n",
        "):\n",
        "  \"\"\" Extract and plot feature maps for an input image \"\"\"\n",
        "\n",
        "  # - Retrieve feature maps\n",
        "  feature_maps, layer_names= extract_feature_maps(\n",
        "    classifier,\n",
        "    image,\n",
        "    modelfile=modelfile,\n",
        "    weightfile=weightfile\n",
        "  )\n",
        "  if feature_maps is None or not feature_maps:\n",
        "    print(\"ERROR: Failed to compute feature maps.\")\n",
        "    return -1\n",
        "\n",
        "  # - Draw feature maps\n",
        "  for layer_name, fmap in zip(layer_names, feature_maps):\n",
        "    num_channels = fmap.shape[0]\n",
        "    size = fmap.shape[1]\n",
        "    n_cols = max(1, num_channels // images_per_row)\n",
        "    n_rows = min(images_per_row, num_channels)\n",
        "    display_grid = np.zeros((size * n_cols, size * n_rows))\n",
        "\n",
        "    for col in range(n_cols):\n",
        "      for row in range(images_per_row):\n",
        "        idx= col * n_rows + row\n",
        "        channel_image = fmap[idx]\n",
        "\n",
        "        # - Normalize for display\n",
        "        channel_image = (channel_image - channel_image.mean()) / (channel_image.std() + 1e-5)\n",
        "        channel_image = (channel_image * 64 + 128).clip(0, 255).astype(\"uint8\")\n",
        "        display_grid[\n",
        "          col * size : (col + 1) * size,\n",
        "          row * size : (row + 1) * size\n",
        "        ] = channel_image\n",
        "\n",
        "    scale = 1.0 / size\n",
        "    plt.figure(figsize=(scale * display_grid.shape[1], scale * display_grid.shape[0]))\n",
        "    plt.title(f\"Feature Maps - {layer_name}\", fontsize=12)\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
        "    plt.grid(False)\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "\n",
        "  return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9e6ffe4",
      "metadata": {
        "id": "f9e6ffe4"
      },
      "source": [
        "Let's plot the feature maps for a sample image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78785381",
      "metadata": {
        "id": "78785381"
      },
      "outputs": [],
      "source": [
        "# - Take a sample image from the test dataset\n",
        "data_index= 0 # take the first\n",
        "image, target= dataset_test[data_index]\n",
        "label= dataset_test.target2label[target]\n",
        "image= image.unsqueeze(0)\n",
        "\n",
        "print(\"image\")\n",
        "print(type(image))\n",
        "print(image.shape)\n",
        "\n",
        "# - Draw feature maps\n",
        "weightfile= os.path.join(rundir, \"resnet_model_weights.pth\")\n",
        "\n",
        "draw_feature_maps(\n",
        "  classifier_resnet,\n",
        "  image,\n",
        "  weightfile=weightfile,\n",
        "  images_per_row=16\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d62d8bd",
      "metadata": {
        "id": "1d62d8bd"
      },
      "source": [
        "#### Visualizing heatmaps of class activation\n",
        "**Class Activation Maps (CAM)** help us understand which regions of an input image most influenced the model's classification decision. These maps highlight **spatial regions** in the image that are important for predicting a specific class.\n",
        "\n",
        "A **class activation heatmap** is a 2D grid of scores—computed for each spatial location in the input—that indicates how important each region is with respect to the output class.\n",
        "\n",
        "In this tutorial, we'll use the **Grad-CAM** technique (*Gradient-weighted Class Activation Mapping*), as described in:\n",
        "\n",
        "> *“Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization”*\n",
        "\n",
        "This method works by:\n",
        "\n",
        "- Taking the output feature map from a convolutional layer (given an input image)  \n",
        "- Computing the gradient of the class score with respect to each channel in the feature map  \n",
        "- Using these gradients to **weight each channel**  \n",
        "- Combining them to produce a spatial heatmap of class-relevant activations\n",
        "\n",
        "> *Intuitively, this highlights \"how intensely the image activates the class\" across different regions.*\n",
        "\n",
        "We will use an existing Python implementation of Grad-CAM for this visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da2e8a6f",
      "metadata": {
        "id": "da2e8a6f"
      },
      "source": [
        "Let's plot the class activation heatmaps from the **last convolutional layer**, with respect to the **predicted class**, for a sample of test images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b29b79c0",
      "metadata": {
        "id": "b29b79c0"
      },
      "outputs": [],
      "source": [
        "def plot_sample_predictions(\n",
        "  classifier,\n",
        "  dataset,\n",
        "  dataset_gradcam=None,\n",
        "  modelfile=\"\",\n",
        "  weightfile=\"\",\n",
        "  plot_gradcam=True,\n",
        "  gradcam_method=\"gradcam\",\n",
        "  layer_names=[],\n",
        "  aug_smooth=False,\n",
        "  eigen_smooth=False,\n",
        "  gradcam_alpha=0.5,\n",
        "  apply_heatmap_thr=False,\n",
        "  heatmap_thr=0.7,\n",
        "  plot_gradcam_only=False\n",
        "):\n",
        "  \"\"\" Plot gradCAM on some images \"\"\"\n",
        "\n",
        "  # - Load model from file?\n",
        "  if modelfile==\"\":\n",
        "    model= classifier.model\n",
        "  else:\n",
        "    print(f\"Loading model from: {modelfile}\")\n",
        "    model= torch.load(modelfile, weights_only=False)\n",
        "\n",
        "  # - Check for model/dataloader\n",
        "  if model is None:\n",
        "    print(\"ERROR: No model present, cannot run prediction on test set!\")\n",
        "    return None\n",
        "\n",
        "  # - Load model weights\n",
        "  if weightfile!=\"\":\n",
        "    print(f\"Loading weights from: {weightfile}\")\n",
        "    model.load_state_dict(torch.load(weightfile, weights_only=True))\n",
        "\n",
        "  model= model.to(classifier.device).eval()\n",
        "\n",
        "  # - Set dataset to be used for gradcam\n",
        "  if dataset_gradcam is None:\n",
        "    dataset_gradcam= dataset\n",
        "\n",
        "  # - Map Grad-CAM methods\n",
        "  methods = {\n",
        "    \"gradcam\": GradCAM,\n",
        "    \"hirescam\": HiResCAM,\n",
        "    \"scorecam\": ScoreCAM,\n",
        "    \"gradcam++\": GradCAMPlusPlus,\n",
        "    \"ablationcam\": AblationCAM,\n",
        "    \"xgradcam\": XGradCAM,\n",
        "    \"eigencam\": EigenCAM,\n",
        "    \"eigengradcam\": EigenGradCAM,\n",
        "    \"layercam\": LayerCAM,\n",
        "    \"fullgrad\": FullGrad,\n",
        "    #\"fem\": FEM,\n",
        "    \"gradcamelementwise\": GradCAMElementWise,\n",
        "    \"kpcacam\": KPCA_CAM,\n",
        "    #\"shapleycam\": ShapleyCAM\n",
        "  }\n",
        "  cam_algorithm = methods.get(gradcam_method.lower())\n",
        "  if cam_algorithm is None:\n",
        "    raise ValueError(f\"Unsupported Grad-CAM method: {gradcam_method}\")\n",
        "\n",
        "  # - Retrieve target layers\n",
        "  print(\"Finding target ReLU layers in model...\")\n",
        "  print(find_layer_types_recursive(model, [torch.nn.ReLU]))\n",
        "  target_layers= []\n",
        "  for name in layer_names:\n",
        "    if hasattr(model, name):\n",
        "      target_layers.append(getattr(model, name))\n",
        "    elif name in model._modules:\n",
        "      target_layers.append(model._modules[name])\n",
        "    else:\n",
        "      print(f\"WARNING: Layer '{name}' not found in model.\")\n",
        "\n",
        "    #layer= model._modules[name]\n",
        "    #print(\"layer\")\n",
        "    #print(layer)\n",
        "    #target_layers.append(layer)\n",
        "\n",
        "  if not target_layers:\n",
        "    print(\"ERROR: No valid target layers provided for Grad-CAM.\")\n",
        "    return None\n",
        "\n",
        "  print(\"--> target_layers\")\n",
        "  print(target_layers)\n",
        "\n",
        "  # - Plot images\n",
        "  fig = plt.figure(figsize=(15, 15))\n",
        "\n",
        "  #for i, ((input_img, target), (input_img_gradcam, target_gradcam)) in islice(enumerate(zip(dataset,dataset_gradcam)), 16):\n",
        "  for i, ((img, target), (img_gc, _)) in islice(enumerate(zip(dataset, dataset_gradcam)), 16):\n",
        "    with torch.enable_grad():\n",
        "      # - Compute model prediction\n",
        "      label= dataset.target2label[target]\n",
        "      print(\"--> Computing model prediction for image (label=%s, target=%d) ...\" % (label, target))\n",
        "      input_tensor= img.unsqueeze(0).to(classifier.device)\n",
        "      input_tensor_gc= img_gc.unsqueeze(0).to(classifier.device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        logits = model(input_tensor)\n",
        "        pred_idx = logits.argmax(dim=1).item()\n",
        "        softmax_probs = torch.nn.functional.softmax(logits, dim=1)\n",
        "        prob_pred = softmax_probs.max().item()\n",
        "        label_pred = dataset.target2label[pred_idx]\n",
        "\n",
        "      # - Convert input image to numpy\n",
        "      rgb_img = img_gc.permute(1, 2, 0).cpu().numpy()\n",
        "      grayscale_img = cv2.cvtColor(rgb_img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "      # - Compute cam\n",
        "      if plot_gradcam:\n",
        "        with cam_algorithm(model=model, target_layers=target_layers) as cam:\n",
        "          grayscale_cam = cam(\n",
        "            #input_tensor=input_tensor_gc,\n",
        "            input_tensor=input_tensor,\n",
        "            #targets=None,\n",
        "            targets=[ClassifierOutputTarget(pred_idx)],\n",
        "            aug_smooth=aug_smooth,\n",
        "            eigen_smooth=eigen_smooth\n",
        "          )[0,:]\n",
        "          cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True, image_weight=gradcam_alpha)\n",
        "          cam_image = cv2.cvtColor(cam_image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "          # - Create thresholded heatmap\n",
        "          heatmap = cv2.applyColorMap(np.uint8(255 * grayscale_cam), cv2.COLORMAP_JET)\n",
        "          heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB) / 255.0\n",
        "          alphas = np.ones(grayscale_img.shape) * gradcam_alpha\n",
        "          if apply_heatmap_thr:\n",
        "            alphas[grayscale_cam < heatmap_thr] = 0 # set invisible\n",
        "\n",
        "      # - Plot image\n",
        "      ax = fig.add_subplot(4, 4, i+1)\n",
        "      ax.set_xticks([]); ax.set_yticks([])\n",
        "      if not plot_gradcam_only:\n",
        "        ax.imshow(grayscale_img, cmap=\"gray\")\n",
        "\n",
        "      if plot_gradcam:\n",
        "        if plot_gradcam_only:\n",
        "          ax.imshow(cam_image)\n",
        "        else:\n",
        "          ax.imshow(heatmap, alpha=alphas)\n",
        "\n",
        "      ax.set_title(f'{label} \\n pred: {label_pred}, p={prob_pred:.1f})', fontsize=11)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec9331f4",
      "metadata": {
        "id": "ec9331f4"
      },
      "outputs": [],
      "source": [
        "# - Run gradCAM on some test data\n",
        "target_layers= [\"layer4\"]\n",
        "weightfile= os.path.join(rundir, \"resnet_model_weights.pth\")\n",
        "\n",
        "plot_sample_predictions(\n",
        "  classifier_resnet,\n",
        "  dataset=dataset_test,\n",
        "  dataset_gradcam=dataset_test_draw,\n",
        "  weightfile=weightfile,\n",
        "  plot_gradcam=True,\n",
        "  layer_names=target_layers,\n",
        "  gradcam_method=\"gradcam\",\n",
        "  aug_smooth=False,\n",
        "  eigen_smooth=False,\n",
        "  gradcam_alpha=0.3,\n",
        "  apply_heatmap_thr=True,\n",
        "  heatmap_thr=0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f317ea91",
      "metadata": {
        "id": "f317ea91"
      },
      "source": [
        "## Custom classifier\n",
        "Let’s implement a custom classifier by creating a class that defines a configurable neural network architecture using PyTorch's `nn.Sequential`.\n",
        "This will allow us to easily stack layers and experiment with different configurations for training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa96d86c",
      "metadata": {
        "id": "fa96d86c"
      },
      "outputs": [],
      "source": [
        "class CustomClassifier():\n",
        "  \"\"\"Build a configurable custom CNN classifier using nn.Sequential.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "    self,\n",
        "    num_classes: int = 4,\n",
        "    n_conv_layers: int = 3,\n",
        "    n_filters: Union[int, List[int]] = [8, 16, 32],\n",
        "    kern_sizes: Union[int, List[int]] = [3, 5, 7],\n",
        "    strides: Union[int, List[int]] = [1, 1, 1],\n",
        "    add_maxpool: bool = True,\n",
        "    pool_sizes: Union[int, List[int]] = 2,\n",
        "    add_batchnorm: bool = True,\n",
        "    n_dense_layers: int = 1,\n",
        "    dense_layer_sizes: Union[int, List[int]] = [64],\n",
        "    add_dropout: bool = True,\n",
        "    dropout_prob: float = 0.5,\n",
        "  ):\n",
        "    self.model= None\n",
        "    self.device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.num_classes= num_classes\n",
        "    self.n_conv_layers= n_conv_layers\n",
        "    self.n_dense_layers= n_dense_layers\n",
        "    self.add_maxpool= add_maxpool\n",
        "    self.add_batchnorm= add_batchnorm\n",
        "    self.add_dropout= add_dropout\n",
        "    self.dropout_prob= dropout_prob\n",
        "\n",
        "    # - Normalize all parameters to lists\n",
        "    self.n_filters = self.__expand_param(n_filters, n_conv_layers, \"n_filters\")\n",
        "    self.kern_sizes = self.__expand_param(kern_sizes, n_conv_layers, \"kern_sizes\")\n",
        "    self.strides = self.__expand_param(strides, n_conv_layers, \"strides\")\n",
        "    self.pool_sizes = self.__expand_param(pool_sizes, n_conv_layers, \"pool_sizes\")\n",
        "    self.dense_layer_sizes = self.__expand_param(dense_layer_sizes, n_dense_layers, \"dense_layer_sizes\")\n",
        "\n",
        "    # - Build network\n",
        "    print(\"Building custom CNN architecture...\")\n",
        "    if self.__build_model()<0:\n",
        "      raise RuntimeError(\"Failed to build neural network.\")\n",
        "\n",
        "    # - Move model to device\n",
        "    print(f\"Moving model to device: {self.device}\")\n",
        "    self.model.to(self.device)\n",
        "\n",
        "  def __expand_param(self, param, expected_len, name):\n",
        "    \"\"\"Ensure param is a list of length expected_len.\"\"\"\n",
        "    if isinstance(param, list):\n",
        "      if len(param) != expected_len:\n",
        "        raise ValueError(f\"Parameter '{name}' must have length equal to number of layers ({expected_len})\")\n",
        "      return param\n",
        "    else:\n",
        "      return [param] * expected_len\n",
        "\n",
        "  def __build_model(self):\n",
        "    \"\"\"Create the CNN model using nn.Sequential.\"\"\"\n",
        "\n",
        "    self.model = torch.nn.Sequential()\n",
        "\n",
        "    # - Add convolutional layers\n",
        "    for i in range(self.n_conv_layers):\n",
        "      # - Add convolution layer\n",
        "      self.model.add_module(\n",
        "        f\"conv{i+1}\",\n",
        "        torch.nn.LazyConv2d(\n",
        "          out_channels=self.n_filters[i],\n",
        "          kernel_size=self.kern_sizes[i],\n",
        "          stride=self.strides[i],\n",
        "          padding=\"same\"\n",
        "        )\n",
        "      )\n",
        "\n",
        "      # - Add activation\n",
        "      self.model.add_module(f\"relu{i+1}\", torch.nn.ReLU())\n",
        "\n",
        "      # - Add BatchNorm\n",
        "      if self.add_batchnorm:\n",
        "        self.model.add_module(f\"bn{i+1}\", torch.nn.LazyBatchNorm2d())\n",
        "\n",
        "      # - Add MaxPool\n",
        "      if self.add_maxpool:\n",
        "        self.model.add_module(f\"pool{i+1}\", torch.nn.MaxPool2d(kernel_size=self.pool_sizes[i]))\n",
        "\n",
        "    # - Flatten layer\n",
        "    self.model.add_module('flatten', torch.nn.Flatten())\n",
        "\n",
        "    # - Fully connected layers\n",
        "    for i in range(self.n_dense_layers):\n",
        "      self.model.add_module(f\"fc{i+1}\", torch.nn.LazyLinear(self.dense_layer_sizes[i]))\n",
        "      self.model.add_module(f\"relu_fc{i+1}\", torch.nn.ReLU())\n",
        "      if self.add_dropout:\n",
        "        self.model.add_module(f\"dropout{i+1}\", torch.nn.Dropout(p=self.dropout_prob))\n",
        "\n",
        "    # - Optional dropout even if no dense layers\n",
        "    if self.n_dense_layers==0 and self.add_dropout:\n",
        "      self.model.add_module(\"dropout\", torch.nn.Dropout(p=self.dropout_prob))\n",
        "\n",
        "    # - Add output layer\n",
        "    self.model.add_module(\"output\", torch.nn.LazyLinear(self.num_classes))\n",
        "\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6fcdb61",
      "metadata": {
        "id": "e6fcdb61"
      },
      "source": [
        "It's time to build your own custom CNN architecture!\n",
        "You can experiment with different configurations - such as the number of convolutional layers, filter sizes, strides, and the fully connected head.\n",
        "\n",
        "Below is an example that creates a custom classifier with 3 convolutional layers and a classification head with 1 dense hidden layer of 64 neurons:\n",
        "\n",
        "\n",
        "```python\n",
        "classifier= CustomClassifier(\n",
        "  n_conv_layers= 3,\n",
        "  n_filters= [8,16,32],\n",
        "  kern_sizes= [3,5,7],\n",
        "  strides= [1,1,1],\n",
        "  add_maxpool= True,\n",
        "  pool_sizes= 2,\n",
        "  add_batchnorm= True,\n",
        "  n_dense_layers= 1,\n",
        "  dense_layer_sizes= [64],\n",
        "  add_dropout= True,\n",
        "  dropout_prob = 0.5\n",
        ")\n",
        "summary(classifier.model, input_shape)\n",
        "```\n",
        "\n",
        "Feel free to modify:\n",
        "\n",
        "- Number of layers\n",
        "- Filter sizes\n",
        "- Add or remove max pooling or batch normalization\n",
        "- Dropout settings\n",
        "- Dense layer size\n",
        "\n",
        "Use this as a starting point to explore and test different CNN architectures!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fe4a9f4",
      "metadata": {
        "id": "0fe4a9f4"
      },
      "outputs": [],
      "source": [
        "##### ADD YOUR CODE HERE ######\n",
        "# ...\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bb03d85",
      "metadata": {
        "id": "7bb03d85"
      },
      "source": [
        "### Train model\n",
        "Train the custom classifier using the `CustomClassifier` class defined above, following the same steps used for the ResNet classifier.\n",
        "\n",
        "You can reuse the previously defined components:\n",
        "\n",
        "- The training method: `run_train()`\n",
        "- The data loaders: `dataloader_train`, `dataloader_val`\n",
        "- The plotting utilities: e.g., `draw_metric_hist`\n",
        "\n",
        "> 💡 *Consider whether to apply ImageNet normalization as part of your input transform — this may or may not be beneficial for a custom model trained from scratch.*\n",
        "\n",
        "Feel free to experiment with different model configurations and observe how they affect training performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7863714b",
      "metadata": {
        "id": "7863714b"
      },
      "outputs": [],
      "source": [
        "##### ADD YOUR CODE HERE ######\n",
        "# ...\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67ddb438",
      "metadata": {
        "id": "67ddb438"
      },
      "source": [
        "Let's plot the training and validation metrics after the training run is complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18d0d8cd",
      "metadata": {
        "id": "18d0d8cd"
      },
      "outputs": [],
      "source": [
        "##### ADD YOUR CODE HERE ######\n",
        "# ...\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3b1b32f",
      "metadata": {
        "id": "e3b1b32f"
      },
      "source": [
        "### Evaluate model\n",
        "Let's load the saved trained model and run inference on the test dataset.\n",
        "\n",
        "You can reuse the previously defined methods:\n",
        "\n",
        "- Evaluation: `run_test()`\n",
        "- Plotting utilities:\n",
        "  - `plot_sample_predictions()`\n",
        "  - `draw_feature_maps()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22535e8e",
      "metadata": {
        "id": "22535e8e"
      },
      "outputs": [],
      "source": [
        "##### ADD YOUR CODE HERE ######\n",
        "# ...\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2a4605e",
      "metadata": {
        "id": "d2a4605e"
      },
      "source": [
        "#### Visualizing metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a7f7d4e",
      "metadata": {
        "id": "6a7f7d4e"
      },
      "outputs": [],
      "source": [
        "##### ADD YOUR CODE HERE ######\n",
        "# ...\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21f87e7f",
      "metadata": {
        "id": "21f87e7f"
      },
      "source": [
        "#### Visualizing feature maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "686c9bf4",
      "metadata": {
        "id": "686c9bf4"
      },
      "outputs": [],
      "source": [
        "##### ADD YOUR CODE HERE ######\n",
        "# ...\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c342bac",
      "metadata": {
        "id": "1c342bac"
      },
      "source": [
        "#### Visualizing heatmaps of class activation\n",
        "Visualize heatmaps using ReLU layers by setting `target_layers` in `plot_sample_predictions` to one of the ReLU layer names.  \n",
        "For example, if your model includes a layer named `\"relu3\"`, you can set:\n",
        "\n",
        "```python\n",
        "target_layers = [\"relu3\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d913cce",
      "metadata": {
        "id": "0d913cce"
      },
      "outputs": [],
      "source": [
        "##### ADD YOUR CODE HERE ######\n",
        "# ...\n",
        "# ..."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "usc8-ai-workshop",
      "language": "python",
      "name": "usc8-ai-workshop"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}